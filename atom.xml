<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://roshameow.github.io//personal_homepage/atom.xml" rel="self" type="application/atom+xml" /><link href="https://roshameow.github.io//personal_homepage/" rel="alternate" type="text/html" /><updated>2024-02-07T17:01:24+00:00</updated><id>https://roshameow.github.io//personal_homepage/atom.xml</id><title type="html">Liu, Wen’s Home Page</title><subtitle>Work, Experiments and Ideas.</subtitle><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;title&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;icon&quot;=&gt;nil}]}</name><email>w.liuatnk@gmail.com</email></author><entry><title type="html">stable-diffusion的用法: 用 ipadatper+controlnet canny做风格转换</title><link href="https://roshameow.github.io//personal_homepage/docs/photo/stable-diffusion1/" rel="alternate" type="text/html" title="stable-diffusion的用法: 用 ipadatper+controlnet canny做风格转换" /><published>2024-02-07T00:00:00+00:00</published><updated>2024-02-08T08:58:53+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/photo/stable-diffusion1</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/photo/stable-diffusion1/"><![CDATA[<p>用ipadapter和canny做风格转换</p>

<h2 id="comfyui使用">ComfyUI使用</h2>

<ol>
  <li>调用ComfyUI中default的工作流
    <ul>
      <li>下载<a href="https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0">sdxl模型</a></li>
    </ul>
  </li>
  <li><strong>连上IPAdapter节点</strong>
    <ul>
      <li>安装<a href="https://github.com/cubiq/ComfyUI_IPAdapter_plus">IPAdapter_plus插件</a>
        <ul>
          <li>下载需要的ipadater微调模型和clip vision编码器: 把图像用clip vision编码后输入base model的cross-attention层</li>
        </ul>
      </li>
      <li>ComfyUI里连接顺序是: Checkpoint: model-&gt; IPAdapter: model -&gt; KSampler: model</li>
      <li>weigt设为1, text prompt都空着, latent prompt也空着</li>
    </ul>
  </li>
  <li><strong>连上Controlnet节点</strong>
    <ul>
      <li>下载<a href="https://huggingface.co/collections/diffusers/sdxl-controlnets-64f9c35846f3f06f5abe351f">canny模型</a>
        <ul>
          <li><del>找不到其他适合sdxl的controlnet模型</del>因为我们的原图是个线稿适合用canny</li>
        </ul>
      </li>
      <li>ComfyUI里连接顺序是: Checkpoint-&gt;prompt-&gt;controlnet-&gt;Ksampler: text prompt
        <ul>
          <li>controlnet最后是作用在text prompt上的?</li>
        </ul>
      </li>
      <li>调整controlnet权重和影响步数: stength=0.5, start_percent=0, end_percent=0.6
        <ul>
          <li>如果设置过大就只能得到一张有颜色的线稿图</li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<h2 id="结果">结果</h2>

<p>原图: 
<img src="/personal_homepage/docs/attachment/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202024-02-07%20224110.png" alt="屏幕截图 2024-02-07 224110.png" width="600" /> + <img src="/personal_homepage/docs/attachment/2e2b76d160ca4317bb8e095ffd8ff878.jpg.png" alt="2e2b76d160ca4317bb8e095ffd8ff878.jpg.png" width="150" /></p>

<p>合成图: 
<img src="/personal_homepage/docs/attachment/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202024-02-07%20224141.png" alt="屏幕截图 2024-02-07 224141.png" width="600" /></p>

<ul>
  <li>竟然可以识别龙的眼睛啊…有好几幅图都把猫眼替换到了龙眼上
    <ul>
      <li>如果两种图有同样的元素(眼睛之类的), 可能可以比较好的替换</li>
      <li>最后的龙和人, 龙和山, 都不能对应起来</li>
    </ul>
  </li>
  <li>ipadapter没法主动控制合成哪个图像中的哪种元素:
    <ul>
      <li>虽然替换了颜色, 背景, 但是没法替换材质, 笔触</li>
      <li>有几张图还替换了原图的色块分布, 这是我们不想要的</li>
    </ul>
  </li>
  <li>另外, controlnet-canny和ipadapter的龙都不能省略
    <ul>
      <li>如果ipadapter只用风格图:
        <ul>
          <li><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240208003655.png" alt="Pasted image 20240208003655.png" width="100" /> <img src="/personal_homepage/docs/attachment/Pasted%20image%2020240208003712.png" alt="Pasted image 20240208003712.png" width="100" /></li>
          <li>增加control-net的权重也是没用的, weight=0.8时出现了噪声图案</li>
        </ul>
      </li>
      <li>如果不加controlnet:
        <ul>
          <li><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240208003804.png" alt="Pasted image 20240208003804.png" width="100" /> <img src="/personal_homepage/docs/attachment/Pasted%20image%2020240208003819.png" alt="Pasted image 20240208003819.png" width="100" /></li>
        </ul>
      </li>
      <li>看起来controlnet保持了龙的外形结构, ipadpter加龙提高了龙的权重, 不过ipadapter从龙图中究竟学到了什么? 是扁平的画风吗? 只用风格图的情况下, 确实画风更接近风格图, 但是内容也更接近了
        <ul>
          <li>可以通过text prompt控制吗? 但是clip又无法理解简笔画的龙.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="comfyui-workflow">ComfyUI workflow</h2>

<p><a href="https://gist.github.com/roshameow/d952bf0157a25ab3e9e724df1449b160#file-ipadapter_controlnet-canny_loong_style_change-json"><strong>ipadapter_controlnet-canny_loong_style_change.json</strong></a></p>

<p><img src="/personal_homepage/docs/attachment/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202024-02-07%20224530.png" alt="屏幕截图 2024-02-07 224530.png" width="800" /></p>

<h2 id="comfyui难用的地方">ComfyUI难用的地方</h2>

<ul>
  <li>UI的奇怪地方
    <ul>
      <li>有参数的地方字体相对整个节点特别小, 而且没法调节. 输入text prompt的时候是根本看不清..</li>
      <li>鼠标的十字倒是特别大…</li>
    </ul>
  </li>
  <li>节点连接非常反常识
    <ul>
      <li>load节点
        <ul>
          <li>输入都在右侧, 一般会觉得应该把prompt输入模型</li>
          <li>load节点的存在就很奇怪, 为什么不把load和apply合并到一起? 或者在右侧用菜单栏切换</li>
          <li>model里面还有个model_name? 模型的名字一长全混在一起真的分不清啊…</li>
        </ul>
      </li>
      <li>preview Image节点
        <ul>
          <li>为什么不直接做成一个下拉选项…</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>为什么不能做成一个Chromium内核的本地app..
    <ul>
      <li>据说有一些方法可以打包成一个app?</li>
      <li>凡需要inpaint输入的地方, 输入方式很不方便, 能和其他专门用来绘图的软件结合就好了</li>
    </ul>
  </li>
  <li>虽然如此, 可以用节点和分享工作流就比webui好用太多了</li>
</ul>

<h2 id="其他讨论这个链接">其他讨论这个链接</h2>

<p>[1] https://zhuanlan.zhihu.com/p/664201523 里面介绍了一个换画风的工作流, 不过例子是人像, 可能本身网络对人像的理解能力更强吧?　</p>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;title&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;icon&quot;=&gt;nil}]}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="photo" /><category term="content" /><category term="ComfyUI" /><category term="ipadapter" /><category term="controlnet" /><category term="canny" /><category term="sdxl" /><summary type="html"><![CDATA[用ipadapter和canny做风格转换]]></summary></entry><entry><title type="html">神经网络attention结构理解</title><link href="https://roshameow.github.io//personal_homepage/docs/deeplearning/attention/" rel="alternate" type="text/html" title="神经网络attention结构理解" /><published>2024-02-04T00:00:00+00:00</published><updated>2024-02-08T01:13:42+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/deeplearning/attention</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/deeplearning/attention/"><![CDATA[<p>在网络中, block是把input信息转换成output信息的过程: 一般, output(position, vector)是input(token, embedding vector)的线性组合, 组合的weight (position, token) 由input和output两方关系确定. 把着重强调这种信息交互的模块叫attention.</p>
<h2 id="convolution">convolution</h2>

<p><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240207151034.png" alt="Pasted image 20240207151034.png" width="400" /></p>

<p>convolution在神经网络流行之前就已经在图像任务里广泛使用了</p>
<ul>
  <li><strong>目的:</strong>
    <ul>
      <li>对spatial information进行特征的提取和转换</li>
    </ul>
  </li>
  <li><strong>特点:</strong>
    <ul>
      <li>pixel的weight只和(input, output)的相对位置有关, 因此也是平移不变的. 对每个输出像素有影响的只有input里kernel覆盖到的区域, 也就是response field</li>
      <li>每个不同的相对位置对应vector mapping不同
        <ul>
          <li>这个符合图像处理的直观, 左边有条线和右边有条线当然要映射成不同的结果</li>
          <li>有时也会把卷积拆分成1x1 conv和spatial conv(通道无关) 的形式</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="gate-attention">gate attention</h2>

<p><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240207151122.png" alt="Pasted image 20240207151122.png" width="400" /> <img src="/personal_homepage/docs/attachment/Pasted%20image%2020240207151136.png" alt="Pasted image 20240207151136.png" width="400" /></p>

<ul>
  <li><strong>目的:</strong>
    <ul>
      <li>提取channel<a href="#ref">1</a>或spatial的权重, 让网络关注更重要的信息</li>
    </ul>
  </li>
  <li><strong>特点:</strong>
    <ul>
      <li>spatial 信息对人类来说更有可读性, 所以可以把spatial weight可视化, 看看图片什么位置更加重要</li>
    </ul>
  </li>
</ul>

<h2 id="self-attention--cross-attention">self-attention &amp; cross-attention</h2>

<p><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240207151214.png" alt="Pasted image 20240207151214.png" width="400" /></p>

<ul>
  <li><strong>目的:</strong>
    <ul>
      <li>信息的交互: 在我的图示中, 是把文字信息加入视觉信息</li>
    </ul>
  </li>
  <li><strong>特点:</strong>
    <ul>
      <li>self-attention和cross-attention结构类似, 只是变成了一种输入</li>
      <li>cross-attention是现在常见的把一种信息加入另一种信息的方法
        <ul>
          <li>也用到了图像信息和文字信息的统一形式, 即(position, embedding) , 在图像信息中, position(S=H x W)是像素或patch的位置; 文字信息中, position是token在句中的前后位置</li>
          <li>我们可以对relative weight可视化, 从而知道某个文字token和图像哪个位置最相关</li>
        </ul>
      </li>
      <li>Q,K,V(query, key, value) 的叫法是nlp搜索(匹配)任务的术语, 额, 其实我一直没法对这个望文生义…
        <ul>
          <li>其实从计算过程可以看出, 不管它们的原义, Q, K交换一下也没差</li>
          <li>query和key的embedding channel数应该相同, 为了之后计算他们relation的目的</li>
          <li>和卷积不一样, 放了更大的计算量在交互部分, 相对的, 每个input token对应vector mapping相同, 也就是用”value”部分解决了channel的映射: 这样导致同样的文字信息不论放在前面还是后面, 对应同一种output信息, 区别的只是他们权重可能不同, 或在图像的不同位置</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="attention的一些变形和优化">attention的一些变形和优化</h3>

<ul>
  <li>mulit-head attention: 映射多组QKV, 计算得到的多组output再concat
    <ul>
      <li>在实际实现过程中,  先映射成一个大的QKV, 再把Q, K, V切分是完全等价的</li>
      <li>做mulit-head可以得到多组不同的映射方式, 或减小inner_dimension(T)的大小</li>
    </ul>
  </li>
  <li>分层attention: 在做attention之前对数据排序, 切分
    <ul>
      <li>可能有些pixel之间关联不大, 这样的话把它们分到不同的bucket, 分别做attention, 可以减少计算量<a href="#ref">2</a></li>
    </ul>
  </li>
  <li>多个attention合成:
    <ul>
      <li>在最近流行的ipadapter里面也讨论了多种不同来源的数据在cross-attention 里合成的问题, 结论是分别与原数据做attention最后再合成比较好<a href="#ref">3</a>, 这个也是我们一般naive的想法</li>
    </ul>
  </li>
</ul>

<h2 id="光流估计网络">光流估计网络</h2>

<p>视觉匹配任务</p>

<h2 id="代码">代码</h2>

<p><a href="https://gist.github.com/roshameow/503ec3769d75c47b82f2a7372e8c2dab#file-attention_block-py"><strong>attention_block.py</strong></a></p>

<h2 id="reference">reference</h2>
<p><span id="ref"></span></p>

<p>[1] Woo, Sanghyun, Jongchan Park, Joon-Young Lee, and In So Kweon. “CBAM: Convolutional Block Attention Module.” arXiv, July 18, 2018. <a href="https://doi.org/10.48550/arXiv.1807.06521">https://doi.org/10.48550/arXiv.1807.06521</a>. 介绍gate attention</p>

<p>[2] Cai, Yuanhao, Jing Lin, Xiaowan Hu, Haoqian Wang, Xin Yuan, Yulun Zhang, Radu Timofte, and Luc Van Gool. “Coarse-to-Fine Sparse Transformer for Hyperspectral Image Reconstruction.” arXiv, July 10, 2022. <a href="https://doi.org/10.48550/arXiv.2203.04845">https://doi.org/10.48550/arXiv.2203.04845</a>. 介绍了Spectra-aware hashing attention block的结构</p>

<p>[3] https://github.com/tencent-ailab/IP-Adapter</p>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;title&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;icon&quot;=&gt;nil}]}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="deeplearning" /><category term="content" /><category term="network" /><category term="attention" /><category term="block" /><summary type="html"><![CDATA[在网络中, block是把input信息转换成output信息的过程: 一般, output(position, vector)是input(token, embedding vector)的线性组合, 组合的weight (position, token) 由input和output两方关系确定. 把着重强调这种信息交互的模块叫attention. convolution]]></summary></entry><entry><title type="html">常用的图像 reconstruction loss</title><link href="https://roshameow.github.io//personal_homepage/docs/deeplearning/restruction-loss/" rel="alternate" type="text/html" title="常用的图像 reconstruction loss" /><published>2024-02-01T00:00:00+00:00</published><updated>2024-02-02T20:02:40+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/deeplearning/restruction-loss</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/deeplearning/restruction-loss/"><![CDATA[<p>输出为图像的任务, 比如enhance, deblur, super-resolution等用到的loss, 主要分为以下两类</p>
<h2 id="output和label相近">output和label相近</h2>

<table>
  <thead>
    <tr>
      <th>loss</th>
      <th>公式</th>
      <th>目的</th>
      <th>特点<br /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>L1 loss</td>
      <td>$||I_1-I_2||_1$</td>
      <td>大体相近</td>
      <td>最常用的loss</td>
    </tr>
    <tr>
      <td>L2 loss<br />(MSE)</td>
      <td>$||I_1-I_2||_2$<br />或$MSE=\overline{(I_1-I_2)^2}$</td>
      <td> </td>
      <td>因为导数是线性所以计算最快</td>
    </tr>
    <tr>
      <td><a href="[https://github.com/CVMI-Lab/UHDM/blob/main/utils/common.py](https://github.com/CVMI-Lab/UHDM/blob/main/utils/common.py)">SSIM</a> <br />(stuctural similarity<br /> index measure)</td>
      <td>$\frac{2\mu_1\mu_2+C_1}{\mu_1^2+\mu_2^2+C_1}\cdot \frac{2\sigma_{12}+C_2}{\sigma_1^2+\sigma_2^2+C_2}$ <br />$\mu, \sigma$ 分别为mean, variance<br />$\sigma_{12}$是covariance<br />$C_1, C_2$ 是常数</td>
      <td>纹理相近</td>
      <td>要分patch计算<br />$C_1, C_2$ 的值要根据图像的范围调整</td>
    </tr>
    <tr>
      <td>PSNR<br />(Peak signal-<br />to-noise ratio)</td>
      <td>$-10\log_{10}(MSE(I_1,I_2))$</td>
      <td> </td>
      <td>经常是用来验证</td>
    </tr>
    <tr>
      <td><a href="[https://github.com/varun19299/deep-atrous-guided-filter/tree/master/PerceptualSimilarity](https://github.com/varun19299/deep-atrous-guided-filter/tree/master/PerceptualSimilarity)">PerceptualLoss</a></td>
      <td>一个分类网络</td>
      <td>语义相近</td>
      <td>一般用vgg16, 输入RGB图像<br />一般会用后几层的语义特征对比<br /></td>
    </tr>
    <tr>
      <td><a href="https://github.com/richzhang/PerceptualSimilarity">LPIPS</a> <br />(Learned Perceptual<br />image patch similarity)</td>
      <td> </td>
      <td> </td>
      <td>perceptualLoss+分块</td>
    </tr>
    <tr>
      <td>DeltaE</td>
      <td>转换成Lab空间的MSE</td>
      <td>颜色相近</td>
      <td> </td>
    </tr>
    <tr>
      <td>Gan Loss</td>
      <td>在训练过程中学习到的loss<br />用determinate网络表示<br /></td>
      <td>determinate网络<br />区别不出</td>
      <td> </td>
    </tr>
    <tr>
      <td>KL散度</td>
      <td> </td>
      <td> </td>
      <td> </td>
    </tr>
  </tbody>
</table>

<h2 id="output图像符合自然图像的性质">output图像符合自然图像的性质</h2>

<ul>
  <li>Total Variant(TV) loss: 图像gradient稀疏性
    <ul>
      <li>anisotropic定义: <code class="language-plaintext highlighter-rouge">$|D_x I|_1+|D_y I|_1$</code></li>
    </ul>
  </li>
</ul>

<h2 id="代码">代码</h2>

<p><a href="https://gist.github.com/roshameow/c59d5708610ae30eb4329b140ccab3a7#file-reconstruction_loss-py"><strong>reconstruction_loss.py</strong></a></p>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;title&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;icon&quot;=&gt;nil}]}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="deeplearning" /><category term="content" /><category term="loss" /><category term="image" /><summary type="html"><![CDATA[输出为图像的任务, 比如enhance, deblur, super-resolution等用到的loss, 主要分为以下两类 output和label相近]]></summary></entry><entry><title type="html">龙年春联</title><link href="https://roshameow.github.io//personal_homepage/docs/design/spring-couplets/" rel="alternate" type="text/html" title="龙年春联" /><published>2024-01-31T00:00:00+00:00</published><updated>2024-01-31T23:44:44+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/design/spring-couplets</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/design/spring-couplets/"><![CDATA[<p><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240131145110.png" alt="Pasted image 20240131145110.png" width="400" /></p>

<p>上联是“查找，回退，终止，定义，返回，格式化，删除”<br />
下联是“启动，切换，上一级，根目录，查找，替换，退出”<br />
横批是“全部历史”<br /></p>

<p>本以为会简单的制作过程, 居然感触挺多的</p>

<h2 id="制作时间轴">制作时间轴</h2>

<ul>
  <li>看到小红书上这个<a href="https://www.xiaohongshu.com/explore/65b781c8000000000c00534a">ps春联的帖子</a></li>
  <li>最开始是想做vscode和obsidian的图标</li>
  <li>做着做着, 感觉快捷键更有分享欲; 而且, 图标截图下来有清晰度和大小不一的问题</li>
  <li>直接写在红底黑字的春联底色上, 看起来太单调了
    <ul>
      <li><img src="/personal_homepage/docs/attachment/Screenshot%202024-01-31%20at%2000.55.22.png" alt="Screenshot 2024-01-31 at 00.55.22.png" width="200" /></li>
    </ul>
  </li>
  <li>想用stable diffusion生成个华丽的背景, 但是… 放弃
    <ul>
      <li>基于v1.5的模型没法理解春联, 也没法理解龙
        <ul>
          <li>生成的这种看起来挺奇怪的东西: <img src="/personal_homepage/docs/attachment/00010-3341009059.png" alt="00010-3341009059.png" width="200" /></li>
          <li>canny的controlnet没法保证文字的细节一致: <img src="/personal_homepage/docs/attachment/00008-497390711.png" alt="00008-497390711.png" width="200" /></li>
          <li>生成的龙😮‍💨: <img src="/personal_homepage/docs/attachment/00014-2368775189.png" alt="00014-2368775189.png" width="200" /></li>
        </ul>
      </li>
      <li>SDXL可以生成不错的龙, 虽然细节不对</li>
      <li>ComfyUI的工作流我用的不太熟练, 而且, 我不太了解各种风格的prompt</li>
    </ul>
  </li>
  <li>在小红书上搜索春联的版式, 看到了<a href="https://www.xiaohongshu.com/explore/63c12c19000000001f0223f0">这个帖子</a>
    <ul>
      <li>模仿失败: 卖家秀$\rightarrow$ <img src="/personal_homepage/docs/attachment/Pasted%20image%2020240131150246.png" alt="Pasted image 20240131150246.png" width="200" />  买家秀$\rightarrow$ <img src="/personal_homepage/docs/attachment/Pasted%20image%2020240131150756.png" alt="Pasted image 20240131150756.png" width="200" /></li>
      <li>我的图看起来像东南亚黑帮, 又像个祭坛</li>
      <li>当然我的颜色调的不对, 但是我也明白了这个排版不适合龙(你的内容是现代的, 但是排版却相当古老, 你究竟是什么人😂)</li>
    </ul>
  </li>
  <li>又回归了黑白红配色, 改了版式, 感觉能看了, 蛮喜庆的, 有种过年的气氛了</li>
</ul>

<h2 id="工具">工具</h2>

<ul>
  <li>pinterest搜索, 然后推荐相似风格的图片</li>
  <li>eagle存图</li>
  <li>Freeform排版
    <ul>
      <li>可惜Freeform里面没法改图片整体的颜色, 也没法旋转</li>
    </ul>
  </li>
  <li>preview的魔棒工具
    <ul>
      <li>中间想用photoshop, 但是发现我的photoshop不太熟练, 还是preview简单粗暴</li>
      <li>感慨平时不熟悉的工作流, 一着急起来压根不想用…</li>
    </ul>
  </li>
  <li>写这篇blog的时候发现markdown居然没有官方的时间轴排版, 明明应该很好做的吧</li>
</ul>

<h2 id="资源">资源</h2>

<p>字体<a href="https://www.fontspace.com/category/artistic">OpenseaThecrownismineRegular</a></p>

<p><a href="https://www.pinterest.com/pin/732116483197126476/">龙</a></p>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;title&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;icon&quot;=&gt;nil}]}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="design" /><category term="content" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">blender学习: 用graph editor做赛博车流</title><link href="https://roshameow.github.io//personal_homepage/docs/blender/blender-learning4/" rel="alternate" type="text/html" title="blender学习: 用graph editor做赛博车流" /><published>2024-01-30T00:00:00+00:00</published><updated>2024-02-02T06:14:22+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/blender/blender-learning4</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/blender/blender-learning4/"><![CDATA[<h2 id="步骤">步骤</h2>

<p>参考RuiHuang_art在<a href="https://www.bilibili.com/video/BV19C4y1S7dL/">b站的教学视频</a></p>
<ul>
  <li>world property: 加入一个CyberPunk背景的贴图
    <ul>
      <li><a href="https://docs.blender.org/manual/en/latest/render/shader_nodes/input/texture_coordinate.html">texture Coordinate</a> 选择camera坐标</li>
    </ul>
  </li>
  <li>制做车流:
    <ul>
      <li>添加一个Plane mesh</li>
      <li>在Edit编辑模式下, 按A选中, <a href="https://blog.csdn.net/BuladeMian/article/details/79625926">S+X, S+Y</a> 沿X,Y轴双向拉伸, 拉成长条形</li>
    </ul>
  </li>
  <li>shader: 目的是把车流改成带纹理, 自发光, 透明的效果
    <ul>
      <li>color:
        <ul>
          <li>下载一个灯火通明的城市俯瞰图
            <ul>
              <li><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240201173155.png" alt="Pasted image 20240201173155.png" width="100" /></li>
            </ul>
          </li>
          <li>我们只想要图中亮的地方</li>
        </ul>
      </li>
      <li>emission: 把贴图纹理连到bsdf的<a href="https://docs.blender.org/manual/en/latest/render/shader_nodes/shader/emission.html">emission</a>
        <ul>
          <li>需要调整strength: 把strength调大之后好像发的都是白光?</li>
        </ul>
      </li>
      <li>alpha(调整车流的透明度): 除了贴图本身暗的地方转成透明, 我们还想要一个两端完全透明-&gt;中心的渐变
        <ul>
          <li>根据原图亮度设置透明: 暗处透明, 亮处不透明
            <ul>
              <li>给贴图连接一个Color Ramp</li>
            </ul>
          </li>
          <li>设置两端完全透明-&gt;中心的渐变
            <ol>
              <li>用<a href="https://docs.blender.org/manual/en/latest/render/shader_nodes/input/texture_coordinate.html">texture Coordinate</a> 的uv生成平面的坐标</li>
              <li>用<a href="https://docs.blender.org/manual/en/latest/render/shader_nodes/textures/gradient.html">Gradient Texture</a> 提取x方向</li>
              <li>用 <a href="https://docs.blender.org/manual/en/latest/editors/texture_node/types/converter/color_ramp.html">Color Ramp</a> 做一个 暗-&gt;亮-&gt;暗 的渐变</li>
            </ol>
          </li>
          <li>用<a href="https://docs.blender.org/manual/en/latest/compositing/types/color/mix/mix_color.html">Mix Node</a> 把上面两个控制透明度的图融合
            <ul>
              <li>调成Multiply模式: 原视频用的是Mix模式, 把一个设成alpha, 和黑色平均..这应该和直接用Multiply等价..</li>
            </ul>
          </li>
          <li>在Material Properties把blend改成alpha blend</li>
        </ul>
      </li>
      <li>用<a href="https://docs.blender.org/manual/en/latest/render/shader_nodes/color/hue_saturation.html">Hue/Saturation/Value Node</a> 调颜色</li>
    </ul>
  </li>
  <li>加入动画:
    <ul>
      <li>在shader里给贴图加上mapping</li>
      <li>调整mapping的location
        <ul>
          <li>在Mapping的location插入关键帧</li>
          <li>在<a href="https://docs.blender.org/manual/zh-hans/dev/editors/graph_editor/fcurves/editing.html">Graph Editor</a> 里编辑: 需要快捷键T调出Select Box, 把需要更改的mapping选中</li>
          <li>选中后在Graph Editor的Shader Nodetree里编辑Y Default Value</li>
          <li>添加Modifier, 把x^1改成需要的速度0.05</li>
        </ul>
      </li>
      <li>另外用<a href="https://docs.blender.org/manual/en/latest/render/shader_nodes/input/object_info.html">Object Info</a> 获取object的位置加到location里, 给每个不同位置的车流加一个location的变化</li>
    </ul>
  </li>
  <li>复制多个车流组合</li>
  <li>结果: 当然也只能远看
    <ul>
      <li><img src="/personal_homepage/docs/attachment/car_flow.mp4" alt="car_flow.mp4" width="320" /></li>
    </ul>
  </li>
</ul>

<h2 id="shader流程">shader流程</h2>

<p><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240201172349.png" alt="Pasted image 20240201172349.png" width="800" /></p>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;title&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;icon&quot;=&gt;nil}]}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="blender" /><category term="content" /><category term="render" /><category term="shader" /><summary type="html"><![CDATA[步骤]]></summary></entry><entry><title type="html">blender学习: 用volume shader做气态行星</title><link href="https://roshameow.github.io//personal_homepage/docs/blender/blender-learning3/" rel="alternate" type="text/html" title="blender学习: 用volume shader做气态行星" /><published>2024-01-26T00:00:00+00:00</published><updated>2024-01-30T00:47:25+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/blender/blender-learning3</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/blender/blender-learning3/"><![CDATA[<h2 id="步骤">步骤</h2>

<p>参考RuiHuang_art在<a href="https://www.bilibili.com/video/BV1Aj411j7CD">b站的教学视频</a></p>
<ul>
  <li>world property: 加入一个太空背景的贴图</li>
  <li>光源：改成<a href="https://docs.blender.org/manual/en/latest/render/lights/light_object.html#sun-light">sun light</a>
    <ul>
      <li>strength改成12: 单位是 $W/m^2$</li>
      <li>volume改成3: 控制在volume shader时的影响?</li>
    </ul>
  </li>
  <li>render: 用eevee
    <ul>
      <li>设置volumetrics参数
        <ul>
          <li><strong>打开volumetric shadows</strong></li>
          <li>start, end: 相对相机的体积效果范围, 设为(3.3m-&gt;200m)</li>
          <li>tile size: volume块大小</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>shader:
    <ul>
      <li>用<a href="https://docs.blender.org/manual/en/latest/render/shader_nodes/shader/volume_principled.html">principled volume</a> : 把mesh包含的部分看成volume块
        <ul>
          <li>color:
            <ol>
              <li>用<a href="https://docs.blender.org/manual/en/latest/render/shader_nodes/input/texture_coordinate.html">texture Coordinate</a> 生成volume的坐标</li>
              <li><a href="https://docs.blender.org/manual/en/latest/render/shader_nodes/textures/image.html">Image Texture</a> 把2d的木星贴图映射到3d的volume上</li>
              <li>用<a href="https://docs.blender.org/manual/en/latest/render/shader_nodes/color/hue_saturation.html">Hue/Saturation/Value Node</a> 调颜色</li>
            </ol>
          </li>
          <li>density: 一个从里到外逐渐稀薄的球形+被纹理的影响
            <ol>
              <li>用<a href="https://docs.blender.org/manual/en/latest/render/shader_nodes/input/texture_coordinate.html">texture Coordinate</a> 生成volume的坐标
                <ol>
                  <li>object生成一个在中心的坐标系</li>
                  <li>用 <a href="https://docs.blender.org/manual/en/latest/render/shader_nodes/textures/image.html">Image Texture</a> 生成一个按照贴图纹理亮度的scale, 模拟气体的效果
                    <ul>
                      <li>用Mulitiply node的Factor node调整受纹理影响的强度</li>
                      <li>用<a href="https://docs.blender.org/manual/en/latest/render/shader_nodes/color/invert.html">invert node</a> 调整: 亮的纹理密度低(凹陷), 暗得纹理密度高(突出), 如果Fac设置成1就会反过来
                        <ul>
                          <li>Fac参数:  类似$\alpha \frac{1}{x}+(1-\alpha) x$ ?</li>
                        </ul>
                      </li>
                      <li>这部分应该可以优化, 不是用multiply 而是用其他方式做纹理的distortion?</li>
                    </ul>
                  </li>
                </ol>
              </li>
              <li>用<a href="https://docs.blender.org/manual/en/latest/render/shader_nodes/textures/gradient.html">gradient texture</a> 设置渐变的密度</li>
              <li>用 <a href="https://docs.blender.org/manual/en/latest/editors/texture_node/types/converter/color_ramp.html">Color Ramp</a> 做一个动态范围的压缩:
                <ul>
                  <li>我们想要一个(实体-&gt;气态-&gt;空) 的渐变, 中间density在0-1之间的部分, 是气态部分</li>
                  <li>调整value参数, 也就是最后输入的密度</li>
                </ul>
              </li>
            </ol>
          </li>
        </ul>
      </li>
      <li>视频里还另外加了一个底色, 但是看起来没什么影响</li>
    </ul>
  </li>
  <li>相机: Camera-&gt; Data
    <ul>
      <li>Focal Length 调大到60mm: 调整物体在镜头内的大小</li>
    </ul>
  </li>
  <li>结果: 和视频里的结果不完全一样, 不清楚是哪里的问题😮‍💨
    <ul>
      <li><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240129164109.png" alt="Pasted image 20240129164109.png" width="200" /> <img src="/personal_homepage/docs/attachment/Pasted%20image%2020240129164354.png" alt="Pasted image 20240129164354.png" width="200" /></li>
    </ul>
  </li>
</ul>

<h2 id="shader流程">shader流程</h2>

<p><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240129164505.png" alt="Pasted image 20240129164505.png" width="900" /></p>

<h2 id="texture-coordinate-的设置"><a href="https://docs.blender.org/manual/en/latest/render/shader_nodes/input/texture_coordinate.html">texture Coordinate</a> 的设置</h2>

<p>texture coordinate不同模式的区别是坐标系的原点位置不一样: generate模式的原点是在mesh的bounding box一角, 而object模式的原点是在bounding box的中心(从名字是看不出来..也没看源码验证, 都是我猜的)</p>
<ul>
  <li>用<a href="https://docs.blender.org/manual/en/latest/render/shader_nodes/textures/gradient.html">gradient texture</a> 调整密度时, 输入的是vector的模长
    <ul>
      <li>如果generate模式 , 密度最大的点在box的一角, 得到<img src="/personal_homepage/docs/attachment/Pasted%20image%2020240129151203.png" alt="Pasted image 20240129151203.png" width="120" />;</li>
      <li>用object模式  , 密度最大的点在中心, 得到<img src="/personal_homepage/docs/attachment/Pasted%20image%2020240129151233.png" alt="Pasted image 20240129151233.png" width="150" />, 也就是我们需要的行星的形态</li>
    </ul>
  </li>
  <li>而我们用<a href="https://docs.blender.org/manual/en/latest/render/shader_nodes/textures/image.html">Image Texture</a> 设置颜色的映射时, Sphere的映射方法默认是围绕点 $(0.5,0.5,0.5)$ 进行映射, 这个时候我们就需要在generate模式的坐标系下映射
    <ul>
      <li>Sphere的映射方法: <img src="https://docs.blender.org/manual/en/latest/_images/render_shader-nodes_textures_image_projection-sphere.png" alt="sphere" /></li>
      <li>把贴图<img src="/personal_homepage/docs/attachment/jupiter_01_pd.png" alt="jupiter_01_pd.png" width="200" /> 映射到<img src="/personal_homepage/docs/attachment/Pasted%20image%2020240129144125.png" alt="Pasted image 20240129144125.png" width="120" /></li>
    </ul>
  </li>
</ul>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;title&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;icon&quot;=&gt;nil}]}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="blender" /><category term="content" /><category term="render" /><category term="shader" /><summary type="html"><![CDATA[步骤]]></summary></entry><entry><title type="html">blender学习: 用displacement做动态鸟群</title><link href="https://roshameow.github.io//personal_homepage/docs/blender/blender-learning2/" rel="alternate" type="text/html" title="blender学习: 用displacement做动态鸟群" /><published>2024-01-24T00:00:00+00:00</published><updated>2024-01-26T18:49:15+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/blender/blender-learning2</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/blender/blender-learning2/"><![CDATA[<h2 id="步骤">步骤</h2>

<p>参考RuiHuang_art在<a href="https://www.bilibili.com/video/BV1ta4y1f7ui/">b站的教学视频</a></p>
<ul>
  <li>world property: background改成黑色</li>
  <li>加入鸟群所在的mesh</li>
  <li>加入鸟群贴图: 在material property里修改
    <ul>
      <li>把贴图连到alpha通道</li>
      <li>blend mode改为alpha blend</li>
    </ul>
  </li>
  <li>加入displacement: 在modifier property里修改
    <ul>
      <li>加入subdivision surface: 用<a href="https://en.wikipedia.org/wiki/Catmull–Clark_subdivision_surface">Catmull-Clark</a> 的方法细分(迭代进行, 每次加入新的点后, 会移动顶点位置, 让整体更平滑)
        <ul>
          <li>levels viewport 增加到5: 对应编辑时, 看到的细分的迭代次数, 可以调的比render低一些</li>
          <li>render增加到5: 对应最终渲染时, 看到的细分的迭代次数</li>
        </ul>
      </li>
      <li>加入displace: 根据texture的灰度值变换顶点的coordinate
        <ul>
          <li>代码可能在: https://github.com/blender/blender/blob/main/source/blender/modifiers/intern/MOD_volume_displace.cc</li>
          <li>加入texture-&gt;调整texture的属性
            <ul>
              <li>type用Clouds, 也就是<a href="https://en.wikipedia.org/wiki/Perlin_noise">Perlin noise</a> : 多个不同粒度的随机叠加
                <ul>
                  <li>Depth参数控制模糊程度: 应该是Perlin noise生成时的粒度有几层</li>
                  <li>其中nabla($\nabla$ ) 参数好像对Perlin noise没影响..</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>coordinate设置成global: 每个世界坐标位置根据texture固定displacement(如果是local就会相对于我们的mesh不动)</li>
          <li>direction: 就用默认的normal, 我们的mesh是个平面, 和用z axis一样</li>
          <li>调整stength到0.1</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>加入运镜，参考教学视频 <a href="https://www.bilibili.com/video/BV1eq4y1y7D2/?vd_source=9cd152be6fbc1b9ad36a33604a13fb6e">https://www.bilibili.com/video/BV1eq4y1y7D2</a>
    <ul>
      <li>在layout界面开启auto key之后，点击播放：这个时候鼠标只能移动，需要按G快捷键配合移动。</li>
      <li>在animation界面看效果.</li>
    </ul>
  </li>
  <li>输出视频: 这么做只是鸟的贴图在平面上波动, 不能细看.
    <ul>
      <li><img src="/personal_homepage/docs/attachment/0030-0090.mp4" alt="0030-0090.mp4" width="200" /></li>
    </ul>
  </li>
</ul>

<h2 id="其他方法">其他方法</h2>

<p>用粒子制作, 参考 <a href="https://www.bilibili.com/video/BV11i4y1r7GU/">这个b站教程</a> : 实在啰嗦我没有耐心看😂</p>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;title&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;icon&quot;=&gt;nil}]}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="blender" /><category term="content" /><summary type="html"><![CDATA[步骤]]></summary></entry><entry><title type="html">电子产品的频闪讨论</title><link href="https://roshameow.github.io//personal_homepage/docs/camera/flicker/" rel="alternate" type="text/html" title="电子产品的频闪讨论" /><published>2024-01-19T00:00:00+00:00</published><updated>2024-01-31T22:52:26+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/camera/flicker</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/camera/flicker/"><![CDATA[<p>频闪就是亮度随时间周期性变化的情况.</p>

<p>b站影视飓风关于频闪的介绍:</p>
<iframe src="//player.bilibili.com/player.html?aid=666304538&amp;bvid=BV1ua4y127pk&amp;cid=1408505034&amp;p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>

<h2 id="频闪的分类">频闪的分类</h2>

<table>
  <thead>
    <tr>
      <th>种类</th>
      <th>原因</th>
      <th>频率</th>
      <th>形态</th>
      <th>可能解决方式</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>光源</td>
      <td>交流电产生的</td>
      <td>100hz或120hz</td>
      <td>$|\cos x|$</td>
      <td>调整拍摄频率和交流电保持一致<br />调整快门时间, 让快门覆盖整数个周期</td>
    </tr>
    <tr>
      <td>LED屏幕</td>
      <td>PWM调光</td>
      <td>都有</td>
      <td>rectangular pulse<br />由duty cycle(占空比)决定</td>
      <td>除了增加快门时间, 让条纹不明显<br />目前没有什么好的解决方式</td>
    </tr>
  </tbody>
</table>

<h2 id="我手里的电子设备观测">我手里的电子设备观测</h2>

<p>按照模型, 影响拍到的pattern的有: 相机的频率$f$, 快门时间(&lt;$\frac{1}{f}$), 相位(快门扫描速度), pwm的频率, 占空比, 每行的相位.</p>

<p>我们用相机去拍屏幕的时候,拍到的理论上亮, 暗的部分都是由于相位的不同, 不会相差超过一个周期, 所以$\frac{最亮}{最暗}&lt;\frac{ceil(\frac{快门时间}{pwm周期})}{floor(\frac{快门时间}{pwm周期})}$</p>

<p><strong>条件:</strong> 用我的iphone11拍摄, 240fps的慢镜头, 快门时间不知道, 但是大概有1/500s左右?</p>

<p><strong>iphone12手机:</strong></p>

<p><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240122233401.png" alt="Pasted image 20240122233401.png" width="100" />  另一个方向: <img src="/personal_homepage/docs/attachment/Pasted%20image%2020240122233636.png" alt="Pasted image 20240122233636.png" width="200" /></p>

<ol>
  <li>在同一帧内, 在一个方向是横条纹, 把镜头换了一个方向却出现了斜向的条纹:
    <ul>
      <li>合理的解释是可能屏幕上不同行的led灯相位不同. 考虑到相机每行是同时曝光的, 相机和屏幕垂直拍摄就会出现在一行拍到了多个相位的情况, 也就是斜向的条纹</li>
    </ul>
  </li>
  <li>出现黑色和白色: 看起来亮度就只有两档, 白色比例远高于黑色
    <ul>
      <li>从黑色的清晰度来看, pwm的周期可能比快门时间要长…</li>
      <li>在最暗的情况下, 可能低位完全覆盖快门时间, 所以看到的黑色是纯黑的, 而白色是在高位时间积分得到的
        <ul>
          <li><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240123112400.png" alt="Pasted image 20240123112400.png" width="200" /></li>
        </ul>
      </li>
      <li>在网上看到的<a href="#其他手机的效果">另一个手机测试视频里</a>, 黑白的比例能看出变化</li>
    </ul>
  </li>
  <li>出现一紫三白的规律:
    <ul>
      <li>绿色灯的规律好像没法用我们的模型解释. 似乎出现了两个周期?</li>
    </ul>
  </li>
  <li>随着调亮屏幕: pattern的样子没有变, 但是渐渐不那么黑白分明了, 暗条纹变宽, 屏幕的整体亮度也有所提高:
    <ul>
      <li>pwm低位的时间在减少</li>
      <li>当高位完全覆盖快门时间, 其亮度比例大约为 $\frac{暗}{亮}\approx \frac{快门时间\% pwm周期-低位时间}{快门时间\% pwm周期}$</li>
    </ul>
  </li>
  <li>在视频中, 条纹随时间变化:
    <ul>
      <li>有评论说iphone12的屏幕频率是240, 从实验结果来说, 和240fps的镜头并不匹配, 说明频率不是240. 从条纹的清晰度推测频率在几百量级</li>
    </ul>
  </li>
</ol>

<p><strong>MacbookPro:</strong></p>

<p><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240122233738.png" alt="Pasted image 20240122233738.png" width="150" /></p>
<ul>
  <li>网上有说法是mac的pwm频率为11800, 那么, 亮-暗/亮 对比不会超过 $\approx 500/11800\approx 1/23$ , 从图像的亮暗清晰度来看, 也不能算是和我拍摄的图像矛盾吧.  另外, 改用60fps的视频模式去拍, 这个图案也是可见的. 这个数字是否可信不好判断呢…但是能肯定频率至少是上千的</li>
</ul>

<p><strong>问题</strong></p>

<ul>
  <li>能否通过所有参数已知的相机, 通过图像,视频解出pwm调光的参数?
    <ul>
      <li>TOF问题中有些类似的方法<a href="https://zhuanlan.zhihu.com/p/505843064">解相位</a></li>
      <li>如果已知屏幕信号和相机信号的auto-correlation结果, 自然知道屏幕的信号, 但是我们目前能测量的是在快门时间内积分的亮度</li>
    </ul>
  </li>
</ul>

<h2 id="strobe-light-control"><a href="https://www.urvision-tw.com/article_detail/17/4.htm">Strobe Light Control</a></h2>

<p>闪频控制提供了一个和相机曝光同时控制的光源, 可以解决高速摄影(即频率高, 快门时间短) 的频闪问题. 
奇怪的是, 有些手机CIS产品(<a href="https://www.gcoreinc.com/products/index?cid=2&amp;subcid=5">比如格科微的GC02M1B</a>), 一般工作频率是30fps, 也提供一个闪频控制的灯光, 想不通应用场景是什么.</p>

<h2 id="频闪的检测">频闪的检测</h2>

<p>在淘宝上看到的几种频闪检测仪器</p>

<p><img src="/personal_homepage/docs/attachment/IMG_8267.jpg" alt="IMG_8267.jpg" width="100" /> <img src="/personal_homepage/docs/attachment/IMG_8268.jpg" alt="IMG_8268.jpg" width="100" /> <img src="/personal_homepage/docs/attachment/IMG_8265.jpg" alt="IMG_8265.jpg" width="100" /> <img src="/personal_homepage/docs/attachment/IMG_8266.jpg" alt="IMG_8266.jpg" width="100" /></p>

<h2 id="频闪对人眼的风险">频闪对人眼的风险</h2>

<table>
  <thead>
    <tr>
      <th>参数</th>
      <th>公式</th>
      <th>含义</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>频闪百分比(波动深度, percent filcker, FPF)</td>
      <td>$\frac{A\text{(最大值)}-B\text{(最小值)}}{A+B}\cdot$ 100%</td>
      <td>表示波动的剧烈程度</td>
    </tr>
    <tr>
      <td>频闪指数(Filcker index)</td>
      <td>$\frac{A_1\text{(平均值上面积)}}{A_1+A_2\text{(平均值下面积)}}$</td>
      <td>表示频闪的不稳定程度?<br />pwm调光时降低占空比,<br /> 平均值降低, <br />频闪指数会变高</td>
    </tr>
  </tbody>
</table>

<ul>
  <li><strong>几种频闪的标准</strong>
    <ul>
      <li>IEEE std 1789-2015
        <ul>
          <li><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240122105434.png" alt="Pasted image 20240122105434.png" width="300" /></li>
        </ul>
      </li>
      <li>GB/T31831《LED室内照明技术应用要求》</li>
      <li><a href="http://222.66.64.153:8080/sdzg_admin/upload/myupload_3674.pdf">CQC1601 2016《视觉作业台灯认证技术规范》</a>: 和IEEE std 1789-2015规定相似</li>
    </ul>
  </li>
  <li><strong>理解</strong>
    <ul>
      <li>规定里只提及了频闪百分比, 没有提及频闪指数. PWM调光的频闪百分比理论上应该都是100%? 感觉这个标准考虑的只有灯光, 根本没有考虑oled屏幕</li>
      <li>规定里认为只要是&gt;3125hz的高频, 怎么样的波形都无风险</li>
    </ul>
  </li>
</ul>

<h2 id="仿真代码">仿真代码</h2>

<p><a href="https://gist.github.com/roshameow/46245e2e9772d9afb1913a27a350bc4b#file-rect_pluse-py"><strong>rect_pluse.py</strong></a></p>
<h2 id="reference">reference</h2>
<p><span id="ref"></span></p>

<p>[1] https://www.hangjianet.com/topic/15627339606370002 标准</p>

<p>[2] https://post.smzdm.com/p/ag8lgep6/ 手机测评</p>

<p>[3] https://zhuanlan.zhihu.com/p/30939047 用测量仪测试 手机上不同行的led可能相位并不同, 所以这种直接对着屏幕的测量方式应该是有问题的..</p>

<p>[4] https://discussions.apple.com/thread/254350073?sortBy=best 评论了iphone和mac的频率</p>

<p>[5] https://spectrum.ieee.org/the-iphone-12-mini-makes-me-sick-literally 评论说制造商可能让pwm频率约等于屏幕刷新率的4倍</p>

<p>[6] https://www.hangjianet.com/topic/14734042955300000 一些从硬件上处理灯光频闪的方案</p>

<p>[7] https://www.dxomark.com/flicker-the-display-affliction/ dxomark对一些手机的具体测试过程和结果: 可以理解成手机上不同行虽然相位不同, 频率都是一致的吧? 虽然不同相位相同频率的波相加可能得到更高频的信号.. 这里我们就假设相位随机, 测出的频率就是pwm频率</p>
<h2 id="其他手机的效果">其他手机的效果</h2>

<p><a href="https://zhuanlan.zhihu.com/p/414216581">这个知乎帖子看到的</a> 明显占空比变化</p>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;title&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;icon&quot;=&gt;nil}]}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="camera" /><category term="content" /><category term="test" /><category term="sci-pop" /><summary type="html"><![CDATA[频闪就是亮度随时间周期性变化的情况.]]></summary></entry><entry><title type="html">传感器颜色调制 (三) – 数据</title><link href="https://roshameow.github.io//personal_homepage/docs/data/color-moderate2/" rel="alternate" type="text/html" title="传感器颜色调制 (三) – 数据" /><published>2024-01-15T00:00:00+00:00</published><updated>2024-01-25T22:33:35+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/data/color-moderate2</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/data/color-moderate2/"><![CDATA[<p>各种颜色调制的数据对难以采集, 所以现在大部分颜色调制还是用多光谱数据仿真得到.</p>
<h2 id="多光谱数据集">多光谱数据集</h2>

<table>
  <thead>
    <tr>
      <th>数据集$\downarrow$</th>
      <th>size</th>
      <th>bands</th>
      <th>格式</th>
      <th>数量</th>
      <th>拍摄场景</th>
      <th>发布时间</th>
      <th>大小</th>
      <th>条件</th>
      <th>拍摄条件</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><a href="https://www1.cs.columbia.edu/CAVE/databases/multispectral/">CAVE</a></td>
      <td>512x512</td>
      <td>400-700nm<br />10nm steps<br />31bands</td>
      <td>.png<br />每个通道<br />分别存</td>
      <td>32</td>
      <td>实验室:<br />真假人脸<br />真假水果</td>
      <td>2008</td>
      <td>419.9MB</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td><a href="https://cave.cs.columbia.edu/projects/categories/project?cid=Computational+Imaging&amp;pid=Multispectral+Imaging+Using+Multiplexed+Illumination">CAVE1024</a></td>
      <td>1024x1024</td>
      <td> </td>
      <td> </td>
      <td>205</td>
      <td> </td>
      <td> </td>
      <td>13.06GB</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td><a href="http://vclab.kaist.ac.kr/siggraphasia2017p1/kaistdataset.html">KAIST</a></td>
      <td>2704x3376</td>
      <td>420-720nm<br />10nm steps<br />31bands</td>
      <td>.exr<br />每个图片<br />单独下载</td>
      <td>30</td>
      <td>实验室</td>
      <td>2017</td>
      <td>8.67GB</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td><a href="https://github.com/mengziyi64/TSA-Net">TSA</a></td>
      <td>660x660</td>
      <td>28通道<br />特殊</td>
      <td>.mat</td>
      <td>10(simu)<br />5(real)</td>
      <td>实验室</td>
      <td>2020</td>
      <td> </td>
      <td>simu是从<br />KAIST的数据<br />中截取的</td>
      <td> </td>
    </tr>
    <tr>
      <td><a href="https://vision.seas.harvard.edu/hyperspec/download.html">harvard</a></td>
      <td>1040x1392</td>
      <td>420-720nm<br />10nm steps<br />31bands</td>
      <td>.mat</td>
      <td>50(自然光)<br />27(人工光)</td>
      <td>场景丰富</td>
      <td>2011</td>
      <td>约7GB</td>
      <td>research-only</td>
      <td>商业相机:<br />Nuance FX, CRI Inc<br />liquid crystal tunable filter</td>
    </tr>
    <tr>
      <td><a href="https://github.com/boazarad/ARAD_1K">ARAD 1K</a></td>
      <td>512x482</td>
      <td>400-700nm<br />10nm steps<br />31bands</td>
      <td>.mat-v7.3</td>
      <td>900(train)<br />50(val)<br />50(test)</td>
      <td>场景丰富</td>
      <td>2022</td>
      <td>约21GB<br />压缩包</td>
      <td>需要注册</td>
      <td> </td>
    </tr>
    <tr>
      <td>TokyoTech<br /><a href="http://www.ok.sc.e.titech.ac.jp/res/MSI/MSIdata31.html">31-band</a></td>
      <td>不固定<br />500~2k<br />左右</td>
      <td>420-720nm<br />10nm steps<br />31bands</td>
      <td>.mat</td>
      <td>30</td>
      <td>色卡(齐全)<br />布料,<br />蝴蝶<br />局部特写</td>
      <td>2015</td>
      <td>2.97GB</td>
      <td>research-only<br />redistribute-<br />prevent</td>
      <td> </td>
    </tr>
    <tr>
      <td>TokyoTech<br /><a href="http://www.ok.sc.e.titech.ac.jp/res/MSI/MSIdata59.html">59-band</a></td>
      <td>512x512</td>
      <td>420-1000nm<br />10nm steps<br />59bands</td>
      <td>.mat-v7.3</td>
      <td>16</td>
      <td>类似<br /></td>
      <td>2019</td>
      <td>1.7GB</td>
      <td>research-only<br />redistribute-<br />prevent</td>
      <td> </td>
    </tr>
    <tr>
      <td><a href="http://www2.cmp.uea.ac.uk/Research/compvis/MultiSpectralDB.htm">CMP_UEA</a></td>
      <td>不固定<br />200~400<br />左右</td>
      <td>400-700nm<br />10nm steps<br />31bands</td>
      <td>.mat<br />每个图片<br />单独下载</td>
      <td>23</td>
      <td>色卡,<br />广告包装</td>
      <td>2004</td>
      <td>566.6MB</td>
      <td> </td>
      <td> </td>
    </tr>
  </tbody>
</table>

<h2 id="格式读取和注意事项">格式读取和注意事项</h2>

<ol>
  <li><strong>从网页抓数据</strong>
    <ul>
      <li>因为数据是分散在网页上的, 需要用python从网页上抓.exr图片
        <ul>
          <li>问题: 用<code class="language-plaintext highlighter-rouge">requests.get(absolute_url).content</code> , 这么下载下来的文件可能会有不全
            <ul>
              <li>我批量下载kaist数据时, 就有一张图片错误, 还好.exr可以预览, 发现不正常的图片再单独下载下来就好</li>
              <li>在批量下载CMP_UEA网页上的.mat文件时, 也出错了, 但是.mat文件没法预览, 等到读数据才发现</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>读.exr数据</strong>
    <ul>
      <li>网上的<a href="https://github.com/jamesbowman/openexrpython.git">读exr的python包</a>都有些问题, 而尴尬的是OpenEXR的官方放出了要做官方python binding的消息, 截止目前还没发布. 需要我们自己按如下流程操作: 写调用OpenEXR的C++代码-&gt;用pybind11编成.so库-&gt;在python里面调用
        <ul>
          <li>参考<a href="https://openexr.com/en/latest/API.html#the-openexr-api">OpenEXR的API文档</a> : 一定要把所有channel都在datawindow 里面排好一次性读</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>读.mat数据</strong>
    <ul>
      <li>.mat数据有两种: 老版的.mat和新的v7.3格式的.mat, 不实际读的话, 没法知道究竟是哪种. 我还没找到可以先提取.mat的metadata的方法</li>
      <li>老版的.mat可以用<code class="language-plaintext highlighter-rouge">scipy.io.loadmat</code> 读</li>
      <li>v7.3其实就是H5DF格式, 和.h5文件一样, 可以用<code class="language-plaintext highlighter-rouge">h5py</code> 读. 看到一些地方是建议用 <a href="https://pypi.python.org/pypi/hdf5storage">hdf5storage</a> 这个包(不光可以读, 还可以存成v7.3格式).</li>
      <li>.mat数据好像没法看数据类型..</li>
    </ul>
  </li>
  <li><strong>怎么正确的在pytorch里加载</strong>
    <ul>
      <li>目前是存在dataset的一个list里面
        <ul>
          <li>一定不能把所有image存在一个大的<code class="language-plaintext highlighter-rouge">ndarray</code>里面, 我们的数据都比较大, 很有可能会内存不够</li>
          <li>存在list里面是不受内存的限制吗?在训练过程中可以及时的动态加载吗? 还没有研究</li>
          <li>我们的图片大, 而训练需要的crop_size比较小, 一个想法是先把图片切分成多个小的patch, 这样是否就不用加载一整张图片了?</li>
        </ul>
      </li>
      <li>在第一次读数据时存成.h5文件的cache, 之后从.h5文件读取
        <ul>
          <li>因为数据集中的波长和我们要的channel可能不一样, 我们的channel是用<code class="language-plaintext highlighter-rouge">interp1d</code> 插值得到的, 这个插值的过程比较浪费时间, 所以至少需要把插值后的结果存下来</li>
          <li>.h5文件读出来是<code class="language-plaintext highlighter-rouge">ndarray</code> 格式, 要转成tensor使用, 不过这个过程是很快的</li>
          <li><strong>为什么不存成.pkl</strong>
            <ul>
              <li>.pkl读起来比.h5慢, 在我的mac M1上速度慢了一倍.  .pkl 倒是可以直接存tensor的, 而且存数据更快.</li>
              <li><a href="https://docs.python.org/3/library/pickle.html">.pkl</a>可以做的操作比较多, 如果在网上把.pkl发给别人, 对方可能不敢打开.. 所以一般不会用.pkl分发大文件</li>
              <li>.pkl适合自己开发时存一些中间结果</li>
            </ul>
          </li>
          <li>为什么不是一张图片一个文件而是把全部list存在一起? 没有测试, 可能速度是差不多的吧?</li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<h2 id="其他有用的资源">其他有用的资源</h2>

<ul>
  <li><a href="https://github.com/caiyuanhao1998/BiSCI">BiSCI的repo</a> 里提供了CAVE512, CAVE1024, KAIST, TSA的下载链接, 是已经转成28通道的, .mat格式
    <ul>
      <li>其中kaist的数据load超慢.. 可能是因为把原本的16bit的half长度存成了64bit的complex?</li>
      <li>我想kaist里面存half格式的exr可能也是出于缩小空间的考虑.. OpenEXR本身读起来挺快的</li>
    </ul>
  </li>
  <li><a href="https://github.com/colour-science/colour">color-science</a> : 里面包含各种光学方面的标准, 对各种色彩空间转换, 可视化很有用</li>
</ul>

<h2 id="代码">代码</h2>

<ul>
  <li>exr: <a href="https://gist.github.com/roshameow/c2710dedf5ab067517d622b2a7ed4679#file-readexr-cpp"><strong>readexr.cpp</strong></a></li>
  <li>网页抓图片: <a href="https://gist.github.com/roshameow/30aae94815c6b8ddb2253191d4f86649#file-download_kaist-py"><strong>download_kaist.py</strong></a></li>
  <li>读数据集: <a href="https://gist.github.com/roshameow/30aae94815c6b8ddb2253191d4f86649#file-multisepc_load-py"><strong>multisepc_load.py</strong></a></li>
  <li>多光谱图片转成RGB显示: <a href="https://gist.github.com/roshameow/30aae94815c6b8ddb2253191d4f86649#file-multispec2rgb-py"><strong>multispec2rgb.py</strong></a>
    <ul>
      <li>用TSA的图片测试, 转出的RGB有偏色, 可能是因为只对应了channel的主波长的XYZ, 没有在光谱上积分吗?</li>
    </ul>
  </li>
</ul>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;title&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;icon&quot;=&gt;nil}]}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="data" /><category term="content" /><category term="dataset" /><category term="pytorch" /><category term="script" /><summary type="html"><![CDATA[各种颜色调制的数据对难以采集, 所以现在大部分颜色调制还是用多光谱数据仿真得到. 多光谱数据集]]></summary></entry><entry><title type="html">aperture衍射模型 (二)</title><link href="https://roshameow.github.io//personal_homepage/docs/simulation/diffraction1/" rel="alternate" type="text/html" title="aperture衍射模型 (二)" /><published>2024-01-11T00:00:00+00:00</published><updated>2024-01-15T01:10:47+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/simulation/diffraction1</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/simulation/diffraction1/"><![CDATA[<h2 id="仿真">仿真</h2>

<p><img src="/personal_homepage/docs/attachment/circle_diffraction.mp4" alt="circle_diffraction.mp4" width="200" /> <img src="/personal_homepage/docs/attachment/square_diffraction.mp4" alt="square_diffraction.mp4" width="200" /></p>
<ol>
  <li>在aperture区域均匀采样，发现Frensel pattern是很难出现的，在每个像素对应sample数不足1000的时候，基本观测不到。。应该是因为在那个范围内结果受$(x^\prime,y^\prime)$ 的位置影响更大, 也就表现出更大的随机性。</li>
  <li>确实可以看到sensor到aperture距离增大后, pattern变化的全过程。</li>
  <li>在一个网上找的<a href="https://www.falstad.com/diffraction/">Frensel diffraction仿真</a> java代码里面，作者也是通过先把部分积分形式通过公式运算先化简之后做的仿真<a href="#ref">1</a>，没有用原始的传播公式。</li>
  <li>如果用全光谱的光源，Fraunhofer衍射中因为波长影响, 看起来的效果像是从中间不同颜色的光被diffuse了，我看一些3d建模制作里说的衍射，一般是指这种效果，可以看<a href="https://www.bilibili.com/video/BV1C5411E78d/">b站上一个用blender仿这种效果的视频</a>
    <ul>
      <li>另外判断天然珍珠和人造珍珠的区别的一种方法，也是看天然珍珠里面有微小的结构可以把不同波长的光区分出来的效果</li>
      <li>blender的cycles用的是粒子模型, 没有相位变化，可能没法直接得到衍射</li>
    </ul>
  </li>
</ol>

<h2 id="应用">应用</h2>

<ol>
  <li><strong>在相机系统中，我们关心成像的分辨率</strong>：用Fraunhofer的模型估算相机的resolution
    <ul>
      <li>相机aperture参数：这两个都是dimensionless版本的aperture
        <ul>
          <li><a href="https://en.wikipedia.org/wiki/F-number">f-number</a>: $N=\frac{f}{D}=\frac{f}{2a}$
            <ul>
              <li>相机焦距$f=z$, 光圈直径$D=2a$</li>
            </ul>
          </li>
          <li><a href="https://en.wikipedia.org/wiki/Numerical_aperture">numerical aperture</a> $NA=\frac{a}{\sqrt{f^2+a^2}}=\frac{1}{\sqrt{4N^2+1}}\approx \frac{1}{2N}$</li>
        </ul>
      </li>
      <li>第一圈黑环处距中心距离，认为是相机的optical resolution:
        <ul>
          <li><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240112160824.png" alt="Pasted image 20240112160824.png" width="300" /></li>
          <li>此时是$J_1(x)$ 的第一个0点， $J_1(ka\sin\theta)=0$，$ka\sin\theta\approx 3.83$ ，即$\sin\theta\approx \frac{3.83}{ka}=\frac{3.83\lambda}{2\pi a}=1.22\frac{\lambda}{D}$</li>
          <li>得到optical resolution $q=r_1\sin\theta\approx \sqrt{f^2+a^2}\sin\theta=\frac{a}{NA}\sin\theta\approx \frac{a}{NA}\frac{1.22\lambda}{2a}=\frac{0.61\lambda}{NA}$</li>
          <li>理论上，numberical aperture越大，optical resolution越小，分辨率越高(也就是衍射更不明显)</li>
        </ul>
      </li>
      <li>相机的成像清晰度指标：MTF
        <ul>
          <li><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240112154649.png" alt="Pasted image 20240112154649.png" width="400" /></li>
          <li><a href="https://en.wikipedia.org/wiki/Point_spread_function">PSF</a>(intensity function): $PSF=|G(p,q)|^2=|F(g(x^\prime,y^\prime))|^2$
            <ul>
              <li>PSF的单位是 (distance(mm),intesity)</li>
              <li>对于圆形aperture：$PSF=\frac{J_1^2(ka\sin\theta)}{(ka\sin\theta)^2}$ , PSF的零点也就是$J_1$ 的零点</li>
              <li>生成图像的image: $I=PSF*\text{scene}$ 是场景和PSF的卷积</li>
            </ul>
          </li>
          <li><a href="https://en.wikipedia.org/wiki/Optical_transfer_function">MTF</a>: $MTF=F(PSF)$
            <ul>
              <li>MTF的单位是（frequency(lines/mm),response(%)）</li>
              <li>$\frac{J_1^2(x)}{x^2}$ 的Fourier变化是个连续下降函数</li>
              <li>根据 <a href="https://en.wikipedia.org/wiki/Fourier_transform#Applications">Fourier transform</a> time scaling的性质：因为PSF的零点$PSF_0\propto \frac{\lambda}{NA}$ , 得到MTF的cutoff点 $c(MTF)\propto \frac{NA}{\lambda}$  (即频率更高的物体就看不清了)</li>
              <li>MTF是个衡量成像系统分辨率的常用指标，不光衍射，所有成像过程，包括软件做的deblur等操作都可以用这个指标衡量。</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>用PSF做图像调制</strong> 
    <ul>
      <li>从PSF的表达式可以看出, PSF受距离, 波长(光的颜色), 位置影响, 可以在相机系统中主动利用这些信息</li>
    </ul>
  </li>
</ol>

<h2 id="代码">代码</h2>

<p><a href="https://gist.github.com/roshameow/7d44196b703ade645b34b164d779cdfe#file-diffraction_simulation-py"><strong>diffraction_simulation.py</strong></a></p>

<p><a href="https://gist.github.com/roshameow/7d44196b703ade645b34b164d779cdfe#file-pst_bessel-py"><strong>pst_bessel.py</strong></a></p>

<h2 id="其他讨论这个的链接">其他讨论这个的链接：</h2>
<p><span id="ref"></span>
仿真</p>

<p>[5] http://www.dauger.com/fresnel/</p>

<p>[6] Dauger, Dean E. “Simulation and Study of Fresnel Diffraction for Arbitrary Two-Dimensional Apertures.” <em>Computers in Physics</em> 10 (November 1, 1996): 591–604. <a href="https://doi.org/10.1063/1.168584">https://doi.org/10.1063/1.168584</a>.</p>

<p>相机resolution</p>

<p>[7] https://www.microscopyu.com/tutorials/imageformation-airyna (包含一个<a href="https://en.wikipedia.org/wiki/Airy_disk">Airy disk</a> 影响清晰度的演示)</p>

<p>[8] https://www.iasj.net/iasj/download/8ab5ecbce0ead154</p>

<p>[9] https://micro.magnet.fsu.edu/primer/java/mtf/airydisksize/index.html</p>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;title&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;icon&quot;=&gt;nil}]}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="simulation" /><category term="content" /><category term="simulation" /><category term="sensor" /><category term="physics" /><summary type="html"><![CDATA[仿真]]></summary></entry></feed>