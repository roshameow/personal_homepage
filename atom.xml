<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://roshameow.github.io//personal_homepage/atom.xml" rel="self" type="application/atom+xml" /><link href="https://roshameow.github.io//personal_homepage/" rel="alternate" type="text/html" /><updated>2024-04-25T07:43:37+00:00</updated><id>https://roshameow.github.io//personal_homepage/atom.xml</id><title type="html">Liu, Wen’s Home Page</title><subtitle>Work, Experiments and Ideas.</subtitle><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;title&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;icon&quot;=&gt;nil}]}</name><email>w.liuatnk@gmail.com</email></author><entry><title type="html">小面积光流传感器算法测试 (二)</title><link href="https://roshameow.github.io//personal_homepage/docs/algorithm/optical_flow_train2/" rel="alternate" type="text/html" title="小面积光流传感器算法测试 (二)" /><published>2024-04-25T00:00:00+00:00</published><updated>2024-04-25T22:34:30+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/algorithm/optical_flow_train2</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/algorithm/optical_flow_train2/"><![CDATA[<p>正样本: 和patch距离&lt;0.5的patch
负样本: 和patch有部分相同pattern </p>

<h3 id="torch-grid-sample">torch grid sample</h3>

<p>torch grid 的采样方式有align_corners=True和align_cornes=False两种</p>
<ul>
  <li><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240415160510.png" alt="Pasted image 20240415160510.png" width="250" /></li>
  <li>转换关系
    <ul>
      <li>pixel -&gt; grid(align_corner=True): <code class="language-plaintext highlighter-rouge">x=x/(n-1)*2-1</code></li>
      <li>grid(align_corner=True) -&gt; pixel: <code class="language-plaintext highlighter-rouge">x=</code></li>
    </ul>
  </li>
</ul>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;title&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;icon&quot;=&gt;nil}]}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="algorithm" /><category term="content" /><summary type="html"><![CDATA[正样本: 和patch距离&lt;0.5的patch 负样本: 和patch有部分相同pattern ]]></summary></entry><entry><title type="html">stable-diffusion中k-sampling的不同版本</title><link href="https://roshameow.github.io//personal_homepage/docs/algorithm/stable-diffusion7/" rel="alternate" type="text/html" title="stable-diffusion中k-sampling的不同版本" /><published>2024-04-16T00:00:00+00:00</published><updated>2024-04-16T23:56:49+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/algorithm/stable-diffusion7</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/algorithm/stable-diffusion7/"><![CDATA[]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;title&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;icon&quot;=&gt;nil}]}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="algorithm" /><category term="content" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">小面积光流传感器算法测试 (一)</title><link href="https://roshameow.github.io//personal_homepage/docs/algorithm/optical-flow-train/" rel="alternate" type="text/html" title="小面积光流传感器算法测试 (一)" /><published>2024-04-11T00:00:00+00:00</published><updated>2024-04-25T23:31:12+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/algorithm/optical-flow-train</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/algorithm/optical-flow-train/"><![CDATA[<p>大概分为: preprocess -&gt; instant flow compute -&gt; filter correct 三个步骤</p>

<h2 id="计算连续两帧的光流">计算连续两帧的光流</h2>

<table>
  <thead>
    <tr>
      <th>算法</th>
      <th>改进</th>
      <th>公式</th>
      <th>效果</th>
      <th>存储占用</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>LK<br /><a href="https://en.wikipedia.org/wiki/Lucas–Kanade_method">Lucas-Kanade</a></td>
      <td> </td>
      <td>对图像$I$ 的每个像素, 有 $\frac{\partial I}{\partial x}dx+\frac{\partial I}{\partial y}dy=\frac{dI}{dt}$ <br />即, $\begin{bmatrix}dx \\ dy\end{bmatrix}=\begin{bmatrix}\frac{\partial I}{\partial x}\frac{\partial I}{\partial x} &amp; \frac{\partial I}{\partial x}\frac{\partial I}{\partial y} \\ \frac{\partial I}{\partial x}\frac{\partial I}{\partial y} &amp; \frac{\partial I}{\partial y}\frac{\partial I}{\partial y} \end{bmatrix}^{-1}\begin{bmatrix}\frac{\partial I}{\partial x}\frac{dI}{dt} \\ \frac{\partial I}{\partial y}\frac{dI}{dt}\end{bmatrix}=H^{-1}\begin{bmatrix}\frac{\partial I}{\partial x}\frac{dI}{dt} \\ \frac{\partial I}{\partial y}\frac{dI}{dt}\end{bmatrix}$<br />其中$\frac{\partial I}{\partial x}\approx I(x+1,y)-I(x,y)$  <br /></td>
      <td>只在光流在0-1附近有效(即subpixel的尺度)<br /><br />和$\frac{\partial I}{\partial x}$ 的计算方式有关</td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td>LK_MEAN_NORM<br /></td>
      <td>把$\frac{dI}{dt}$ 改为 $\frac{dI}{dt}-mean(\frac{dI}{dt})$<br />($mean(\frac{dI}{dt})$ 表示整体亮度的变化, 和光流无关)</td>
      <td>解决亮度变化的情况</td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td>@pre_shift</td>
      <td>先对齐到上一次计算的光流位置</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td>@compute_pyd</td>
      <td>把图像分为多层下采样计算</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td>LK-DIS<br />dense inverse search<br />结合两种方法</td>
      <td>分块计算<br />- 每块都迭代计算光流<br />  - 先按整像素移动到ssd最小的位置<br />   - 再用LK_MEAN_NORM的方法不断微调计算光流<br />       - 按照光流计算的方向对齐, 计算ssd<br />        - ssd不再变小就跳出循环<br />用所有分块的平均光流作为最终结果</td>
      <td>比LK更稳定</td>
      <td>需要所有patch的Hessian, dx, dy矩阵<br /><br />对齐patch时移动patch的中间结果<br /></td>
    </tr>
    <tr>
      <td>像素neighbor patch比对<br /></td>
      <td> </td>
      <td>$diff=dist(F(I_1(x,y))-F(I_2(x+dx,y+dy)))$ <br /><br />$argmin_{(dx,dy)}\sum_{x,y}dist(F(I_1(x,y))-F(I_2(x+dx,y+dy)))$ <br />其中, $F$ 是特征提取器, dist是距离函数</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>: 特征</td>
      <td>SAD_SIMPLE<br />sum of average differences</td>
      <td>$sad=L_1(diff)$<br />或者$sad=L_1(diff-mean(diff))$</td>
      <td>第二种更好</td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td>SAD_SUBPIXEL<br />用多项式拟合sad平面</td>
      <td>$P(x+\Delta x)\approx P(x)+P^\prime(x)\Delta x+\frac{P^{\prime\prime}(x)}{2}\Delta x^2$ <br />即, $\Delta x=-\frac{P^\prime(x)}{P^{\prime\prime}(x)}$ <br />其中 $P^\prime(x)\approx \frac{dist(x+1,y)-dist(x-1,y)}{2}$ , $P^{\prime\prime}$ 类似<br />-  dx, dy分开计算<br />- 直接用多项式代入也是等价的</td>
      <td>没用<br />可能是因为subpixel的部分不符合多项式</td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td>SAD_BLUR</td>
      <td>先对图像做2x2的blur<br />😮‍💨因为觉得不准的地方是不是因为刚好对齐的地方在像素的中间</td>
      <td>垃圾<br /></td>
      <td> </td>
    </tr>
    <tr>
      <td>: 距离<br /></td>
      <td>SSD_SIMPLE<br />sum of squared differences<br /></td>
      <td>$ssd=Var(diff)$</td>
      <td>比SAD稳定</td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td>SAD_BINARY</td>
      <td>$hamming=popcount(ref\ \hat\ current)$<br />其中图像是binary(image-mean(image))</td>
      <td>效果明显变差<br />binary有没有必要呢?</td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td>BINARY_FEATURE</td>
      <td>用gradient descent训练一个特征</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td>BINARY_FEATURE_BOOST</td>
      <td>用adaboost训练一个特征</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>: neighbor</td>
      <td>SAD_SIMPLE_CROSS<br />neighbor变成cross形状, 节省一些存储</td>
      <td> </td>
      <td>节省存储&amp;计算<br /></td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td>SAD_SPIRAL</td>
      <td>- 从上一次的光流位置向外螺旋状计算<br />- 遇到更小的distance提前结束循环</td>
      <td>节省存储&amp;计算<br />和LK效果类似<br />应该是都用了preshift的原因</td>
      <td> </td>
    </tr>
    <tr>
      <td><a href="https://en.wikipedia.org/wiki/Phase_correlation">phase_correlation</a></td>
      <td> </td>
      <td>$dx,dy=argmax_{x,y} F^{-1}(\frac{F(I_1)\cdot \bar{F(I_2)}}{|F(I_1)\cdot \bar{F(I_2)}|})$ <br />相当于提取图像的phase部分<br />然后用cross correlation的dist</td>
      <td><strong>效果最好</strong> <br />另外如果不做phase correlation,<br />直接做cross correlation<br />效果并不好</td>
      <td>需要ref和current的fft频域</td>
    </tr>
  </tbody>
</table>

<h2 id="测试">测试</h2>

<ul>
  <li>仿真数据: 距离仿真数据的位置</li>
  <li>真实数据:
    <ul>
      <li>稳定性: 电机带动匀速转动</li>
      <li>响应速度: 电机急停急转</li>
    </ul>
  </li>
</ul>

<p>一些结果$\downarrow$</p>

<p><img src="/personal_homepage/docs/attachment/result_compare_2000_test.png" alt="result_compare_2000_test.png" width="400" />  <img src="/personal_homepage/docs/attachment/Pasted%20image%2020240425142720.png" alt="Pasted image 20240425142720.png" width="400" /></p>

<ul>
  <li>对于测试场景来说, 所有subpixel的方法似乎都没有必要</li>
  <li>phase correlation &gt; 训练得到的几种特征平移匹配 $\approx$ sad(-mean(diff)) $\approx$ ssd &gt; sad binary » sad spiral $\approx$ LK
    <ul>
      <li><strong>对图像做detail的提取(如image-blur(image,(6x6)))</strong> 之后, sad的结果得到提高, 和训练得到的特征类似</li>
      <li>几个特征平移方法出现错误的地方可能因为超出了搜索范围导致的</li>
      <li>平移后diff的例子: <img src="/personal_homepage/docs/attachment/Pasted%20image%2020240425151045.png" alt="Pasted image 20240425151045.png" width="100" /></li>
    </ul>
  </li>
  <li>响应的测试: 除了lk会反应慢一些, 其他都在可以接受范围内</li>
  <li>这真是个特别枯燥的工作😑, 很多方法觉得, 啊, 应该不会有效果的, 但还是想着, 坚持着写出来测一下究竟差在哪里吧</li>
</ul>

<h2 id="几种opencv支持的方法测试">几种opencv支持的方法测试</h2>

<ul>
  <li>代码: <a href="https://gist.github.com/roshameow/7843f23826791c152ab2ed8c169590b9#file-flow_opencv-py"><strong>flow_opencv.py</strong></a></li>
  <li>复现修改opencv的算法, 有时需要opencv代码的中间结果
    <ul>
      <li>因为我没有下载opencv完整的源码, 而是用pip装opencv的库</li>
      <li>用<code class="language-plaintext highlighter-rouge">pkg-config --cflags --libs opencv4</code> 查看opencv的lib,include path</li>
      <li>单独把要复现的函数复制一个.cpp, 就可以随便打印中间结果了</li>
    </ul>
  </li>
</ul>

<h2 id="有用的链接">有用的链接</h2>

<p>[1] https://dsp.stackexchange.com/questions/16995/image-reconstructionphase-vs-magnitude 关于图像phase部分的提问</p>

<p>[2] https://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/OWENS/LECT7/node2.html 解释图像边缘部分的<a href="https://en.wikipedia.org/wiki/Phase_congruency#:~:text=Phase%20congruency%20is%20a%20measure,changes%20in%20illumination%20and%20contrast.">Phase congruency</a> 更强</p>

<p><img src="https://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/OWENS/LECT7/img31.gif" alt="原图" /> <img src="https://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/OWENS/LECT7/img33.gif" alt="PC图" /> 用Phase Congruency提取图像边缘的结果</p>

<p>[3]  <a href="https://en.wikipedia.org/wiki/Phase_stretch_transform#:~:text=Phase%20stretch%20transform%20(PST)%20is,time%20stretch%20dispersive%20Fourier%20transform.">Phase stretch Transform</a></p>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;title&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;icon&quot;=&gt;nil}]}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="algorithm" /><category term="content" /><summary type="html"><![CDATA[大概分为: preprocess -&gt; instant flow compute -&gt; filter correct 三个步骤]]></summary></entry><entry><title type="html">画一个环形的重复图样</title><link href="https://roshameow.github.io//personal_homepage/docs/geometry/ring-pattern/" rel="alternate" type="text/html" title="画一个环形的重复图样" /><published>2024-04-10T00:00:00+00:00</published><updated>2024-04-11T03:16:48+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/geometry/ring-pattern</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/geometry/ring-pattern/"><![CDATA[<p><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240410165502.png" alt="Pasted image 20240410165502.png" width="200" /></p>

<ul>
  <li>公司需要画一个这样的图像, 本来想法是先画一个方形渐变, 复制需要的份数, 极坐标变换.</li>
  <li>想全部在photoshop里面完成的, 但是发现不知道怎么复制</li>
  <li>转向了python的pil画渐变和复制</li>
  <li>用photoshop的极坐标变换和python opencv都可以完成
    <ul>
      <li>opencv是图到图的变换</li>
    </ul>
  </li>
  <li>又想到直接画出2d的坐标meshgrid再应用变换好像更容易?</li>
</ul>

<p>代码: <a href="https://gist.github.com/roshameow/24e05cc4f336ca61c93ec2bc8e75ae39#file-ring-py"><strong>ring.py</strong></a></p>

<h2 id="其他链接">其他链接</h2>

<p>[1] https://zhuanlan.zhihu.com/p/518229060 ps插件</p>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;title&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;icon&quot;=&gt;nil}]}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="geometry" /><category term="content" /><category term="python" /><category term="photoshop" /><category term="opencv" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">blender学习: 几何节点做摄像头移动阵列</title><link href="https://roshameow.github.io//personal_homepage/docs/blender/blender-learning11/" rel="alternate" type="text/html" title="blender学习: 几何节点做摄像头移动阵列" /><published>2024-04-08T00:00:00+00:00</published><updated>2024-04-10T23:13:14+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/blender/blender-learning11</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/blender/blender-learning11/"><![CDATA[<p>参考<a href="https://www.bilibili.com/video/BV1t5411v7b1/">这个教学</a></p>

<h3 id="建模">建模</h3>

<p>直接复制作者的模型和材质</p>
<ul>
  <li>箭头</li>
  <li>摄像头</li>
</ul>

<h3 id="步骤">步骤:</h3>

<ul>
  <li>制作摄像头阵列: 用<a href="https://docs.blender.org/manual/en/latest/modeling/geometry_nodes/instances/instance_on_points.html#">Instance on Points节点</a>
    <ul>
      <li>添加一个Plane mesh, 在modifier添加几何节点</li>
      <li>在Points的地方制作一个meshgrid:
        <ul>
          <li>用Grid节点调整Grid大小和距离: 相对plane平面</li>
          <li>用<a href="https://docs.blender.org/manual/en/latest/render/shader_nodes/vector/vector_rotate.html#">Vector Rotate节点</a> 批量调整Plane里面顶点的位置</li>
        </ul>
      </li>
      <li>把摄像头主体和摄像机臂分别设置成为Plane顶点的instance: 用Join Geometry节点连接</li>
    </ul>
  </li>
  <li>设置摄像头主体追踪箭头: 分为 箭头在xz平面平移(看向箭头)和箭头在x轴旋转(跟随箭头点头) 两部分
    <ul>
      <li>平移-&gt;关于y轴旋转:
        <ul>
          <li>计算Plane里面顶点到箭头的vector: 这里面<a href="https://docs.blender.org/manual/en/latest/modeling/geometry_nodes/geometry/read/position.html">Position节点</a>给出的是Plane每个顶点的location</li>
          <li>用<a href="https://docs.blender.org/manual/en/latest/modeling/geometry_nodes/utilities/rotation/align_euler_to_vector.html">Align Euler to Vector节点</a> 设置成关于y轴旋转(因为y轴是摄像机头本来的朝向?)</li>
        </ul>
      </li>
      <li>旋转-&gt; 旋转:
        <ul>
          <li>提取箭头Rotation的x轴反向旋转, 用Rotate Euler节点的local模式添加到Plane的Rotation(plane每个顶点的rotation)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>制作箭头绕圈和点头动画:
    <ol>
      <li>绕圈: 让箭头围绕一个圈移动
        <ul>
          <li>添加一个Circle曲线</li>
          <li>给箭头添加Constraint-&gt; <a href="https://docs.blender.org/manual/en/4.1/animation/constraints/relationship/follow_path.html">Follow Path</a>
            <ul>
              <li>Target选择刚才的Circle</li>
              <li>Option+G清除位置: 加了follow path constraint之后, position会变成相对path的, 所以要清除position保证箭头在path上</li>
              <li>在起始和结束打上关键帧
                <ul>
                  <li>起始设置offset=0</li>
                  <li>结束设置offset=100: 按照文档的说法似乎应该是0到1? 但是实际用的是0到100?</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>点头: 打rotation的关键帧
        <ul>
          <li>打3个关键帧: 原始位置-&gt; 最低位置 -&gt; 原始位置
        - 在graph editor调整运动曲线</li>
        </ul>
      </li>
    </ol>
  </li>
  <li>结果: 感觉这是个很有用的互动功能, 但是这个场景有点意义不明? 是在表达什么?
    <ul>
      <li><img src="/personal_homepage/docs/attachment/camera_follow.mp4" alt="camera_follow.mp4" width="400" /></li>
    </ul>
  </li>
</ul>

<h3 id="geometry-nodes">geometry nodes</h3>

<p><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240410145231.png" alt="Pasted image 20240410145231.png" width="800" /></p>
<h3 id="用到的blender的一些快捷键">用到的blender的一些快捷键</h3>

<ul>
  <li>Option+G/S/R (<a href="https://docs.blender.org/manual/en/latest/scene_layout/object/editing/clear.html#">清除</a> )
    <ul>
      <li>把object位置变成默认的0</li>
    </ul>
  </li>
  <li>G/S (移动/缩放)的多种功能:
    <ul>
      <li>可以在Timeline移动缩放关键帧</li>
      <li>可以在graph editor里面调整运动曲线</li>
    </ul>
  </li>
  <li>I (<a href="https://docs.blender.org/manual/en/latest/animation/keyframes/editing.html#insert-keyframe">打关键帧</a> )
    <ul>
      <li>在Layout界面Object Mode 中选中</li>
    </ul>
  </li>
  <li>Ctrl+手势: 缩放graph editor面板</li>
</ul>

<h3 id="blender-script的用法">blender Script的用法</h3>

<p>看到有人用blender内置的python script编辑器做追踪的效果, 但是这个编辑器实在很难用, 而且没有自动提示的情况下, 要一直关心传参数的格式很麻烦, 远不如几何节点好用.</p>
<ul>
  <li>如果用vs code编辑再同步呢? 需要一些联动的功能:
    <ul>
      <li>安装blender的代码包以便自动提示</li>
      <li>blender执行外部代码的功能</li>
      <li>一个可以自动识别选中blender物体转换为代码的插件.</li>
    </ul>
  </li>
</ul>

<p>网上一个blender和vscode联动的方法: https://blog.csdn.net/qq_43331089/article/details/124490171</p>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;title&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;icon&quot;=&gt;nil}]}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="blender" /><category term="content" /><category term="geometry_node" /><category term="track" /><category term="shortcut" /><category term="script" /><summary type="html"><![CDATA[参考这个教学]]></summary></entry><entry><title type="html">EMVA1288 sensor测试</title><link href="https://roshameow.github.io//personal_homepage/docs/sensor/EMVA1288-sensor/" rel="alternate" type="text/html" title="EMVA1288 sensor测试" /><published>2024-04-05T00:00:00+00:00</published><updated>2024-04-11T17:20:55+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/sensor/EMVA1288-sensor</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/sensor/EMVA1288-sensor/"><![CDATA[<p><img src="/personal_homepage/docs/attachment/Pasted%20image%2020231116154316.png" alt="Pasted image 20231116154316.png" width="600" /></p>

<p>参数: QE $K, \eta$</p>

<table>
  <thead>
    <tr>
      <th>成像模型</th>
      <th>input</th>
      <th>中间结果</th>
      <th>output</th>
      <th>参数</th>
      <th>参数</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>变量下标</td>
      <td>p</td>
      <td>e</td>
      <td>y</td>
      <td>qe或<br />$\eta$</td>
      <td>$K$</td>
    </tr>
    <tr>
      <td>含义</td>
      <td>光子</td>
      <td>电子</td>
      <td>读数</td>
      <td>QE(Quantum Efficiency)</td>
      <td>System Gain</td>
    </tr>
    <tr>
      <td>测量方式</td>
      <td>由积分时间+ sensor面积 得出<br />公式: <br /><br />$\mu_p=\frac{\text{辐射能}}{\text{单个光子的辐射能}}=\frac{A(sensor面积)\cdot t(曝光时间)\cdot E(辐射照度)}{h(\text{普朗克常数})c(\text{光速})/\lambda(\text{波长})}$</td>
      <td> </td>
      <td>直接测量$y(t)$</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>分布</td>
      <td>Poisson分布<br /></td>
      <td> </td>
      <td> </td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>公式关系</td>
      <td> </td>
      <td>$\mu_e=\sigma_e^2$ <br />$K\mu_e=\mu_y-\mu_{y.dark}$</td>
      <td> </td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>统计变量</td>
      <td>$\mu_p, \sigma_p$</td>
      <td> </td>
      <td>dark noise: $\mu_{y.dark},\sigma_{y.dark}$<br />$\mu_y, \sigma_y$<br />spatial $s_y$</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td> </td>
      <td> </td>
      <td> </td>
      <td> </td>
    </tr>
  </tbody>
</table>

<p>测试:</p>

<table>
  <thead>
    <tr>
      <th>测试项名称</th>
      <th>测试</th>
      <th>理论值</th>
      <th>含义</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>PTC(Photon Transfer)曲线</td>
      <td>$(\mu_y-\mu_{y.dark},\sigma_y^2)$</td>
      <td>$\sigma_y^2=K(\mu_y-\mu_{y.dark})+\sigma_{y.dark}^2$<br />是一条直线</td>
      <td>斜率 = System gain $K$</td>
    </tr>
    <tr>
      <td>SNR曲线</td>
      <td>($\mu_p$, SNR$=\frac{\mu_y-\mu_{y.dark}}{\sigma_y}$ )</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>Sensitive Curve</td>
      <td>$(\mu_p,\mu_y-\mu_{y.dark})$</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>Linearity Curve</td>
      <td>?</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>Linear error</td>
      <td>linearity curve上data值和拟合值的距离</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>Deviation Linearity Curve</td>
      <td>$(\mu_p,LE(\text{Linearity Error}))$<br />Linearity Error是<br /></td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>Stablity Check</td>
      <td>两项<br /> $(\mu_y-\mu_{y.dark},\sigma_y)$<br /> $(\mu_y-\mu_{y.dark},\mu[0]-\mu[1])$</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>darkcurrent</td>
      <td>两项<br />$(t,\mu_{y.dark})$ <br />$(t,\sigma_{y.dark})$</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>Temporal dark noise</td>
      <td> </td>
      <td> </td>
      <td> </td>
    </tr>
  </tbody>
</table>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;title&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;icon&quot;=&gt;nil}]}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="sensor" /><category term="content" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">pyside6一些功能的用法</title><link href="https://roshameow.github.io//personal_homepage/docs/tool/pyside6-tech/" rel="alternate" type="text/html" title="pyside6一些功能的用法" /><published>2024-04-01T00:00:00+00:00</published><updated>2024-04-07T20:09:32+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/tool/pyside6-tech</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/tool/pyside6-tech/"><![CDATA[<p>pyside是qt的python封装, API的调用方法基本差不多. 用pyside从零开始写一个gui用于标注或测试(调参数或者看中间结果), 每次花费时间都比想象的要少的多. 功能方便而且代码的可读性非常好.</p>

<h2 id="工具">工具</h2>

<ul>
  <li>designer和vscode
    <ul>
      <li>desginer主要用到promote, 加载资源, 配置qss的功能</li>
    </ul>
  </li>
  <li>vscode的PYQT Integration, 配置好uic, rcc路径后, 可以右键编译</li>
</ul>

<h2 id="事例">事例</h2>
<h3 id="视频播放">视频播放</h3>

<p>用QTimer和opencv实现</p>

<p><img src="/personal_homepage/docs/attachment/Screen%20Recording%202024-04-07%20at%2009.30.20.mp4" alt="Screen Recording 2024-04-07 at 09.30.20.mp4" width="200" /></p>

<ol>
  <li>把label提升到自定义可以drop file的LabelImage</li>
  <li>用timer设置play, pause功能, 进度条拖动功能</li>
</ol>

<ul>
  <li>代码: <a href="https://gist.github.com/roshameow/c1c27989df0ac90a89ee9d99b87d6d59#file-main-py"><strong>main.py</strong></a> , <a href="https://gist.github.com/roshameow/c1c27989df0ac90a89ee9d99b87d6d59#file-ui_labelimage-py"><strong>Ui_LabelImage.py</strong></a></li>
</ul>

<h3 id="动态折线图">动态折线图</h3>

<p>用QtCharts实现</p>

<p><img src="/personal_homepage/docs/attachment/Screen%20Recording%202024-04-07%20at%2009.46.44.mp4" alt="Screen Recording 2024-04-07 at 09.46.44.mp4" width="800" /></p>

<ol>
  <li>界面画出QWidget并提升到自定义的LineChart, 继承QChartView</li>
  <li>定义chart和series</li>
  <li>修改QChartView的样式: 在designer里用qss实现</li>
  <li>修改QChart的样式:
    <ul>
      <li><a href="https://stackoverflow.com/questions/39146502/how-to-remove-margin-from-qchartview-or-qchart">QChart调整和QChartView之间的Margin</a></li>
      <li><a href="https://stackoverflow.com/questions/51398463/qt-chart-remove-space-for-title-legend">QChart调整axis和边界之间的Margin</a></li>
      <li><a href="https://doc.qt.io/qt-6/qtcharts-customchart-example.html">QChart设置样式</a></li>
    </ul>
  </li>
  <li>添加series update代码</li>
</ol>

<ul>
  <li>代码: <a href="https://gist.github.com/roshameow/d1e0892205fa832aeb930a75130864e7#file-update_frame-py"><strong>update_frame.py</strong></a> , <a href="https://gist.github.com/roshameow/d1e0892205fa832aeb930a75130864e7#file-line_chart-py"><strong>line_chart.py</strong></a></li>
</ul>

<h3 id="图像标记">图像标记</h3>

<iframe src="//player.bilibili.com/player.html?aid=1452824219&amp;bvid=BV1bq421F7sF&amp;cid=1496585736&amp;p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>

<ol>
  <li>选择文件: 在QTreeView上设置model为QFileSystemModel</li>
  <li>互动标记图片
    <ul>
      <li>用paintEvent和QPainter实现标记</li>
    </ul>
  </li>
  <li>切换label
    <ul>
      <li>用QFile替换svg的颜色</li>
    </ul>
  </li>
  <li>显示位置数据: 在QTableView上设置model为自定义的PandasModel
    <ul>
      <li>给tableView设置Delegate更改颜色和行为</li>
    </ul>
  </li>
</ol>

<ul>
  <li>代码: <a href="https://gist.github.com/roshameow/7d45d536dd4ab8ff6ff618b7911b5890#file-main-py"><strong>main.py</strong></a> , <a href="https://gist.github.com/roshameow/7d45d536dd4ab8ff6ff618b7911b5890#file-mask_model-py">tableView添加互动: <strong>mask_model.py</strong></a> , <a href="https://gist.github.com/roshameow/7d45d536dd4ab8ff6ff618b7911b5890#file-pyside_util-py">更改svg颜色: <strong>pyside_util.py</strong></a> , <a href="https://gist.github.com/roshameow/7d45d536dd4ab8ff6ff618b7911b5890#file-ui_labelimage-py">图片添加互动: <strong>Ui_LabelImage.py</strong></a></li>
</ul>

<h2 id="资源">资源</h2>

<p>讲的很好的入门视频:</p>

<iframe src="//player.bilibili.com/player.html?aid=610679490&amp;bvid=BV1c84y1N7iL&amp;cid=1098863799&amp;p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>

<p>其他:</p>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;title&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;icon&quot;=&gt;nil}]}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="tool" /><category term="content" /><category term="pyside6" /><category term="gui" /><category term="python" /><summary type="html"><![CDATA[pyside是qt的python封装, API的调用方法基本差不多. 用pyside从零开始写一个gui用于标注或测试(调参数或者看中间结果), 每次花费时间都比想象的要少的多. 功能方便而且代码的可读性非常好.]]></summary></entry><entry><title type="html">blender学习: 用粒子系统做毛毡效果</title><link href="https://roshameow.github.io//personal_homepage/docs/blender/blender-learning10/" rel="alternate" type="text/html" title="blender学习: 用粒子系统做毛毡效果" /><published>2024-03-25T00:00:00+00:00</published><updated>2024-03-26T19:01:05+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/blender/blender-learning10</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/blender/blender-learning10/"><![CDATA[<h3 id="步骤">步骤:</h3>

<p>参考<a href="https://www.xiaohongshu.com/explore/65eb070500000000030320e2/">这个教学</a></p>
<ul>
  <li>Paticles添加毛发粒子: Particle type选择<a href="https://docs.blender.org/manual/en/latest/physics/particles/hair/index.html">hair</a>
    <ul>
      <li>Emission
        <ul>
          <li>number=5000 发根的总数量</li>
          <li>hair length=0.03</li>
          <li>Segments = 5 卷曲?</li>
        </ul>
      </li>
      <li><a href="https://docs.blender.org/manual/en/latest/physics/particles/emitter/render.html">Render</a> -&gt;Path-&gt;Steps=5
        <ul>
          <li>对hair做subdivision的次数</li>
        </ul>
      </li>
      <li><a href="https://docs.blender.org/manual/en/latest/physics/particles/hair/display.html">Viewport Display</a>-&gt; Strand Step=5</li>
      <li><a href="https://docs.blender.org/manual/en/latest/physics/particles/emitter/children.html">Children</a> 选择simple
        <ul>
          <li>display amount=100, render amount=100</li>
          <li>Roughness
            <ul>
              <li>Random = 0.08</li>
              <li>Size = 0.851</li>
            </ul>
          </li>
          <li>Kink(纽结)选择Curl
            <ul>
              <li>Amplitude(振幅) = 0.03</li>
              <li>Hair shape
                <ul>
                  <li>Diameter Root &gt; Tip</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>shader:
    <ul>
      <li>给Particles添加一个Principle Hair BSDF的material
        <ul>
          <li>roughness= 0.86</li>
          <li>Radial Roughness=0.95</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>结果: 没法复刻例图的效果, 比起毛毡更像搓澡巾😣
    <ul>
      <li><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240326101412.png" alt="Pasted image 20240326101412.png" width="300" />  局部: <img src="/personal_homepage/docs/attachment/Pasted%20image%2020240326101810.png" alt="Pasted image 20240326101810.png" width="200" /></li>
      <li>是不是还需要设置其他参数?</li>
    </ul>
  </li>
</ul>

<h3 id="模型">模型</h3>

<p>[1] https://free3d.com/3d-model/pumpkin-57117.html</p>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;title&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;icon&quot;=&gt;nil}]}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="blender" /><category term="content" /><category term="粒子系统" /><category term="hair" /><summary type="html"><![CDATA[步骤:]]></summary></entry><entry><title type="html">attention的优化– flash attention加速</title><link href="https://roshameow.github.io//personal_homepage/docs/algorithm/attention3/" rel="alternate" type="text/html" title="attention的优化– flash attention加速" /><published>2024-03-20T00:00:00+00:00</published><updated>2024-04-07T17:06:18+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/algorithm/attention3</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/algorithm/attention3/"><![CDATA[<p>flash-attention是一种算子合并(kernel fusion)的优化. 把self-attention分块, 直接在SRAM里计算, 省去了HBM来回搬运中间结果S和P的时间(如下图).  self-attention由两层矩阵乘法, softmax, 和其他eltwise计算(mask, dropout)构成.</p>

<p><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240401154241.png" alt="Pasted image 20240401154241.png" width="500" /></p>

<h2 id="attention分块计算">attention分块计算</h2>

<h3 id="forward计算-qkv---o">forward计算: Q,K,V -&gt; O</h3>

<ul>
  <li>矩阵分块如上图: 都是在token的维度分块
    <ul>
      <li>i iteration循环(黄色部分)
        <ul>
          <li>每次i iteration需要load不同位置的 Q, O</li>
        </ul>
      </li>
      <li>j iteration循环(浅色部分)
        <ul>
          <li>每次j iteration需要load不同位置的 K, V</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>softmax计算:
    <ul>
      <li>$softmax(x)=\frac{e^{x}}{\sum_j e^{x_j}}$ , 其中$\sum_j$ 是rowsum</li>
      <li>safe softmax:  $softmax(x)=\frac{e^{x-m}}{\sum_j e^{x_j-m}}$
        <ul>
          <li>为了避免$\sum_j e^{x_j}$ 产生特别大的值溢出</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>softmax分块:
    <ul>
      <li>对于relation S的每个分块的patch $P$, 记: 局部最大值 $m_p=\max_{x\in P}(x)$, 局部rowsum $l_p=\sum_{x\in P} e^{x-m_P}$
        <ul>
          <li>得到: $softmax(x)=\frac{e^x\cdot e^{(m_p-m)}}{\sum_P l_p\cdot e^{(m_p-m)}}$</li>
        </ul>
      </li>
      <li>写成关于$j$ 的迭代形式:(softmax和iterate i无关)
        <ul>
          <li>$m_{j+1}=\max(m_j, m_{local})$</li>
          <li>local rowsum $l_{j+1}=l_j\cdot e^{(m_j-m_{j+1})}+l_{local}\cdot e^{(m_{local}-m_{j+1})}$</li>
          <li>softmax的中间结果 $\tilde P_{local}=e^{S_{local}-m_{local}}$</li>
          <li>$O_{j+1}=softmax(S_{j+1})V_{j+1}=\text{diag}(l_{j+1})^{-1}(\text{diag}(l_j)\cdot O_j\cdot e^{m_j-m_{j+1}}+ \tilde P_{local}V_{j+1}\cdot e^{m_{local}-m_{j+1}})$</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="backward计算-o-do-q-k-v---dv-dk-dq">backward计算: O, dO, Q, K, V -&gt; dV, dK, dQ</h3>

<p>flash-attention里面因为不保存S, P的结果, 在backward的时候要对它们重新计算</p>

<p>用<a href="https://en.wikipedia.org/wiki/Automatic_differentiation">reverse mode</a> 的chain rule做backward计算:  <img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e5986463d8b2e48d0da5233099bb97bc4ea89844" alt="reverse" /></p>
<ul>
  <li>矩阵计算: $A=BC$
    <ul>
      <li>$dC = \frac{\partial y}{\partial C}=\sum_{A_{ij}} \frac{\partial y}{\partial A_{ij}}\cdot \frac{\partial A_{ij}}{\partial C}=B^{T}dA$ , 类似的 $dB=dA\ C^T$</li>
    </ul>
  </li>
  <li>softmax计算: $P=softmax(S)$, $P_{j}=\frac{e^{S_{j}}}{\sum_{x\in S} e^x}$  , 下面矩阵都是eltwise乘法$\downarrow$  \(\begin{align}dS&amp;=\sum_{P_{j}} dP_{j}\frac{\partial P_{j}}{\partial S}&amp;=\sum_{P_{j}} (P_{k}\mathbb 1_{j=k}-P_{j}P_{k})dP_{j}&amp;\ (\text{根据乘除法求导法则}\frac{\partial P_j}{\partial S_k}=P_j\mathbb 1_{k=j}-\frac{e^{s_{j}}e^{s_k}}{(\sum_{x\in S} e^x)^2}=diag(P)-PP^T\text{是softmax的Jacobian})\\ &amp;&amp;=P_{k}dP_{k}+(\sum_{P_{j}}P_{j}dP_{j})P_{k}&amp;=P(dP+\sum_{P_{j}}P_{j}dP_{j})\\ &amp;&amp;&amp;=P(dP+ \sum_{j} P_j(\sum_i V_{ij} dO_{j}))\  (\text{因为}PV=O)\\ &amp;&amp;&amp;=P(dP+ \sum_{j}O_{j}dO_{j})\ (\text{其中}\sum_j \text{就是rowsum})\end{align}\)</li>
  <li>backward采用和forward相同的分块</li>
</ul>

<h3 id="每块占用的sram">每块占用的SRAM</h3>

<ul>
  <li>forward和backward采用相同的分块, backward需要的SRAM比较大, 因此直接以backward的需求量为准</li>
  <li>backward:
    <ul>
      <li>Q, dQ, O, dO : Br x d</li>
      <li>K, V, dK, dV: Bc x d</li>
      <li>S, P, dS, dP,  : Br x Bc</li>
      <li>l, m: Br</li>
    </ul>
  </li>
  <li>在flash-attention文章里, 取 <code class="language-plaintext highlighter-rouge">Bc=floor(M/4d)</code>, <code class="language-plaintext highlighter-rouge">Br=min(floor(M/4d),d)</code> , 这样能保证SRAM够用吗❓</li>
</ul>

<h2 id="flash-attention-v1">flash-attention v1</h2>

<ul>
  <li>让iterate i为内循环, iterate j为外循环</li>
</ul>

<h2 id="flash-attention-v2">flash-attention v2</h2>

<ol>
  <li>更改iteration的顺序(一般的规则是把相关性更高的放在更内层循环)
    <ul>
      <li>forward时让iterate j为内循环, 因为forward最终结果为O, 这样O的分块不用反复进出</li>
      <li>barkward时让iterate i为内循环, backward的最终结果为(dK, dV, dQ), 这样dK, dV不用反复进出</li>
    </ul>
  </li>
  <li>softmax中间结果的存储:
    <ul>
      <li>forward正常计算, 不过不存储$l,m$, 改为存$L=m+\log (l)$
        <ul>
          <li>v2 forward采用j的内循环, 和$l, m$ 计算方向一致, forward的时候本来就可以inplace更新不需要存储</li>
          <li>backward时重新计算P, $P=diag(l)^{-1}e^{S-m}=e^{S-L}$</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>优化thread blocks的并行化计算: 把i iteration和j iteration的部分也切分进行并行计算(尤其推理时batch size=1; 或训练token长度特别长, 需要减小batch_size和attention_heads时, 并行度不够)
    <ul>
      <li>forward时i iteration的相关性比较小, 所以切分i iteration更方便
        <ul>
          <li>但是每个i iteration的切分都要自己load一遍K, V了</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>优化warp的并行计算: warp之间也是并行的
    <ul>
      <li>v2 也是在i iteration切分warp的并行</li>
    </ul>
  </li>
</ol>

<h2 id="reference">reference</h2>

<p>[1] Dao, Tri, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. “<strong>FlashAttention</strong>: Fast and Memory-Efficient Exact Attention with IO-Awareness.” arXiv, June 23, 2022. <a href="https://doi.org/10.48550/arXiv.2205.14135">https://doi.org/10.48550/arXiv.2205.14135</a>.</p>

<p>[2] Dao, Tri. “<strong>FlashAttention-2:</strong> Faster Attention with Better Parallelism and Work Partitioning.” arXiv, July 17, 2023. <a href="https://doi.org/10.48550/arXiv.2307.08691">https://doi.org/10.48550/arXiv.2307.08691</a>.</p>

<p><strong>代码</strong>: <a href="https://github.com/Dao-AILab/flash-attention">https://github.com/Dao-AILab/flash-attention</a></p>

<h2 id="其他讨论的链接">其他讨论的链接</h2>

<p>[1] https://zhuanlan.zhihu.com/p/669926191 flash-attention v1</p>

<p>[2] https://mp.weixin.qq.com/s/5K6yNj23NmNLcAQofHcT4Q flash-attention v2</p>

<p>[3] https://zhuanlan.zhihu.com/p/685020608 概括</p>

<p>[4] https://mp.weixin.qq.com/s/NKShFDrfDGsb0G6PAkUCGw triton的实现</p>

<p>[5] https://triton-lang.org/main/getting-started/tutorials/06-fused-attention.html  基于triton的flash-attention代码</p>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;title&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;icon&quot;=&gt;nil}]}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="algorithm" /><category term="content" /><category term="attention" /><category term="flash attention" /><category term="speedup" /><category term="gpu" /><summary type="html"><![CDATA[flash-attention是一种算子合并(kernel fusion)的优化. 把self-attention分块, 直接在SRAM里计算, 省去了HBM来回搬运中间结果S和P的时间(如下图). self-attention由两层矩阵乘法, softmax, 和其他eltwise计算(mask, dropout)构成.]]></summary></entry><entry><title type="html">blender学习: 镜头推移</title><link href="https://roshameow.github.io//personal_homepage/docs/blender/blender-learning9/" rel="alternate" type="text/html" title="blender学习: 镜头推移" /><published>2024-03-12T00:00:00+00:00</published><updated>2024-03-12T15:32:52+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/blender/blender-learning9</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/blender/blender-learning9/"><![CDATA[<h2 id="前向推镜头">前向推镜头</h2>

<p>参考<a href="https://www.xiaohongshu.com/explore/65a2ce0a000000001d037212">这个教学视频</a>, 用array modifier复制多个模型 , 得到一种穿梭效果</p>

<p><img src="/personal_homepage/docs/attachment/camera_move.mp4" alt="camera_move.mp4" width="300" /></p>

<h2 id="模型资源">模型资源</h2>

<p>[1] https://sketchfab.com/feed</p>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;title&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;icon&quot;=&gt;nil}]}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="blender" /><category term="content" /><category term="camera" /><summary type="html"><![CDATA[前向推镜头]]></summary></entry></feed>