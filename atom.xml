<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://roshameow.github.io//personal_homepage/atom.xml" rel="self" type="application/atom+xml" /><link href="https://roshameow.github.io//personal_homepage/" rel="alternate" type="text/html" /><updated>2024-04-10T06:58:37+00:00</updated><id>https://roshameow.github.io//personal_homepage/atom.xml</id><title type="html">Liu, Wen’s Home Page</title><subtitle>Work, Experiments and Ideas.</subtitle><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;title&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;icon&quot;=&gt;nil}]}</name><email>w.liuatnk@gmail.com</email></author><entry><title type="html">blender学习: 几何节点做摄像头移动阵列</title><link href="https://roshameow.github.io//personal_homepage/docs/blender/blender-learning11/" rel="alternate" type="text/html" title="blender学习: 几何节点做摄像头移动阵列" /><published>2024-04-08T00:00:00+00:00</published><updated>2024-04-10T22:56:01+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/blender/blender-learning11</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/blender/blender-learning11/"><![CDATA[<p>参考<a href="https://www.bilibili.com/video/BV1t5411v7b1/">这个教学</a></p>

<h3 id="建模">建模</h3>

<p>直接复制作者的模型和材质</p>
<ul>
  <li>箭头</li>
  <li>摄像头</li>
</ul>

<h3 id="步骤">步骤:</h3>

<ul>
  <li>制作摄像头阵列: 用<a href="https://docs.blender.org/manual/en/latest/modeling/geometry_nodes/instances/instance_on_points.html#">Instance on Points节点</a>
    <ul>
      <li>添加一个Plane mesh, 在modifier添加几何节点</li>
      <li>在Points的地方制作一个meshgrid:
        <ul>
          <li>用Grid节点调整Grid大小和距离: 相对plane平面</li>
          <li>用<a href="https://docs.blender.org/manual/en/latest/render/shader_nodes/vector/vector_rotate.html#">Vector Rotate节点</a> 批量调整Plane里面顶点的位置</li>
        </ul>
      </li>
      <li>把摄像头主体和摄像机臂分别设置成为Plane顶点的instance: 用Join Geometry节点连接</li>
    </ul>
  </li>
  <li>设置摄像头主体追踪箭头: 分为 箭头在xz平面平移(看向箭头)和箭头在x轴旋转(跟随箭头点头) 两部分
    <ul>
      <li>平移-&gt;关于y轴旋转:
        <ul>
          <li>计算Plane里面顶点到箭头的vector: 这里面<a href="https://docs.blender.org/manual/en/latest/modeling/geometry_nodes/geometry/read/position.html">Position节点</a>给出的是Plane每个顶点的location</li>
          <li>用<a href="https://docs.blender.org/manual/en/latest/modeling/geometry_nodes/utilities/rotation/align_euler_to_vector.html">Align Euler to Vector节点</a> 设置成关于y轴旋转</li>
        </ul>
      </li>
      <li>旋转-&gt; 旋转:
        <ul>
          <li>提取箭头Rotation的x轴反向旋转, 用Rotate Euler节点添加到Plane的Rotation(plane每个顶点的rotation)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>制作箭头绕圈和点头动画:
    <ol>
      <li>绕圈: 让箭头围绕一个圈移动
        <ul>
          <li>添加一个Circle曲线</li>
          <li>给箭头添加Constraint-&gt; <a href="https://docs.blender.org/manual/en/4.1/animation/constraints/relationship/follow_path.html">Follow Path</a>
            <ul>
              <li>Target选择刚才的Circle</li>
              <li>Option+G清除位置: 加了follow path constraint之后, position会变成相对path的, 所以要清除position保证箭头在path上</li>
              <li>在起始和结束打上关键帧
                <ul>
                  <li>起始设置offset=0</li>
                  <li>结束设置offset=100: 按照文档的说法似乎应该是0到1? 但是实际用的是0到100?</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>点头: 打rotation的关键帧
        <ul>
          <li>打3个关键帧: 原始位置-&gt; 最低位置 -&gt; 原始位置
        - 在graph editor调整运动曲线</li>
        </ul>
      </li>
    </ol>
  </li>
  <li>结果: 感觉这是个很有用的互动功能
    <ul>
      <li><img src="/personal_homepage/docs/attachment/camera_follow.mp4" alt="camera_follow.mp4" width="400" /></li>
    </ul>
  </li>
</ul>

<h3 id="geometry-nodes">geometry nodes</h3>

<p><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240410145231.png" alt="Pasted image 20240410145231.png" width="800" /></p>
<h3 id="用到的blender的一些快捷键">用到的blender的一些快捷键</h3>

<ul>
  <li>Option+G/S/R (<a href="https://docs.blender.org/manual/en/latest/scene_layout/object/editing/clear.html#">清除</a> )
    <ul>
      <li>把object位置变成默认的0</li>
    </ul>
  </li>
  <li>G/S (移动/缩放)的多种功能:
    <ul>
      <li>可以在Timeline移动缩放关键帧</li>
      <li>可以在graph editor里面调整运动曲线</li>
    </ul>
  </li>
  <li>I (<a href="https://docs.blender.org/manual/en/latest/animation/keyframes/editing.html#insert-keyframe">打关键帧</a> )
    <ul>
      <li>在Layout界面Object Mode 中选中</li>
    </ul>
  </li>
  <li>Ctrl+手势: 缩放graph editor面板</li>
</ul>

<h3 id="blender-script的用法">blender Script的用法</h3>

<p>看到有人用blender内置的python script编辑器做追踪的效果, 但是这个编辑器实在很难用, 而且没有自动提示的情况下, 要一直关心传参数的格式很麻烦, 远不如几何节点好用.</p>
<ul>
  <li>如果用vs code编辑再同步呢? 需要一些联动的功能:
    <ul>
      <li>安装blender的代码包以便自动提示</li>
      <li>blender执行外部代码的功能</li>
      <li>一个可以自动识别选中blender物体转换为代码的插件.</li>
    </ul>
  </li>
</ul>

<p>网上一个blender和vscode联动的方法: https://blog.csdn.net/qq_43331089/article/details/124490171</p>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;title&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;icon&quot;=&gt;nil}]}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="blender" /><category term="content" /><category term="geometry_node" /><category term="track" /><category term="shortcut" /><category term="script" /><summary type="html"><![CDATA[参考这个教学]]></summary></entry><entry><title type="html">EMVA1288 sensor测试</title><link href="https://roshameow.github.io//personal_homepage/docs/sensor/EMVA1288-sensor/" rel="alternate" type="text/html" title="EMVA1288 sensor测试" /><published>2024-04-05T00:00:00+00:00</published><updated>2024-04-08T07:34:34+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/sensor/EMVA1288-sensor</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/sensor/EMVA1288-sensor/"><![CDATA[<p><img src="/personal_homepage/docs/attachment/Pasted%20image%2020231116154316.png" alt="Pasted image 20231116154316.png" width="600" /></p>

<table>
  <thead>
    <tr>
      <th>成像模型</th>
      <th>input</th>
      <th>中间结果</th>
      <th>output</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>变量下标</td>
      <td>p</td>
      <td>e</td>
      <td>y</td>
    </tr>
    <tr>
      <td>含义</td>
      <td>光子</td>
      <td>电子</td>
      <td>读数</td>
    </tr>
    <tr>
      <td>测量方式</td>
      <td>由积分时间估算<br />光源辐射能量: 由已知的平行光源的辐射照度计算：$A(sensor面积)\cdot t(曝光时间)\cdot E(辐射照度)$ <br />			- 假设辐射照度是常数<br />单个光子的辐射能$Q=\frac{h(\text{普朗克常数})c(\text{光速})}{\lambda(\text{波长})}=\frac{6.6260755\cdot 10^{−34} [Js]\cdot 2.99792458\cdot 10^{8} [m/s]}{\lambda [\mu m]}$</td>
      <td> </td>
      <td> </td>
    </tr>
  </tbody>
</table>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;title&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;icon&quot;=&gt;nil}]}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="sensor" /><category term="content" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">pyside6一些功能的用法</title><link href="https://roshameow.github.io//personal_homepage/docs/tool/pyside6-tech/" rel="alternate" type="text/html" title="pyside6一些功能的用法" /><published>2024-04-01T00:00:00+00:00</published><updated>2024-04-07T20:09:32+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/tool/pyside6-tech</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/tool/pyside6-tech/"><![CDATA[<p>pyside是qt的python封装, API的调用方法基本差不多. 用pyside从零开始写一个gui用于标注或测试(调参数或者看中间结果), 每次花费时间都比想象的要少的多. 功能方便而且代码的可读性非常好.</p>

<h2 id="工具">工具</h2>

<ul>
  <li>designer和vscode
    <ul>
      <li>desginer主要用到promote, 加载资源, 配置qss的功能</li>
    </ul>
  </li>
  <li>vscode的PYQT Integration, 配置好uic, rcc路径后, 可以右键编译</li>
</ul>

<h2 id="事例">事例</h2>
<h3 id="视频播放">视频播放</h3>

<p>用QTimer和opencv实现</p>

<p><img src="/personal_homepage/docs/attachment/Screen%20Recording%202024-04-07%20at%2009.30.20.mp4" alt="Screen Recording 2024-04-07 at 09.30.20.mp4" width="200" /></p>

<ol>
  <li>把label提升到自定义可以drop file的LabelImage</li>
  <li>用timer设置play, pause功能, 进度条拖动功能</li>
</ol>

<ul>
  <li>代码: <a href="https://gist.github.com/roshameow/c1c27989df0ac90a89ee9d99b87d6d59#file-main-py"><strong>main.py</strong></a> , <a href="https://gist.github.com/roshameow/c1c27989df0ac90a89ee9d99b87d6d59#file-ui_labelimage-py"><strong>Ui_LabelImage.py</strong></a></li>
</ul>

<h3 id="动态折线图">动态折线图</h3>

<p>用QtCharts实现</p>

<p><img src="/personal_homepage/docs/attachment/Screen%20Recording%202024-04-07%20at%2009.46.44.mp4" alt="Screen Recording 2024-04-07 at 09.46.44.mp4" width="800" /></p>

<ol>
  <li>界面画出QWidget并提升到自定义的LineChart, 继承QChartView</li>
  <li>定义chart和series</li>
  <li>修改QChartView的样式: 在designer里用qss实现</li>
  <li>修改QChart的样式:
    <ul>
      <li><a href="https://stackoverflow.com/questions/39146502/how-to-remove-margin-from-qchartview-or-qchart">QChart调整和QChartView之间的Margin</a></li>
      <li><a href="https://stackoverflow.com/questions/51398463/qt-chart-remove-space-for-title-legend">QChart调整axis和边界之间的Margin</a></li>
      <li><a href="https://doc.qt.io/qt-6/qtcharts-customchart-example.html">QChart设置样式</a></li>
    </ul>
  </li>
  <li>添加series update代码</li>
</ol>

<ul>
  <li>代码: <a href="https://gist.github.com/roshameow/d1e0892205fa832aeb930a75130864e7#file-update_frame-py"><strong>update_frame.py</strong></a> , <a href="https://gist.github.com/roshameow/d1e0892205fa832aeb930a75130864e7#file-line_chart-py"><strong>line_chart.py</strong></a></li>
</ul>

<h3 id="图像标记">图像标记</h3>

<iframe src="//player.bilibili.com/player.html?aid=1452824219&amp;bvid=BV1bq421F7sF&amp;cid=1496585736&amp;p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>

<ol>
  <li>选择文件: 在QTreeView上设置model为QFileSystemModel</li>
  <li>互动标记图片
    <ul>
      <li>用paintEvent和QPainter实现标记</li>
    </ul>
  </li>
  <li>切换label
    <ul>
      <li>用QFile替换svg的颜色</li>
    </ul>
  </li>
  <li>显示位置数据: 在QTableView上设置model为自定义的PandasModel
    <ul>
      <li>给tableView设置Delegate更改颜色和行为</li>
    </ul>
  </li>
</ol>

<ul>
  <li>代码: <a href="https://gist.github.com/roshameow/7d45d536dd4ab8ff6ff618b7911b5890#file-main-py"><strong>main.py</strong></a> , <a href="https://gist.github.com/roshameow/7d45d536dd4ab8ff6ff618b7911b5890#file-mask_model-py">tableView添加互动: <strong>mask_model.py</strong></a> , <a href="https://gist.github.com/roshameow/7d45d536dd4ab8ff6ff618b7911b5890#file-pyside_util-py">更改svg颜色: <strong>pyside_util.py</strong></a> , <a href="https://gist.github.com/roshameow/7d45d536dd4ab8ff6ff618b7911b5890#file-ui_labelimage-py">图片添加互动: <strong>Ui_LabelImage.py</strong></a></li>
</ul>

<h2 id="资源">资源</h2>

<p>讲的很好的入门视频:</p>

<iframe src="//player.bilibili.com/player.html?aid=610679490&amp;bvid=BV1c84y1N7iL&amp;cid=1098863799&amp;p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>

<p>其他:</p>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;title&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;icon&quot;=&gt;nil}]}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="tool" /><category term="content" /><category term="pyside6" /><category term="gui" /><category term="python" /><summary type="html"><![CDATA[pyside是qt的python封装, API的调用方法基本差不多. 用pyside从零开始写一个gui用于标注或测试(调参数或者看中间结果), 每次花费时间都比想象的要少的多. 功能方便而且代码的可读性非常好.]]></summary></entry><entry><title type="html">blender学习: 用粒子系统做毛毡效果</title><link href="https://roshameow.github.io//personal_homepage/docs/blender/blender-learning10/" rel="alternate" type="text/html" title="blender学习: 用粒子系统做毛毡效果" /><published>2024-03-25T00:00:00+00:00</published><updated>2024-03-26T19:01:05+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/blender/blender-learning10</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/blender/blender-learning10/"><![CDATA[<h3 id="步骤">步骤:</h3>

<p>参考<a href="https://www.xiaohongshu.com/explore/65eb070500000000030320e2/">这个教学</a></p>
<ul>
  <li>Paticles添加毛发粒子: Particle type选择<a href="https://docs.blender.org/manual/en/latest/physics/particles/hair/index.html">hair</a>
    <ul>
      <li>Emission
        <ul>
          <li>number=5000 发根的总数量</li>
          <li>hair length=0.03</li>
          <li>Segments = 5 卷曲?</li>
        </ul>
      </li>
      <li><a href="https://docs.blender.org/manual/en/latest/physics/particles/emitter/render.html">Render</a> -&gt;Path-&gt;Steps=5
        <ul>
          <li>对hair做subdivision的次数</li>
        </ul>
      </li>
      <li><a href="https://docs.blender.org/manual/en/latest/physics/particles/hair/display.html">Viewport Display</a>-&gt; Strand Step=5</li>
      <li><a href="https://docs.blender.org/manual/en/latest/physics/particles/emitter/children.html">Children</a> 选择simple
        <ul>
          <li>display amount=100, render amount=100</li>
          <li>Roughness
            <ul>
              <li>Random = 0.08</li>
              <li>Size = 0.851</li>
            </ul>
          </li>
          <li>Kink(纽结)选择Curl
            <ul>
              <li>Amplitude(振幅) = 0.03</li>
              <li>Hair shape
                <ul>
                  <li>Diameter Root &gt; Tip</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>shader:
    <ul>
      <li>给Particles添加一个Principle Hair BSDF的material
        <ul>
          <li>roughness= 0.86</li>
          <li>Radial Roughness=0.95</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>结果: 没法复刻例图的效果, 比起毛毡更像搓澡巾😣
    <ul>
      <li><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240326101412.png" alt="Pasted image 20240326101412.png" width="300" />  局部: <img src="/personal_homepage/docs/attachment/Pasted%20image%2020240326101810.png" alt="Pasted image 20240326101810.png" width="200" /></li>
      <li>是不是还需要设置其他参数?</li>
    </ul>
  </li>
</ul>

<h3 id="模型">模型</h3>

<p>[1] https://free3d.com/3d-model/pumpkin-57117.html</p>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;title&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;icon&quot;=&gt;nil}]}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="blender" /><category term="content" /><category term="粒子系统" /><category term="hair" /><summary type="html"><![CDATA[步骤:]]></summary></entry><entry><title type="html">attention的优化– flash attention加速</title><link href="https://roshameow.github.io//personal_homepage/docs/algorithm/attention3/" rel="alternate" type="text/html" title="attention的优化– flash attention加速" /><published>2024-03-20T00:00:00+00:00</published><updated>2024-04-07T17:06:18+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/algorithm/attention3</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/algorithm/attention3/"><![CDATA[<p>flash-attention是一种算子合并(kernel fusion)的优化. 把self-attention分块, 直接在SRAM里计算, 省去了HBM来回搬运中间结果S和P的时间(如下图).  self-attention由两层矩阵乘法, softmax, 和其他eltwise计算(mask, dropout)构成.</p>

<p><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240401154241.png" alt="Pasted image 20240401154241.png" width="500" /></p>

<h2 id="attention分块计算">attention分块计算</h2>

<h3 id="forward计算-qkv---o">forward计算: Q,K,V -&gt; O</h3>

<ul>
  <li>矩阵分块如上图: 都是在token的维度分块
    <ul>
      <li>i iteration循环(黄色部分)
        <ul>
          <li>每次i iteration需要load不同位置的 Q, O</li>
        </ul>
      </li>
      <li>j iteration循环(浅色部分)
        <ul>
          <li>每次j iteration需要load不同位置的 K, V</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>softmax计算:
    <ul>
      <li>$softmax(x)=\frac{e^{x}}{\sum_j e^{x_j}}$ , 其中$\sum_j$ 是rowsum</li>
      <li>safe softmax:  $softmax(x)=\frac{e^{x-m}}{\sum_j e^{x_j-m}}$
        <ul>
          <li>为了避免$\sum_j e^{x_j}$ 产生特别大的值溢出</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>softmax分块:
    <ul>
      <li>对于relation S的每个分块的patch $P$, 记: 局部最大值 $m_p=\max_{x\in P}(x)$, 局部rowsum $l_p=\sum_{x\in P} e^{x-m_P}$
        <ul>
          <li>得到: $softmax(x)=\frac{e^x\cdot e^{(m_p-m)}}{\sum_P l_p\cdot e^{(m_p-m)}}$</li>
        </ul>
      </li>
      <li>写成关于$j$ 的迭代形式:(softmax和iterate i无关)
        <ul>
          <li>$m_{j+1}=\max(m_j, m_{local})$</li>
          <li>local rowsum $l_{j+1}=l_j\cdot e^{(m_j-m_{j+1})}+l_{local}\cdot e^{(m_{local}-m_{j+1})}$</li>
          <li>softmax的中间结果 $\tilde P_{local}=e^{S_{local}-m_{local}}$</li>
          <li>$O_{j+1}=softmax(S_{j+1})V_{j+1}=\text{diag}(l_{j+1})^{-1}(\text{diag}(l_j)\cdot O_j\cdot e^{m_j-m_{j+1}}+ \tilde P_{local}V_{j+1}\cdot e^{m_{local}-m_{j+1}})$</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="backward计算-o-do-q-k-v---dv-dk-dq">backward计算: O, dO, Q, K, V -&gt; dV, dK, dQ</h3>

<p>flash-attention里面因为不保存S, P的结果, 在backward的时候要对它们重新计算</p>

<p>用<a href="https://en.wikipedia.org/wiki/Automatic_differentiation">reverse mode</a> 的chain rule做backward计算:  <img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e5986463d8b2e48d0da5233099bb97bc4ea89844" alt="reverse" /></p>
<ul>
  <li>矩阵计算: $A=BC$
    <ul>
      <li>$dC = \frac{\partial y}{\partial C}=\sum_{A_{ij}} \frac{\partial y}{\partial A_{ij}}\cdot \frac{\partial A_{ij}}{\partial C}=B^{T}dA$ , 类似的 $dB=dA\ C^T$</li>
    </ul>
  </li>
  <li>softmax计算: $P=softmax(S)$, $P_{j}=\frac{e^{S_{j}}}{\sum_{x\in S} e^x}$  , 下面矩阵都是eltwise乘法$\downarrow$  \(\begin{align}dS&amp;=\sum_{P_{j}} dP_{j}\frac{\partial P_{j}}{\partial S}&amp;=\sum_{P_{j}} (P_{k}\mathbb 1_{j=k}-P_{j}P_{k})dP_{j}&amp;\ (\text{根据乘除法求导法则}\frac{\partial P_j}{\partial S_k}=P_j\mathbb 1_{k=j}-\frac{e^{s_{j}}e^{s_k}}{(\sum_{x\in S} e^x)^2}=diag(P)-PP^T\text{是softmax的Jacobian})\\ &amp;&amp;=P_{k}dP_{k}+(\sum_{P_{j}}P_{j}dP_{j})P_{k}&amp;=P(dP+\sum_{P_{j}}P_{j}dP_{j})\\ &amp;&amp;&amp;=P(dP+ \sum_{j} P_j(\sum_i V_{ij} dO_{j}))\  (\text{因为}PV=O)\\ &amp;&amp;&amp;=P(dP+ \sum_{j}O_{j}dO_{j})\ (\text{其中}\sum_j \text{就是rowsum})\end{align}\)</li>
  <li>backward采用和forward相同的分块</li>
</ul>

<h3 id="每块占用的sram">每块占用的SRAM</h3>

<ul>
  <li>forward和backward采用相同的分块, backward需要的SRAM比较大, 因此直接以backward的需求量为准</li>
  <li>backward:
    <ul>
      <li>Q, dQ, O, dO : Br x d</li>
      <li>K, V, dK, dV: Bc x d</li>
      <li>S, P, dS, dP,  : Br x Bc</li>
      <li>l, m: Br</li>
    </ul>
  </li>
  <li>在flash-attention文章里, 取 <code class="language-plaintext highlighter-rouge">Bc=floor(M/4d)</code>, <code class="language-plaintext highlighter-rouge">Br=min(floor(M/4d),d)</code> , 这样能保证SRAM够用吗❓</li>
</ul>

<h2 id="flash-attention-v1">flash-attention v1</h2>

<ul>
  <li>让iterate i为内循环, iterate j为外循环</li>
</ul>

<h2 id="flash-attention-v2">flash-attention v2</h2>

<ol>
  <li>更改iteration的顺序(一般的规则是把相关性更高的放在更内层循环)
    <ul>
      <li>forward时让iterate j为内循环, 因为forward最终结果为O, 这样O的分块不用反复进出</li>
      <li>barkward时让iterate i为内循环, backward的最终结果为(dK, dV, dQ), 这样dK, dV不用反复进出</li>
    </ul>
  </li>
  <li>softmax中间结果的存储:
    <ul>
      <li>forward正常计算, 不过不存储$l,m$, 改为存$L=m+\log (l)$
        <ul>
          <li>v2 forward采用j的内循环, 和$l, m$ 计算方向一致, forward的时候本来就可以inplace更新不需要存储</li>
          <li>backward时重新计算P, $P=diag(l)^{-1}e^{S-m}=e^{S-L}$</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>优化thread blocks的并行化计算: 把i iteration和j iteration的部分也切分进行并行计算(尤其推理时batch size=1; 或训练token长度特别长, 需要减小batch_size和attention_heads时, 并行度不够)
    <ul>
      <li>forward时i iteration的相关性比较小, 所以切分i iteration更方便
        <ul>
          <li>但是每个i iteration的切分都要自己load一遍K, V了</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>优化warp的并行计算: warp之间也是并行的
    <ul>
      <li>v2 也是在i iteration切分warp的并行</li>
    </ul>
  </li>
</ol>

<h2 id="reference">reference</h2>

<p>[1] Dao, Tri, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. “<strong>FlashAttention</strong>: Fast and Memory-Efficient Exact Attention with IO-Awareness.” arXiv, June 23, 2022. <a href="https://doi.org/10.48550/arXiv.2205.14135">https://doi.org/10.48550/arXiv.2205.14135</a>.</p>

<p>[2] Dao, Tri. “<strong>FlashAttention-2:</strong> Faster Attention with Better Parallelism and Work Partitioning.” arXiv, July 17, 2023. <a href="https://doi.org/10.48550/arXiv.2307.08691">https://doi.org/10.48550/arXiv.2307.08691</a>.</p>

<p><strong>代码</strong>: <a href="https://github.com/Dao-AILab/flash-attention">https://github.com/Dao-AILab/flash-attention</a></p>

<h2 id="其他讨论的链接">其他讨论的链接</h2>

<p>[1] https://zhuanlan.zhihu.com/p/669926191 flash-attention v1</p>

<p>[2] https://mp.weixin.qq.com/s/5K6yNj23NmNLcAQofHcT4Q flash-attention v2</p>

<p>[3] https://zhuanlan.zhihu.com/p/685020608 概括</p>

<p>[4] https://mp.weixin.qq.com/s/NKShFDrfDGsb0G6PAkUCGw triton的实现</p>

<p>[5] https://triton-lang.org/main/getting-started/tutorials/06-fused-attention.html  基于triton的flash-attention代码</p>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;title&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;icon&quot;=&gt;nil}]}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="algorithm" /><category term="content" /><category term="attention" /><category term="flash attention" /><category term="speedup" /><category term="gpu" /><summary type="html"><![CDATA[flash-attention是一种算子合并(kernel fusion)的优化. 把self-attention分块, 直接在SRAM里计算, 省去了HBM来回搬运中间结果S和P的时间(如下图). self-attention由两层矩阵乘法, softmax, 和其他eltwise计算(mask, dropout)构成.]]></summary></entry><entry><title type="html">blender学习: 镜头推移</title><link href="https://roshameow.github.io//personal_homepage/docs/blender/blender-learning9/" rel="alternate" type="text/html" title="blender学习: 镜头推移" /><published>2024-03-12T00:00:00+00:00</published><updated>2024-03-12T15:32:52+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/blender/blender-learning9</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/blender/blender-learning9/"><![CDATA[<h2 id="前向推镜头">前向推镜头</h2>

<p>参考<a href="https://www.xiaohongshu.com/explore/65a2ce0a000000001d037212">这个教学视频</a>, 用array modifier复制多个模型 , 得到一种穿梭效果</p>

<p><img src="/personal_homepage/docs/attachment/camera_move.mp4" alt="camera_move.mp4" width="300" /></p>

<h2 id="模型资源">模型资源</h2>

<p>[1] https://sketchfab.com/feed</p>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;title&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;icon&quot;=&gt;nil}]}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="blender" /><category term="content" /><category term="camera" /><summary type="html"><![CDATA[前向推镜头]]></summary></entry><entry><title type="html">blender学习: 做流光效果</title><link href="https://roshameow.github.io//personal_homepage/docs/blender/blender-learning8/" rel="alternate" type="text/html" title="blender学习: 做流光效果" /><published>2024-03-11T00:00:00+00:00</published><updated>2024-03-12T14:17:26+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/blender/blender-learning8</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/blender/blender-learning8/"><![CDATA[<p>用金属材质反射world背景的流动</p>
<h3 id="步骤">步骤:</h3>

<p>参考<a href="https://www.bilibili.com/video/BV1Ny421q7HM/">这个教学视频</a></p>
<ul>
  <li>制作物体:
    <ul>
      <li>添加curve</li>
      <li>调整Data-&gt;Geometry-&gt;Bevel(倒角)-&gt;Round-&gt; Depth: 把曲线变成软管</li>
      <li>加modifier-&gt;subdivisor</li>
    </ul>
  </li>
  <li>shader:
    <ul>
      <li>object: 用principled BSDF, 把metallic调到1, roughness 调整到0.1</li>
      <li>world:
        <ul>
          <li>在background添加流动材质
            <ul>
              <li>设置<a href="https://docs.blender.org/manual/en/latest/render/shader_nodes/input/texture_coordinate.html">texture Coordinate</a>  generate-&gt;Mapping(Y=#frame/40)-&gt;background图片</li>
            </ul>
          </li>
          <li>得到黑色背景
            <ul>
              <li>添加一个黑色背景, <a href="https://blenderartists.org/t/why-use-is-camera-ray-in-the-world/650625/3">用Light Path的Is Camera Ray控制混合</a> : 背景和物体分开的原理?</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>light: 改成sun, 给物体增加反光</li>
  <li>结果:
    <ul>
      <li><img src="/personal_homepage/docs/attachment/color_change.mp4" alt="color_change.mp4" width="300" /></li>
    </ul>
  </li>
</ul>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;title&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;icon&quot;=&gt;nil}]}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="blender" /><category term="content" /><category term="shader" /><category term="curve" /><summary type="html"><![CDATA[用金属材质反射world背景的流动 步骤:]]></summary></entry><entry><title type="html">attention的优化– 引进时序结构</title><link href="https://roshameow.github.io//personal_homepage/docs/deeplearning/attention2/" rel="alternate" type="text/html" title="attention的优化– 引进时序结构" /><published>2024-03-09T00:00:00+00:00</published><updated>2024-03-26T01:23:36+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/deeplearning/attention2</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/deeplearning/attention2/"><![CDATA[<p>在token很多(大模型用的超长文本), 或者本身数据是时间序列(比如语音, 视频流)的情况下, attention里面的weight会带来$O(token^2)$ 的内存消耗. state space model可以解决这个问题.</p>

<h2 id="基本结构">基本结构</h2>

<h3 id="ssmstate-space-model">SSM(state space model)</h3>

<ul>
  <li><a href="https://en.wikipedia.org/wiki/State-space_representation">状态空间(state space)</a> : 用state vector记录历史的input, 而不是记录所有的历史token
    <ul>
      <li><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240317183044.png" alt="Pasted image 20240317183044.png" width="200" /></li>
    </ul>
  </li>
  <li>连续表示:
    <ul>
      <li>linear state space model的一般形式:
        <ul>
          <li>$\dot x(t)=A(t)x(t)+B(t)u(t)$ (用input u更新状态 x)</li>
          <li>$y(t)=C(t)x(t)+D(t)u(t)$  (用状态x, 生成output y)</li>
        </ul>
      </li>
      <li>如果A,B,C是time invariant, 以及省略D, 得到,
        <ul>
          <li>$\dot x(t)=Ax(t)+Bu(t)$
            <ul>
              <li>解得 $x(t)=e^{At}x(0)+\int^t_0 e^{A(t-\tau)}Bu(\tau)d\tau$    , $x(t)$ 是$u(t)$ 的卷积形式: $x(t)=K(t)*u(t)$, $K(t)=e^{At}B$</li>
            </ul>
          </li>
          <li>$y(t)=Cx(t)$</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>离散表示:
    <ul>
      <li>迭代的表示: $x_{k+1}=\bar A x_k + \bar Bu_k$ , $y_{k+1}=Cx_{k+1}$
        <ul>
          <li>用<a href="https://en.wikipedia.org/wiki/Zero-order_hold">Zero order hold</a> 离散表示$u(t)$ , $\Delta$ 是step_size, 有: $\bar A=e^{\Delta A}$ , $\bar B=(\Delta A)^{-1}(e^{\Delta A}-I)\cdot \Delta B$
            <ul>
              <li><a href="https://en.wikipedia.org/wiki/Discretization">Discretization wiki</a>中正好有这个推导</li>
            </ul>
          </li>
          <li>用blinear离散表示$u(t)$,  $\Delta$ 是step_size, 有: $\bar A=(I-\Delta/2\cdot A)^{-1}(I+\Delta/2\cdot A)$ , $\bar B=(I-\Delta/2\cdot A)^{-1}\cdot \Delta B$</li>
        </ul>
      </li>
      <li>卷积的表示: $y=x * \bar K$ , $\bar K = (C\bar B, C\bar{AB}, \dots, C\bar A^{N-1}\bar B)$</li>
    </ul>
  </li>
  <li>一些状态空间的应用:
    <ul>
      <li>Kalman filter</li>
      <li>control theory</li>
    </ul>
  </li>
  <li>特点:
    <ul>
      <li>迭代和卷积的形式完全等价, 一般在训练时用卷积形式方便计算并行, 在推理时用迭代形式节省时间和存储</li>
      <li>对于很多任务, 输入$u(t)$ 是多通道的, 这个时候一般把相同ssm应用在每个通道上</li>
      <li>SSM支持无限长的序列推理, 所有的历史token信息都被压缩到state里</li>
    </ul>
  </li>
</ul>

<h3 id="linear-attention">Linear Attention</h3>

<p><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240317103547.png" alt="Pasted image 20240317103547.png" width="1000" /></p>

<ul>
  <li>把self-attention的softmax换掉就可以改写成SSM的形式, 既有SSM计算节省内存和时间的优点, 又贴近self-attention的效果
    <ul>
      <li>按照这种写法, 可以把SSM迭代中的$B, C$ 写成$K, Q$</li>
    </ul>
  </li>
  <li>实际使用中发现, linear attention的state容量是有限度的, 虽然理论上支持无限长的token, 但是对历史信息的遗失不好处理, 效果远不如基于self-attention的方法</li>
</ul>

<h2 id="一些利用ssmlinear-attention的语言模型工作">一些利用SSM/linear attention的语言模型工作</h2>
<h3 id="s4structured-state-space-sequence-结构">S4(Structured state space sequence) 结构</h3>

<p>发现用HiPPO(High-order Polynomial Projection Operator)矩阵做为SSM 中的状态转移矩阵$A$ 的初始化可以极大的提高SSM的性能.</p>

<ul>
  <li>motivation:
    <ol>
      <li>在measure function  $\omega^{(t)}$ 下, 用一组orthogonal basis表示有限维的state $x$ (例如: <a href="https://en.wikipedia.org/wiki/Legendre_polynomials">Legendre polynomials</a>  $P_n^{(t)}$ , $x(t)=\sum_n c_n(t)*P_n^{(t)}$)</li>
      <li>在measure function  $\omega^{(t)}$ 下, 想从state $x$  尽量<strong>恢复出input $u$ 的历史信息</strong> $u_{\le t}$ , 即
  \(x=\text{argmin}_{c_n}||u_{\le t}-\sum_n c_n*P_n||_{\omega}\)
        <ol>
          <li>measure function的选择影响了我们在state $x$ 中保留什么样的历史信息: 例如 $\downarrow$
            <ol>
              <li>LegS(scaled Legendre measure), $\omega(t)=\mathbb{1}[0,t]/t$ 对历史信息权重平均, 保留$\le t$ 的所有历史信息</li>
              <li>LegT(Translated Legendre), $\omega(t)=\frac{1}{\theta} \mathbb 1[t-\theta,t]$ , 保留最近一段的信息</li>
            </ol>
          </li>
        </ol>
      </li>
    </ol>
  </li>
  <li>计算:
    <ol>
      <li>以上两点得出, $c_n$ 是$u_{\le t}$ 在basis $P_n$, measure $\omega$ ,下的projection, $c_n=&lt;u_{\le t}, P_n^{(t)}&gt;_{\omega}$ ,
        <ul>
          <li>即 \(x=\sum_n &lt;u_{\le t}, P_n^{(t)}&gt;_{\omega} P_n^{(t)}=proj_{(\omega, P_n^{(t)})} (u_{\le t})\)</li>
          <li>如果写成SSM卷积的形式, \(x=K*u\) , \(K(t)=\sum_n P_n^{(t)}*\omega^{(t)}\)</li>
        </ul>
      </li>
      <li>因为$P_n$ 本身是basis: $\dot P_n\in span(P_n)$,  我们选取的$\omega$ 也尽量满足: $\dot \omega\in span(\omega)$</li>
      <li>把 $x$ 和 $u$ 带入SSM的ODE( $\dot x(t)=Ax(t)+Bu(t)$ ), 即得到Hippo矩阵 $A(t)$ , 和 $B(t)$
        <ul>
          <li>在计算过程中为了方便会做一些变量替换</li>
          <li>对于LegS的情况, $A(t)$ 不是time-invariant的, 不过, 可以写成 $A(t)=\frac{1}{t}A$ 的形式
            <ul>
              <li>S4的结构里用的是没有$\frac{1}{t}$ 的版本, 可以对应$w(t)=e^{-t}$ 和 $p_n(t)=L_n(e^{-t})$ . 按文章里的说法, 是实验有效之后才反过来找到对应的basis和measure🤔️</li>
            </ul>
          </li>
        </ul>
      </li>
    </ol>
  </li>
</ul>

<p>这个恢复信号的想法和VAE里面用latent space压缩图像信号的想法很相似.</p>
<h3 id="h3hungry-hungry-hippos-结构">H3(Hungry Hungry Hippos) 结构</h3>

<p><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240318042950.png" alt="Pasted image 20240318042950.png" width="400" /></p>

<ul>
  <li>和linear attention不同, H3里面KV并没把不同token的信息加在一起, 整个结构里, 不同token的信息只在SSM的地方进行交互</li>
  <li>SSM模块: 这两个SSM模块是为了语言模型精心设计的
    <ul>
      <li>其中Shift SSM指SSM中的状态转移矩阵$A$ 是个<a href="https://en.wikipedia.org/wiki/Shift_matrix">shift matrix</a> , 有 $A^k=0$ 的特点, 距离和当前token大于k的信息被自然的刷新, 在网络中用于记录较近的索引
        <ul>
          <li>代码: https://github.com/HazyResearch/H3/blob/main/src/models/ssm/ss_kernel_shift.py</li>
        </ul>
      </li>
      <li>Diag SSM指转移矩阵$A$是diagonal matrix, 保存全局记忆, 已经记录的信息不会随着新输入token刷新</li>
    </ul>
  </li>
  <li><a href="https://www.youtube.com/watch?v=TkOSKrlpnU4&amp;t=716s">视频</a> 里详细讲解了h3结构和associatve recall的任务(输入一个dict, 提问某个key对应的value)的关系: 也是我第一次知道q,k,v的名字和任务的联系</li>
</ul>

<h3 id="retnet">RetNet</h3>

<p><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240325140921.png" alt="Pasted image 20240325140921.png" width="700" /></p>

<ul>
  <li>在linear-attention的基础上增加了state的状态转移, 是对linear attention的一个巧妙的变形.</li>
  <li>试图用position embedding和D矩阵的exponential decay代替attention里的softmax的功能</li>
  <li><a href="https://github.com/Jamie-Stirling/RetNet/blob/main/src/retention.py">Rentention的代码</a> 写的很清晰</li>
</ul>

<h3 id="rwkv">RWKV</h3>

<ul>
  <li>time-mixing和channel-mixing两个模块交替: <a href="https://github.com/BlinkDL/ChatRWKV/blob/main/RWKV_in_150_lines.py">代码</a>
    <ul>
      <li>time-mixing是一个类linear attention的结构, 配一个特殊设计的state转移(也就是文章里的WKV operator)
        <ul>
          <li>这么设计state转移的动机, 我是不明白</li>
        </ul>
      </li>
      <li>channel-mixing是分成两路, 分别做 conv(token window_size=2) + linear + activation</li>
    </ul>
  </li>
  <li>RWKV做了配套的chat的很多相关代码工作</li>
</ul>

<h3 id="mamba">Mamba</h3>

<p><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240325123207.png" alt="Pasted image 20240325123207.png" width="300" /></p>

<ul>
  <li>基础的SSM模块改为S6(Selective S4)模块: 把SSM离散表示中$\Delta, B, C$ 改成time-variant. 以适应不同位置的信息密度.
    <ul>
      <li>我觉得这种想法和deformable convolution的想法类似, 都是对一些结构parametrize.</li>
    </ul>
  </li>
  <li>Mamba block:
    <ul>
      <li>用convolution和S6, 而不是attention进行信息交互.</li>
      <li>基于mamba block的大模型用更少的参数达到了和基于attention的大模型类似的效果.</li>
      <li>最近有一些文章用mamba block魔改去做视觉任务? 没有特别了解但是感觉很奇怪, 因为SSM本身有个顺序的结构…在视觉任务上这个顺序要怎么定义?</li>
    </ul>
  </li>
</ul>

<h2 id="reference">reference</h2>

<p>[1] Katharopoulos, Angelos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. “Transformers Are RNNs: Fast Autoregressive Transformers with <strong>Linear Attention</strong>.” arXiv, August 31, 2020. <a href="https://doi.org/10.48550/arXiv.2006.16236">https://doi.org/10.48550/arXiv.2006.16236</a>.</p>

<p>[2] Gu, Albert, Karan Goel, and Christopher Ré. “Efficiently Modeling Long Sequences with <strong>Structured State Spaces</strong>.” arXiv, August 5, 2022. <a href="http://arxiv.org/abs/2111.00396">http://arxiv.org/abs/2111.00396</a>. (S4)</p>

<p>[3] Gu, Albert, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Re. “<strong>HiPPO</strong>: Recurrent Memory with Optimal Polynomial Projections.” arXiv, October 22, 2020. <a href="https://doi.org/10.48550/arXiv.2008.07669">https://doi.org/10.48550/arXiv.2008.07669</a>.</p>

<p>[4] Gu, Albert, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher Ré. “How to Train Your <strong>HiPPO</strong>: State Space Models with Generalized Orthogonal Basis Projections.” arXiv, August 5, 2022. <a href="http://arxiv.org/abs/2206.12037">http://arxiv.org/abs/2206.12037</a>.</p>

<p>[5] Fu, Daniel Y., Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, and Christopher Ré. “<strong>Hungry Hungry Hippos</strong>: Towards Language Modeling with State Space Models.” arXiv, April 28, 2023. <a href="https://doi.org/10.48550/arXiv.2212.14052">https://doi.org/10.48550/arXiv.2212.14052</a>.</p>

<p>[6] Sun, Yutao, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. “<strong>Retentive Network</strong>: A Successor to Transformer for Large Language Models.” arXiv, August 9, 2023. <a href="https://doi.org/10.48550/arXiv.2307.08621">https://doi.org/10.48550/arXiv.2307.08621</a>.</p>

<p>[7] Peng, Bo, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, et al. “<strong>RWKV</strong>: Reinventing RNNs for the Transformer Era.” arXiv, December 10, 2023. <a href="https://doi.org/10.48550/arXiv.2305.13048">https://doi.org/10.48550/arXiv.2305.13048</a>.</p>

<p>[8] Gu, Albert, and Tri Dao. “<strong>Mamba</strong>: Linear-Time Sequence Modeling with Selective State Spaces.” arXiv.org, December 1, 2023. <a href="https://arxiv.org/abs/2312.00752v1">https://arxiv.org/abs/2312.00752v1</a>.</p>
<h2 id="其他讨论的链接">其他讨论的链接</h2>

<p>[1] https://blog.csdn.net/v_JULY_v/article/details/134923301</p>

<p>[2]  <a href="https://github.com/state-spaces/s4/issues/40">https://github.com/state-spaces/s4/issues/40</a> 作者对hippo的答疑</p>

<p>[3] https://www.youtube.com/watch?v=TkOSKrlpnU4&amp;t=716s H3作者的讲解</p>

<p>[3] https://www.zhihu.com/question/602564718/answer/3042600470 RWKV的知乎讨论</p>

<p>[4] https://cloud.tencent.com/developer/article/2377967 Mamba block的理解</p>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;title&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;icon&quot;=&gt;nil}]}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="deeplearning" /><category term="content" /><category term="SSM" /><category term="state space model" /><category term="state" /><category term="linear attention" /><category term="time series" /><category term="Mamba" /><category term="attention" /><summary type="html"><![CDATA[在token很多(大模型用的超长文本), 或者本身数据是时间序列(比如语音, 视频流)的情况下, attention里面的weight会带来$O(token^2)$ 的内存消耗. state space model可以解决这个问题.]]></summary></entry><entry><title type="html">blender学习: 做卡通描边</title><link href="https://roshameow.github.io//personal_homepage/docs/blender/blender-learning7/" rel="alternate" type="text/html" title="blender学习: 做卡通描边" /><published>2024-03-06T00:00:00+00:00</published><updated>2024-04-09T19:52:23+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/blender/blender-learning7</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/blender/blender-learning7/"><![CDATA[<h2 id="用solidify背面剔除法线翻转">用solidify+背面剔除+法线翻转</h2>

<p>给mesh增加厚度, 让描边材质只在厚度边缘生效</p>
<h3 id="步骤">步骤:</h3>

<p>参考<a href="https://www.bilibili.com/video/BV1Ju4y197yk">这个教学视频</a></p>

<ul>
  <li>shader:
    <ul>
      <li>选择一个主material的效果(Slot 1): 用Diffuse BSDF-&gt;shader to RGB-&gt;Color Ramp-&gt;Emission</li>
      <li>添加一个描边材质(Slot 2): 用Emission, 选择我们描边的颜色
        <ul>
          <li>对这个材质开启Backface Culling(背面剔除)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>在Modifier添加<a href="https://docs.blender.org/manual/en/latest/modeling/modifiers/generate/solidify.html">solidify</a>: 给mesh表面增加厚度
    <ul>
      <li><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240307120524.png" alt="Pasted image 20240307120524.png" width="100" /> <img src="/personal_homepage/docs/attachment/Pasted%20image%2020240307120542.png" alt="Pasted image 20240307120542.png" width="110" /></li>
      <li>调整thickness=-0.02m
        <ul>
          <li>offset默认是-1, 要让thickness和offset同方向(让增加的厚度mesh朝外凸)</li>
        </ul>
      </li>
      <li>Materials-&gt;Material Offset=1 :让solidify的厚度mesh采用我们的描边材质, 即下一个slot的material</li>
      <li>开启Normals-&gt;flip: 让颜色上到厚度mesh的内表面</li>
    </ul>
  </li>
  <li>结果: 对三维的模型描边有种新鲜的观感.
    <ul>
      <li><img src="/personal_homepage/docs/attachment/stoke.mp4" alt="stoke.mp4" width="300" /></li>
      <li>描边的前提是提取边缘. 如果是二维图像有很多做法, 比如自动提取图像的边缘合并. 对于我们自己建的模型当然也有办法提取边缘. 但是如果其他方式得到的模型(比如扫描得到的模型), 我们怎么识别边缘, 然后edit呢?</li>
    </ul>
  </li>
</ul>

<h2 id="blender使用技巧">blender使用技巧</h2>

<ul>
  <li>复制material和modifier:
    <ul>
      <li>按<a href="https://blender.stackexchange.com/questions/7044/copy-material-to-another-object">这个方法</a> 可以复制到全部, 但是没法选择materail的单个slot😠, 只能全部复制过去?</li>
    </ul>
  </li>
</ul>

<h2 id="其他描边效果">其他描边效果</h2>

<p>[1] https://svg-animation-booklet.vercel.app/chapter5.html#实现动画 svg描边动画</p>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;title&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;icon&quot;=&gt;nil}]}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="blender" /><category term="content" /><category term="material" /><category term="stoke" /><category term="modifier" /><category term="solidify" /><summary type="html"><![CDATA[用solidify+背面剔除+法线翻转]]></summary></entry><entry><title type="html">stable-diffusion的结构和微调模型</title><link href="https://roshameow.github.io//personal_homepage/docs/deeplearning/stable-diffusion6/" rel="alternate" type="text/html" title="stable-diffusion的结构和微调模型" /><published>2024-02-29T00:00:00+00:00</published><updated>2024-03-27T20:30:05+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/deeplearning/stable-diffusion6</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/deeplearning/stable-diffusion6/"><![CDATA[<h2 id="stable-diffusion结构和controlnet插件">stable-diffusion结构和controlnet插件</h2>

<p><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240306124200.png" alt="Pasted image 20240306124200.png" width="500" /></p>

<ul>
  <li>controlnet保持和stable-diffusion u-net上半部分相同的结构</li>
  <li>controlnet输入是和原图一样的hint图像
    <ul>
      <li>比如controlnet-openpose输入的不是关节坐标数据, 而是彩色的人体坐标图</li>
      <li>这也是像controlnet-tile, controlnet-inpaint等从任务角度明明不需要hint image还是要输入一个的原因</li>
    </ul>
  </li>
</ul>

<h2 id="resnetblock-spatialtransfomer-结构和插件">Resnetblock, SpatialTransfomer 结构和插件</h2>

<p><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240306123108.png" alt="Pasted image 20240306123108.png" width="300" /> <img src="/personal_homepage/docs/attachment/Pasted%20image%2020240306123128.png" alt="Pasted image 20240306123128.png" width="500" /></p>

<ul>
  <li>lora可以应用在网络的任何线性结构上, 比如:
    <ul>
      <li>attention里面的q,k,v,out层</li>
      <li>feedforward里面的linear层</li>
      <li>clip里面的mlp, fc层</li>
    </ul>
  </li>
  <li>这几种模型的作用的粒度不一样, 从小到大是: lora(线性层) &lt; Gligen=IpAdapter(SpatialTransformer层) &lt; Controlnet(U-net的block)
    <ul>
      <li>模型的size也是依次变大</li>
    </ul>
  </li>
</ul>

<h2 id="各种embedding编码方式">各种embedding编码方式</h2>

<table>
  <thead>
    <tr>
      <th>类型</th>
      <th>输入</th>
      <th>编码</th>
      <th>公式</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td> </td>
      <td>图像</td>
      <td>可训练的hint block网络(controlnet)<br />或clip vision(ipadapter)</td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td>文字</td>
      <td>clip</td>
      <td> </td>
    </tr>
    <tr>
      <td>一维特征 $\mathbb R$</td>
      <td>time</td>
      <td><a href="https://github.com/comfyanonymous/ComfyUI/blob/55f37baae85a5f3ef6d87743445fcce1f0477ba9/comfy/ldm/modules/diffusionmodules/model.py#L32">Sinusoidal Embedding</a>  <br />三角编码</td>
      <td>$[\sin\frac{t}{10000^{2n/d}},\cdots,\cos\frac{t}{10000^{2n/d}},\cdots]\in \mathbb R^d$<br />temperature 10000<br />token t<br />embedding dim d, $n\in [0,d/2]$</td>
    </tr>
    <tr>
      <td>多维特征 $\mathbb R^k$</td>
      <td>boundingbox<br />keypoints</td>
      <td><a href="https://github.com/gligen/GLIGEN/blob/f9dccb9c6cf48bad03c3666290a7dec8c5e58f3c/ldm/modules/diffusionmodules/util.py#L12">Fourier Embedding</a> (gligen)<br />就是在三角编码在k维度上做tile</td>
      <td> </td>
    </tr>
  </tbody>
</table>

<ul>
  <li>在gligen里面, 用text encoder编码的关键词embedding和对应的用Fourier Embedding编码的bounding box concat之后再用MLP融合成grounding token</li>
  <li>stable diffusion里面编码方式大部分都是不参与网络训练的</li>
</ul>

<h2 id="reference">reference</h2>
<p><span id="ref"></span>
[1] Hu, Edward J., Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. “<strong>LoRA</strong>: Low-Rank Adaptation of Large Language Models.” arXiv, October 16, 2021. <a href="https://doi.org/10.48550/arXiv.2106.09685">https://doi.org/10.48550/arXiv.2106.09685</a>.</p>

<p>[2] Zhang, Lvmin, and Maneesh Agrawala. “Adding Conditional <strong>Control</strong> to Text-to-Image Diffusion Models.” arXiv, February 10, 2023. <a href="https://doi.org/10.48550/arXiv.2302.05543">https://doi.org/10.48550/arXiv.2302.05543</a>.</p>

<p>[3] Ye, Hu, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. “<strong>IP-Adapter</strong>: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models.” arXiv.org, August 13, 2023. <a href="https://arxiv.org/abs/2308.06721v1">https://arxiv.org/abs/2308.06721v1</a>.</p>

<p>[4] Li, Yuheng, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. “<strong>GLIGEN</strong>: Open-Set Grounded Text-to-Image Generation.” arXiv, April 16, 2023. <a href="https://doi.org/10.48550/arXiv.2301.07093">https://doi.org/10.48550/arXiv.2301.07093</a>.</p>

<p>代码: 参考comfyui里<code class="language-plaintext highlighter-rouge">comfy.ldm.modules.diffusionmodules.openaimodel.py</code> 的实现, 除了controlnet是加在<code class="language-plaintext highlighter-rouge">condition</code> 流里面, 其他几种都是通过<code class="language-plaintext highlighter-rouge">model.patches</code> 切换网络分支实现的.</p>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;title&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;icon&quot;=&gt;nil}]}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="deeplearning" /><category term="content" /><category term="attention" /><category term="stable-diffusion" /><category term="lora" /><category term="controlnet" /><category term="ipadapter" /><category term="gligen" /><summary type="html"><![CDATA[stable-diffusion结构和controlnet插件]]></summary></entry></feed>