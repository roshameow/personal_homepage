<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://roshameow.github.io//personal_homepage/atom.xml" rel="self" type="application/atom+xml" /><link href="https://roshameow.github.io//personal_homepage/" rel="alternate" type="text/html" /><updated>2024-05-21T02:51:29+00:00</updated><id>https://roshameow.github.io//personal_homepage/atom.xml</id><title type="html">Liu, Wen’s Home Page</title><subtitle>Work, Experiments and Ideas.</subtitle><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;&quot;/docs/images/logo.svg&quot;, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil}</name><email>w.liuatnk@gmail.com</email></author><entry><title type="html">FFT计算</title><link href="https://roshameow.github.io//personal_homepage/docs/algorithm/fft/" rel="alternate" type="text/html" title="FFT计算" /><published>2024-05-19T00:00:00+00:00</published><updated>2024-05-20T21:02:00+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/algorithm/fft</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/algorithm/fft/"><![CDATA[<ul>
  <li>fourier Series 定义:
    <ul>
      <li>$F(f)(u)=\int_{-\infty}^{\infty} f(x)e^{-2\pi i x u} dx$</li>
      <li>2维：$G(p,q)=F(g(x,y)) = \int\int^\infty_\infty g(x,y)e^{-i2\pi(px+qy)}dxdy$</li>
    </ul>
  </li>
  <li>离散形式: 信号$x$ 的FFT 信号$X$
    <ul>
      <li>$X_k=\sum_{m=0}^{N-1}x_m\cdot e^{-i\cdot 2\pi km/N}=\sum_{m=0}^{N-1}x_m\cdot TW(N,k)^m$
        <ul>
          <li>$N$ 是信号长度</li>
          <li>$TW(N,k) = e^{-i*2k\pi/N}$ 是<a href="https://en.wikipedia.org/wiki/Twiddle_factor#:~:text=A%20twiddle%20factor%2C%20in%20fast,papers%20of%20the%20FFT%20literature.">FFT 的twiddle factor(旋转因子)</a>
            <ul>
              <li>$TW(N,k)=\cos(-2k\pi/N)+i\cdot \sin(-2k\pi/N)=TW_r(N,k)+i\cdot TW_i(N,k)$</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="butterfly-diagram">butterfly diagram</h3>

<p>利用fft的对称性和周期性</p>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;&quot;/docs/images/logo.svg&quot;, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="algorithm" /><category term="content" /><summary type="html"><![CDATA[fourier Series 定义: $F(f)(u)=\int_{-\infty}^{\infty} f(x)e^{-2\pi i x u} dx$ 2维：$G(p,q)=F(g(x,y)) = \int\int^\infty_\infty g(x,y)e^{-i2\pi(px+qy)}dxdy$ 离散形式: 信号$x$ 的FFT 信号$X$ $X_k=\sum_{m=0}^{N-1}x_m\cdot e^{-i\cdot 2\pi km/N}=\sum_{m=0}^{N-1}x_m\cdot TW(N,k)^m$ $N$ 是信号长度 $TW(N,k) = e^{-i*2k\pi/N}$ 是FFT 的twiddle factor(旋转因子) $TW(N,k)=\cos(-2k\pi/N)+i\cdot \sin(-2k\pi/N)=TW_r(N,k)+i\cdot TW_i(N,k)$]]></summary></entry><entry><title type="html">劳动仲裁流程和资料整理</title><link href="https://roshameow.github.io//personal_homepage/docs/affair/labor-disputes-arbitration/" rel="alternate" type="text/html" title="劳动仲裁流程和资料整理" /><published>2024-05-15T00:00:00+00:00</published><updated>2024-05-21T18:45:39+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/affair/labor-disputes-arbitration</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/affair/labor-disputes-arbitration/"><![CDATA[<h2 id="材料">材料</h2>

<ul>
  <li>信息:
    <ul>
      <li>身份证复印件</li>
      <li>公司注册信息: 在<a href="https://shiming.gsxt.gov.cn/">国家企业信用信息公示系统</a> 查找公司信息并打印</li>
    </ul>
  </li>
  <li>证据清单
    <ul>
      <li>社保缴费记录:  <a href="https://zwdtuser.sh.gov.cn/uc/login/login.jsp?redirect_uri=&amp;type=">一网通办</a> 登陆打印 参保人员城镇职工基本养老保险缴费情况</li>
      <li>银行流水: 在银行app里就可以打印</li>
      <li>劳动合同</li>
      <li>在企业微信里: 企业微信里的内容很难作为证据, 1. 都是聊天格式, 自己重新整理是没有法律效力的, 2. 而且企业微信被人事踢出后backup也没法恢复
        <ul>
          <li>工资条</li>
          <li>聊天记录</li>
          <li>打卡记录: 企业微信可以导出2个月的, 很麻烦</li>
          <li>工时统计表: 没用到, 没研究怎么导出</li>
          <li>周报邮件: 没用到</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>申请书
    <ul>
      <li>申请人信息: (姓名、性别、出生日期、身份证号码、住址、联系方式）</li>
      <li>被申请人信息: (单位名称、统一社会信用代码、法定代表人、单位地址、联系方式）</li>
      <li>仲裁请求(需要计算赔偿)
        <ol>
          <li>裁决劳动关系</li>
          <li>请求被申请人支付拖欠的工资和年终奖 x元（截至申请仲裁之日）；</li>
          <li>请求支付被动解除劳动合同的经济补偿金x元。</li>
        </ol>
      </li>
      <li>事实与理由
        <ul>
          <li>按照模版写</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>地址送达确认书</li>
</ul>

<p>我遇到的情况: 1. 续签劳动合同没给是按照签订了劳动合同处理, 不能索要赔偿, 用社保记录证明劳动关系即可 2. 除了社保记录和银行流水, 其它跟公司的文件都很难作为证据. 但是因为我有收集, 仲裁委的工作人员和调解员迅速的了解了我的情况和薪资结构, 所以应该还算有点用的 3. 在职的情况没法请求经济补偿金, 要先给公司发被动解除劳动合同的通知书 4. 我同意了减少补偿金离职, 所以也就没经历后面的流程了. 工作人员对我的材料给了很具体的指导, 总共一个小时就结束了.</p>

<h2 id="法律依据">法律依据</h2>

<p><a href="https://www.gov.cn/flfg/2007-06/29/content_669394.htm">劳动合同法</a></p>

<p>跟被动离职有关的是: 第38条, 46条, 47条</p>

<h2 id="相关的机构">相关的机构</h2>

<ul>
  <li>xx区劳动仲裁委</li>
  <li>劳动监察大队: 投诉公司欠薪, 处理结果一般比较慢</li>
  <li>地方调解(xx区xx地社区事务受理服务中心): 调解员可以代为联系公司, 同意牺牲一部分补偿金就可以快速结束流程</li>
  <li>工会: 提供法律援助</li>
</ul>

<h2 id="身在其中的感触">身在其中的感触</h2>

<p>公司是15号发工资, 因为公司在工资发放上已经多次食言, 已读不回, 本来已经决定这是最后一次机会, 15号到了一定要去仲裁, 在这之前就应该写好材料.</p>

<p>我对公司产生过侥幸心理: 中间大概10多号的时候, 同事跟我说15号可能会发工资. 虽然我当时有理有据的反驳了, 内心还是松懈了下来, 希望15号可以正常发工资不用去仲裁. 然后15号果然如我本来所料想继续拖欠, 结果还是要临时整理资料.</p>

<p>好在, 资料并不复杂, 只要一鼓作气就可以摆脱和现在公司这种不健康的关系.</p>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;&quot;/docs/images/logo.svg&quot;, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="affair" /><category term="content" /><summary type="html"><![CDATA[材料]]></summary></entry><entry><title type="html">blender学习: 用布料系统做膨胀效果</title><link href="https://roshameow.github.io//personal_homepage/docs/blender/blender-learning12/" rel="alternate" type="text/html" title="blender学习: 用布料系统做膨胀效果" /><published>2024-05-13T00:00:00+00:00</published><updated>2024-05-16T19:48:22+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/blender/blender-learning12</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/blender/blender-learning12/"><![CDATA[<p>参考<a href="https://www.bilibili.com/video/BV13C41177Ze">这个教学</a></p>

<h3 id="建模">建模</h3>

<p>球和boundary两部分</p>

<ul>
  <li>新建一个Ico Sphere(棱角球)</li>
  <li>在Edit Mode选中一些顶点作为boundary, 设为顶点组</li>
  <li>给棱角球做一个表面细分并应用</li>
  <li>在Edit Mode里用Bevel把boundary拉出一点宽度, 把顶点组改为新的boundary
    <ul>
      <li>要先应用表面细分再Bevel? 不然boundary的mesh会变得很复杂</li>
    </ul>
  </li>
  <li>在原位把boundary复制一份(Shift+D), 和球分离(P), 稍微拉大一点(S)</li>
  <li>给boundary添加一个2的表面细分</li>
  <li>设置shade smooth</li>
</ul>

<h3 id="制作膨胀效果">制作膨胀效果:</h3>

<ul>
  <li>选择Physics-&gt;Cloth
    <ul>
      <li>Pressure = 25</li>
      <li><a href="https://docs.blender.org/manual/en/latest/physics/cloth/settings/shape.html">Shape</a>选择boundary的顶点组
        <ul>
          <li>Shrinking Factor = -0.3 负数表示要cloth膨胀</li>
        </ul>
      </li>
      <li>Field Weights-&gt;Gravity=0</li>
    </ul>
  </li>
  <li>结果:
    <ul>
      <li><img src="/personal_homepage/docs/attachment/inhale.mp4" alt="inhale.mp4" width="400" /></li>
    </ul>
  </li>
</ul>

<h3 id="用到的blender的一些快捷键">用到的blender的一些快捷键</h3>

<ul>
  <li>Command+A(应用modifier)</li>
  <li>Shift+D(<a href="https://docs.blender.org/manual/en/2.82/scene_layout/object/editing/duplication.html">Duplication</a>): 复制物体
    <ul>
      <li>RMB(右键): 保留在原来的位置</li>
    </ul>
  </li>
  <li>P(<a href="https://docs.blender.org/manual/en/latest/modeling/meshes/editing/mesh/separate.html">分离</a>)
    <ul>
      <li>把选中的顶点建一个新的object</li>
    </ul>
  </li>
  <li>Command+P(<a href="https://docs.blender.org/manual/en/latest/scene_layout/object/editing/parent.html">Parent</a>)
    <ul>
      <li>把后选中的object设置为先选中的object的parent </li>
    </ul>
  </li>
  <li>Ctrl+B( <a href="https://docs.blender.org/manual/en/2.81/modeling/meshes/editing/subdividing/bevel.html#:~:text=The%20Bevel%20tool%20smooths%20the,above%20to%20run%20the%20tool.">Bevel</a>, 拉伸, 倒角): 把一个edge变成多个edge, 使物体边缘光滑
    <ul>
      <li><img src="https://docs.blender.org/manual/zh-hans/2.81/_images/modeling_meshes_editing_subdividing_bevel_example-4.png" alt="drawing" width="150" /></li>
      <li>按Shift微调: 移动的幅度降1k倍</li>
    </ul>
  </li>
</ul>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;&quot;/docs/images/logo.svg&quot;, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="blender" /><category term="content" /><category term="physics" /><category term="shortcut" /><summary type="html"><![CDATA[参考这个教学]]></summary></entry><entry><title type="html">给网页添加 logo</title><link href="https://roshameow.github.io//personal_homepage/docs/photo/jekyll-add-logo/" rel="alternate" type="text/html" title="给网页添加 logo" /><published>2024-05-09T00:00:00+00:00</published><updated>2024-05-10T20:36:50+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/photo/jekyll-add-logo</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/photo/jekyll-add-logo/"><![CDATA[<p><img src="/personal_homepage/docs/attachment/logo.svg" alt="logo.svg" width="200" /></p>

<h2 id="用照片制作svg">用照片制作svg</h2>

<p>这次只描了轮廓</p>

<p><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240510121915.png" alt="Pasted image 20240510121915.png" width="200" /></p>

<ul>
  <li>用钢笔工具和选择工具画路径:
    <ul>
      <li>在最凸，最凹的地方加锚点</li>
      <li>添加锚点时拉拽得到切线</li>
      <li>按command 单独调整锚点和切线的端点</li>
      <li>option调整切线</li>
      <li>用直接选择工具(白色箭头)调整锚点位置</li>
      <li>用路径选择工具(黑色箭头)复制路径</li>
      <li>用颜色填充功能实时查看路径闭包</li>
    </ul>
  </li>
  <li>导出:
    <ul>
      <li>选中填充图层复制svg</li>
    </ul>
  </li>
</ul>

<h2 id="设置网页的favicon">设置网页的favicon</h2>

<ul>
  <li>在对应html设置icon:  <code class="language-plaintext highlighter-rouge">&lt;link rel="shortcut icon" type="image/x-icon" href="/personal_homepage/docs/images/logo.ico"&gt;</code>
    <ul>
      <li>我的路径是<code class="language-plaintext highlighter-rouge">head.html-&gt;head-custom.html</code></li>
    </ul>
  </li>
</ul>

<h2 id="资源">资源</h2>

<p>[1]  <a href="https://www.bilibili.com/video/BV1pP4y1R7r6/">https://www.bilibili.com/video/BV1pP4y1R7r6</a> 钢笔工具使用</p>

<p>[2]  <a href="https://www.taoxuemei.com/chuli/ps/754.html">https://www.taoxuemei.com/chuli/ps/754.html</a> 钢笔工具画的形状-&gt;svg</p>

<p>[3]  <a href="https://zhuanlan.zhihu.com/p/446194623">https://zhuanlan.zhihu.com/p/446194623</a> 路径转形状</p>

<p>[4]  https://stackoverflow.com/questions/30551501/unable-to-set-favicon-using-jekyll-and-github-pages  关于icon路径的讨论</p>

<p>[5]  https://convertio.co/download/9dceab468ba01473f3d49fc765b6f73f983c7f/ svg转ico</p>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;&quot;/docs/images/logo.svg&quot;, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="photo" /><category term="content" /><category term="jekyll" /><category term="photoshop" /><category term="logo" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">各种 moving function</title><link href="https://roshameow.github.io//personal_homepage/docs/algorithm/moving_function/" rel="alternate" type="text/html" title="各种 moving function" /><published>2024-05-05T00:00:00+00:00</published><updated>2024-05-16T00:59:20+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/algorithm/moving_function</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/algorithm/moving_function/"><![CDATA[<h2 id="场景">场景</h2>
<h3 id="moving-mean">moving mean</h3>

<p>leetcode 239</p>
<ul>
  <li>用一个queue保存window里面的数据</li>
  <li>每次+新进的数据-出去的数据</li>
</ul>

<h3 id="moving-median">moving median</h3>

<p>leetcode 480</p>

<ul>
  <li>naive: 保存window里的所有数据,排序, 找出median
    <ul>
      <li>每次更新进出data的顺序</li>
    </ul>
  </li>
  <li>优化1: 当进出的data在median同一侧时, 不需要更新median</li>
  <li>优化2: 不需要严格的排序, 只需要维护median两边堆的结构, 就可以找到left的最大值, 和right的最小值
    <ul>
      <li>用window记录进出的data</li>
      <li>同样的data只存一个位置, 通过一个counter记录data的重复次数, 次数=0就是待删除的data</li>
      <li>记录left_heap, right_heap的实际size, 当两边size偏离的时候移动heap</li>
      <li>如果删除/移动的data在堆顶, 需要更新堆(把堆顶待删除的data全部删除)</li>
    </ul>
  </li>
  <li>错误方向:
    <ul>
      <li>本来想像moving max一样只保留时间近的, median附近的data. 但是moving median中, 所有的元素都可能在之后变得重要, 所以要全部保留的
        <ul>
          <li>如果window_size=k, 需要&lt;2/k个更新的data, 或&gt;2/k个更新的data, 才能确定这个data不可能成为median, 这个条件达成的概率很小</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="moving-minmax">moving min/max</h3>

<p>leetcode 239</p>

<p>以max为例</p>
<ul>
  <li>naive: 保存window里的所有数据, 找出max</li>
  <li>优化1: 不需要每次计算min/max, 只需要当前max出window的时候重新计算
    <ul>
      <li>记录当前max的值和index</li>
      <li>con: 这样计算速度不稳定
        <ul>
          <li>比如: window_size较大, 而数据又是降序排列的时候, 每新进一个数据都要计算一次max, 时间是O(k)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>优化2: window里不需要全部保存, 用deque结构, 只需要按顺序保存可能成为local max的data和index (不符合降序的data永远不会成为max)
    <ul>
      <li>当新data进入deque时, 把deque里&lt;=它的data删除</li>
      <li>deque里的当前max生命耗尽时, 需要移出deque
        <ul>
          <li>用保存的index判断生命耗尽</li>
          <li>或保存一份window, 如果window移除data = deque的第一个data, 把data从deque里移除
            <ul>
              <li>deque里不会有重复data</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>deque里的第一个data就是local max</li>
      <li>例子: <img src="/personal_homepage/docs/attachment/Pasted%20image%2020240515165236.png" alt="Pasted image 20240515165236.png" width="300" />
        <ul>
          <li>data6进入window后, 4,5就不可能成为local max了</li>
          <li>data7进入window后, 6也不可能成为local max了</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>应用:
    <ul>
      <li>动态调整画图界面适应data</li>
      <li>mormophic filter去除基线漂移</li>
    </ul>
  </li>
</ul>

<h2 id="结构">结构</h2>

<h3 id="deque双向队列">deque(双向队列)</h3>

<p>支持在两端进出</p>
<ul>
  <li>用途:
    <ul>
      <li>实现LRU缓存(leetcode 146)</li>
    </ul>
  </li>
</ul>

<h3 id="heap优先队列">heap(优先队列)</h3>

<ul>
  <li>priority_queue(C++), heapq(python)</li>
  <li><strong>heap的数据结构</strong>:
    <ul>
      <li><img src="https://upload.wikimedia.org/wikipedia/commons/c/c4/Max-Heap-new.svg" alt="drawing" width="200" /></li>
      <li>parent比children大的完备二叉树(以最大堆为例)
        <ul>
          <li>完备是指, 从上到下每层填满, 最后一层靠左排列</li>
        </ul>
      </li>
      <li>可以用数组表示: 第 $i$个节点的左子节点编号为 $2i+1$，右子节点编号为 $2i+2$，父节点编号为 $⌊\frac{i−1}{2}⌋$
        <ul>
          <li>完备的=可以连续存储, 数组可以在任意位置访问-&gt;可以做swap</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>heap支持的操作</strong>: 都是把中间data和最后data互换, 然后在最后增删, 所以不会破坏完备性
    <ul>
      <li>新插入一个元素(“上浮”操作): 把新data添到heap的尾部, 不断和parent比较, 并swap到合适位置
        <ul>
          <li>只需要O(logk) 的swap(如果要排序就需要O(k)个swap)</li>
        </ul>
      </li>
      <li>移除heap 顶的元素(“下沉”操作): 和其children比较, 跟更大的那个swap, 直到到达底部, 删除</li>
    </ul>
  </li>
  <li><strong>用途</strong>:
    <ul>
      <li>调度器, 用来实时添加任务, 和取出最合适的任务进行执行
        <ul>
          <li>比如优先级最高, 或截止时间最早的任务</li>
        </ul>
      </li>
      <li>堆排序: 每轮找到最小的data排在后面, 时间复杂度是O(nlogn)
        <ul>
          <li>时间稳定, inplace操作, 适合大数据量, 内存有限的场景</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="hashtable">hashtable</h3>

<ul>
  <li>unordered_map(C++), Counter(python), dict, set</li>
</ul>

<h3 id="multiset">multiset</h3>

<ul>
  <li>multiset(C++)
    <ul>
      <li>类似set, 允许重复的data, 可以按顺序遍历</li>
    </ul>
  </li>
</ul>

<h2 id="其它有用的链接">其它有用的链接</h2>

<p>[1]  https://cplusplus.com/reference/unordered_map/unordered_map/</p>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;&quot;/docs/images/logo.svg&quot;, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="algorithm" /><category term="content" /><category term="leetcode" /><category term="basic" /><category term="data_structure" /><summary type="html"><![CDATA[场景 moving mean]]></summary></entry><entry><title type="html">小面积光流传感器算法测试 (三) – 滤波</title><link href="https://roshameow.github.io//personal_homepage/docs/algorithm/optical_flow_train3/" rel="alternate" type="text/html" title="小面积光流传感器算法测试 (三) – 滤波" /><published>2024-04-28T00:00:00+00:00</published><updated>2024-05-05T15:17:18+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/algorithm/optical_flow_train3</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/algorithm/optical_flow_train3/"><![CDATA[<p>在高速场景下, 每次中断收集的数据是光流的累加值, 其实本来就相当于一个滤波…况且在硬件有限的条件下, 复杂的滤波没有什么实用价值.</p>

<h2 id="实验">实验</h2>

<table>
  <thead>
    <tr>
      <th>方法</th>
      <th> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ConfidenceFilter</td>
      <td>输出光流时同时输出一个置信度, 如果置信度较低, 选择历史值而不是测量值</td>
    </tr>
    <tr>
      <td>FIR</td>
      <td>在临近window上的一个线性filter</td>
    </tr>
    <tr>
      <td>Kalman</td>
      <td>在gain值稳定后, kalman滤波其实相当于一个<a href="https://en.wikipedia.org/wiki/Infinite_impulse_response">IIR filter</a></td>
    </tr>
  </tbody>
</table>

<ul>
  <li>对于整像素的光流结果, 相当于Kalman的MeasureNoise有一部分量化噪声, 应该加大MeasureNoiseCov参数的设置, 不过实验中看不出区别</li>
  <li>Kalman在反应速度和平滑度上都要好于FIR的</li>
</ul>

<h2 id="kalman滤波原理"><a href="https://en.wikipedia.org/wiki/Kalman_filter#:~:text=The%20Kalman%20filter%20produces%20an,uncertainty%20are%20%22trusted%22%20more.">Kalman滤波</a>原理</h2>

<h3 id="ssmstate-space-model">SSM(state space model)</h3>

<ul>
  <li><a href="https://en.wikipedia.org/wiki/State-space_representation">状态空间(state space)</a> : 用state vector记录历史的input, 而不是记录所有的历史token
    <ul>
      <li><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240317183044.png" alt="Pasted image 20240317183044.png" width="200" /></li>
    </ul>
  </li>
  <li>连续表示:
    <ul>
      <li>linear state space model的一般形式:
        <ul>
          <li>$\dot x(t)=A(t)x(t)+B(t)u(t)$ (用input u更新状态 x)</li>
          <li>$y(t)=C(t)x(t)+D(t)u(t)$  (用状态x, 生成output y)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="滤波步骤">滤波步骤</h3>

<table>
  <thead>
    <tr>
      <th>变量</th>
      <th>input</th>
      <th>predict</th>
      <th>correct</th>
      <th>output</th>
      <th>使用</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>state</td>
      <td> </td>
      <td>$\hat x_n$</td>
      <td>$x_n$</td>
      <td> </td>
      <td>[x,y,dx,dy]</td>
    </tr>
    <tr>
      <td>measure</td>
      <td>$u_n$</td>
      <td>$\hat y_n$</td>
      <td> </td>
      <td>$y_n$</td>
      <td>[x, y]</td>
    </tr>
  </tbody>
</table>

<table>
  <thead>
    <tr>
      <th>参数</th>
      <th> </th>
      <th>使用</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>transition matrix P</td>
      <td>state -&gt; predict state</td>
      <td>\begin{bmatrix}1&amp;0&amp;1&amp;0\\0&amp;1&amp;0&amp;1\\0&amp;0&amp;1&amp;0\\0&amp;0&amp;0&amp;1\end{bmatrix}</td>
    </tr>
    <tr>
      <td>measure matrix $C$</td>
      <td>state -&gt; measure</td>
      <td>\begin{bmatrix}1&amp;0&amp;0&amp;0\\0&amp;1&amp;0&amp;0\end{bmatrix}</td>
    </tr>
    <tr>
      <td>gain $g_n$</td>
      <td>measure error -&gt; state error</td>
      <td>根据仿真模型和运动模型更新</td>
    </tr>
  </tbody>
</table>

<p>参考<a href="https://github.com/opencv/opencv/blob/4.x/modules/video/src/kalman.cpp">opencv里面的写法</a></p>
<ul>
  <li><strong>predict</strong>:
    <ul>
      <li>$\hat x_n=P(x_{n-1})$ 是state 在当前时间的predict</li>
    </ul>
  </li>
  <li><strong>correct</strong>:
    <ul>
      <li>$x_{n}-\hat x_{n}=g_n\cdot(u_n-\hat y_{n})=g_n\cdot(u_n-C\hat x_n)$
        <ul>
          <li>state的error和measure的error(measure-predict) 是线性关系
            <ul>
              <li><strong>如果measure从predict偏移越少, state越保持predict的结果</strong></li>
            </ul>
          </li>
          <li>写成<strong>SSM</strong>的形式: $x_n=\hat x_{n}+g_n\cdot(u_n-C\hat x_n)=(I-g_n\cdot C)\hat x_n +g_n\cdot u_n=(I-g_n\cdot C)P x_{n-1} +g_n\cdot u_n$</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="通过运动模型更新gain---g_n">通过运动模型更新gain   $g_n$</h3>

<table>
  <thead>
    <tr>
      <th>变量</th>
      <th>仿真</th>
      <th>predict</th>
      <th>correct</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>state</td>
      <td>$x_n^r$</td>
      <td>$\hat x_n^r$</td>
      <td>$\tilde x_n$</td>
    </tr>
    <tr>
      <td>error</td>
      <td> </td>
      <td>$\hat \epsilon_n=x_n^r-\hat x_n^r$</td>
      <td>$\epsilon_n=x_n^r-\tilde x_n$</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>仿真模型:
    <ul>
      <li>$x_n^r$ 是<strong>仿真的state</strong></li>
      <li>$x_{n}^r=P(x_{n-1}^r)+ProcessNoise$(状态转移噪声)</li>
      <li>$u_n^r=C(x_n^r)+MeasureNoise$    测量值仿真结果
        <ul>
          <li>假设ProcessNoise和MeasureNoise都是高斯分布的<a href="https://en.wikipedia.org/wiki/Random_variable">Random Variable</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>计算模型:
    <ul>
      <li>predict:
        <ul>
          <li>$\hat x_n^r=P(\tilde x_{n-1})$  是predict值</li>
          <li>PredictError:
            <ul>
              <li>$\hat \epsilon_n=x_n^r-\hat x_n^r=P(x_{n-1}^r)+ProcessNoise-P(\tilde x_{n-1})=P\epsilon_{n-1}+ProcessNoise$ ①</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>correct:
        <ul>
          <li>$\tilde x_{n}=\hat x_{n}^r+g_n\cdot(u_n^r-C\hat x_n^r)=\hat x_{n}^r+g_n\cdot(C\cdot \hat\epsilon_n+MeasureNoise)$    是<strong>correct之后的state</strong></li>
          <li>CorrectError:
            <ul>
              <li>$\epsilon_n= x_n^r-\tilde x_n =\hat \epsilon_{n}-g_n\cdot( C\cdot \hat \epsilon_{n}+MeasureNoise)$ ②</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Gain: argmin(CorrectError) 
        <ul>
          <li>
\[\begin{align}g_n=\text{argmin} ||x_{n}^r-\tilde x_{n}||&amp;=\text{argmin} ||\hat \epsilon_n-g_n\cdot(C\cdot\hat \epsilon_n + MeasureNoise)||\\&amp;=(\hat \epsilon_n)(C\hat \epsilon_n+MeasureNoise)^T((C\hat \epsilon_n+MeasureNoise)(C\hat \epsilon_n+MeasureNoise)^T)^{-1} &amp; (\text{因为 }\text{argmin}||xA-b||=(bA^T)(AA^T)^{-1})\\&amp;=(\hat \epsilon_n^{cov}C^T)(C\hat \epsilon_n^{cov} C^T+MeasureNoise^{cov})^{-1}&amp;(\text{展开, }mean(MeasureNoise)=0)\end{align}\]
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>特点:
    <ul>
      <li>gain  $g_n$ 和实际采集的数据$u_n$ 无关, 是由仿真过程和计算过程确定的</li>
      <li>① , ② 为$\hat \epsilon_n$ 的更新过程</li>
      <li>在gain的计算中, 每次更新$\hat \epsilon_n^{cov}, \epsilon_n^{cov}$ 就可以了</li>
      <li>gain 在一段时间后会收敛到一个固定的值</li>
    </ul>
  </li>
</ul>

<h3 id="推导least-square">推导(Least Square)</h3>

<p>定义 L2 norm: 
	\(||A|| = ∑_{ij} A_{ij}^2  = trace(A^TA)\)</p>

<ul>
  <li>
    <p>Normal Least Square: 
  \(\min_x ||Ax-b|| = \min_x (Ax-b)^T(Ax-b)= \min_x (x^T(A^TA)x - 2x^T(A^Tb))\)</p>
  </li>
  <li>其中:
    <ul>
      <li>$\partial_x x^T(G)x = 2Gx   \ (\partial_{x_k} \sum G_{ij} x_i\cdot x_j = \sum G_{ik} x_i+\sum G_{kj} x_j\text{ related part are one for k-th row and one for k-th col)}$</li>
      <li>$\partial_x x^T(A^T b) = A^T b$</li>
    </ul>
  </li>
  <li>得到: $(A^TA)^{-1}(A^Tb)$</li>
</ul>

<p>变形:</p>

\[\text{argmin}_x ||xA-b||=\text{argmin}_{x^T} ||A^Tx^T-b^T||=((AA^T)^{-1}(Ab^T))^T=(bA^T)((AA^T)^{-1})^T=(bA^T)(AA^T)^{-1}\]

<h2 id="代码">代码</h2>

<p><a href="https://gist.github.com/roshameow/bee4e4ebe065cc3159094590bd873eb1#file-confidence_filter-py"><strong>confidence_filter.py</strong></a> <a href="https://gist.github.com/roshameow/bee4e4ebe065cc3159094590bd873eb1#file-fir-py"><strong>fir.py</strong></a> <a href="https://gist.github.com/roshameow/bee4e4ebe065cc3159094590bd873eb1#file-kalman-py"><strong>kalman.py</strong></a></p>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;&quot;/docs/images/logo.svg&quot;, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="algorithm" /><category term="content" /><category term="kalman" /><category term="optical_flow" /><category term="filter" /><category term="norm" /><summary type="html"><![CDATA[在高速场景下, 每次中断收集的数据是光流的累加值, 其实本来就相当于一个滤波…况且在硬件有限的条件下, 复杂的滤波没有什么实用价值.]]></summary></entry><entry><title type="html">小红书学到的几种图片调色 (二)</title><link href="https://roshameow.github.io//personal_homepage/docs/photo/photo-color1/" rel="alternate" type="text/html" title="小红书学到的几种图片调色 (二)" /><published>2024-04-27T00:00:00+00:00</published><updated>2024-05-14T00:21:54+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/photo/photo-color1</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/photo/photo-color1/"><![CDATA[<h2 id="人物美白">人物美白</h2>

<p><a href="https://www.xiaohongshu.com/explore/661ab1fb000000000401bfe2">教程</a></p>

<ul>
  <li>cameraRaw滤镜
    <ul>
      <li>基本:
        <ul>
          <li>
            <ul>
              <li>色温, + 色调</li>
            </ul>
          </li>
          <li>
            <ul>
              <li>曝光, + 对比度, -高光</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>混色器:
        <ul>
          <li>色相: + 红色, -橙色, –蓝色</li>
          <li>明亮度: -红色</li>
        </ul>
      </li>
      <li>校准:
        <ul>
          <li>绿原色: +色相</li>
          <li>蓝原色: +饱和度</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p>我明白了这件事的难度, 肤色和环境光的作用太subtle了, 美的定义又太多样了. 难怪现在无论什么滤镜都没法把所有人统一的变漂亮.</p>

<h2 id="梦幻发光">梦幻发光</h2>

<p><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240428143139.png" alt="Pasted image 20240428143139.png" width="200" /> <img src="/personal_homepage/docs/attachment/Pasted%20image%2020240428143157.png" alt="Pasted image 20240428143157.png" width="200" /> <img src="/personal_homepage/docs/attachment/Pasted%20image%2020240428143227.png" alt="Pasted image 20240428143227.png" width="200" /></p>

<p><a href="https://www.xiaohongshu.com/explore/660fbca9000000001a014c53">教程</a></p>

<ul>
  <li>调整画面颜色(增加绿色): 可选颜色, 在 <a href="https://en.wikipedia.org/wiki/CMYK_color_model">CMYK 颜色</a> 调整
    <ul>
      <li>黄色: + 青色,黄色,黑色  -洋红</li>
      <li>绿色: + 青色,黄色  -洋红,黑色</li>
    </ul>
  </li>
  <li>给高光部分做高斯模糊</li>
  <li>混合模式改为变亮</li>
  <li>和blender里面的<a href="https://docs.blender.org/manual/en/latest/render/eevee/render_settings/bloom.html">bloom(辉光)</a> 功能原理一样</li>
</ul>

<h2 id="赛博朋克">赛博朋克</h2>

<p><a href="https://www.xiaohongshu.com/explore/65be0795000000002c015bf1">教程</a></p>
<ul>
  <li>增加暗处清晰度</li>
  <li>把整体颜色调成偏蓝, 偏红</li>
  <li>在阴影,高光的地方分别加冷暖色</li>
  <li>在混色器里把所有附近颜色调的偏青色, 洋红</li>
</ul>

<p>这效果并不好看…</p>

<h2 id="eva红色天空">EVA红色天空</h2>

<p><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240513161940.png" alt="Pasted image 20240513161940.png" width="300" /> <img src="/personal_homepage/docs/attachment/Pasted%20image%2020240513161959.png" alt="Pasted image 20240513161959.png" width="300" /></p>

<p><a href="https://www.bilibili.com/video/BV1Xj411r7P5/">教程</a></p>

<ul>
  <li>把颜色调成好看的红橙色</li>
</ul>

<h2 id="photoshop快捷键">photoshop快捷键</h2>

<ul>
  <li>command+alt+shift+E(盖印)
    <ul>
      <li>把效果和图层合并生成一个新图层</li>
    </ul>
  </li>
  <li>command+alt+2(提取高光)
    <ul>
      <li>command+J 可以把高光变成一个新图层</li>
    </ul>
  </li>
</ul>

<h2 id="可选颜色功能">可选颜色功能</h2>

<ul>
  <li>根据选中的颜色设置一个mask, 只调整mask之中的颜色
    <ul>
      <li>颜色容差控制mask边缘的模糊程度</li>
      <li>CMYK调整RGB单个通道对应的互补色</li>
    </ul>
  </li>
  <li>可以用颜色取样工具监测颜色的变化</li>
</ul>

<h2 id="camerarawacr参数理解">CameraRaw(ACR)参数理解</h2>

<ol>
  <li>基本
    <ol>
      <li>色温(Temperature)/色调(Tint): 改变<strong>图片整体</strong>hue
        <ol>
          <li>色温调整蓝/黄, 色调调整洋红/绿</li>
        </ol>
      </li>
      <li>自然饱和度/饱和度: 改变图片的Saturation
        <ol>
          <li>自然饱和度只增加图片饱和度低的地方的饱和度</li>
          <li>饱和度增加图片整体饱和度</li>
        </ol>
      </li>
      <li>亮度(Brightness): 所有像素变亮</li>
      <li>对比度(Contrast): 增加和0.5距离的绝对值</li>
      <li>高光/阴影/白色/黑色: 调整清晰度
        <ol>
          <li>高光/阴影: 只会影响图片的 亮部/暗部</li>
          <li>白色/黑色: 影响图片整体</li>
        </ol>
      </li>
    </ol>
  </li>
  <li>混色器: <a href="https://www.bilibili.com/video/BV1584y1C7nP/">教程</a>
    <ul>
      <li>HSL(Hue, Saturation, Lightness) 调整:
        <ol>
          <li>对图片上颜色归类(按红,橙,黄,绿,青,蓝,紫,洋红分为8类)</li>
          <li>调整<strong>所选颜色附近45度的颜色</strong>mask, 颜色拉到最右侧是变成下一级
            <ul>
              <li>比如红色hue拉到100, 会把红色调成橙色</li>
            </ul>
          </li>
        </ol>
      </li>
    </ul>
  </li>
  <li>颜色分级: <a href="https://www.bilibili.com/video/BV1zQ4y1R7J2/">教程</a>
    <ol>
      <li>把图片分为高光,中间调, 阴影</li>
      <li><strong>在对应亮度的像素位置增加颜色</strong>
        <ol>
          <li>调整颜色面板和下面的(色相,饱和度,明亮度)是一样效果: 都是选颜色</li>
        </ol>
      </li>
      <li>平衡: 控制偏高光还是偏阴影</li>
    </ol>
  </li>
  <li>校准: <a href="https://www.bilibili.com/video/BV1JK411d7ed/">教程</a>
    <ol>
      <li>调整图片上所有颜色, 把所有颜色顺时针旋转, <strong>原色附近权重更大</strong></li>
    </ol>
  </li>
</ol>

<ul>
  <li>颜色调整的精细度: 混色器 &gt; 校准的原色 &gt; 基本的色温/色调</li>
  <li>测试:
    <ul>
      <li>用色环测试颜色调整的效果</li>
      <li>用灰度卡测试亮度变化效果</li>
    </ul>
  </li>
  <li>photoshop找不到功能的具体公式, 只能靠一些网络教程, 有些功能实在没法一下明白…</li>
  <li>一般需要想好要什么颜色, 然后看着调…</li>
</ul>

<h2 id="reference">reference</h2>

<p>[1]  https://www.zhihu.com/question/54879117#:~:text=高光和白色滑块,的保护更好一些%E3%80%82    高光/阴影/白色/黑色讨论, 看完还是没懂</p>

<p>[2] Paris, Sylvain, Samuel W Hasinoff, and Jan Kautz. “Local Laplacian Filters: Edge-Aware Image Processing with a Laplacian Pyramid,” n.d.      据说可能是高光/阴影的公式</p>

<p>[3] https://lab.magiconch.com/eva-title/?layout=e10 EVA字体生成器</p>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;&quot;/docs/images/logo.svg&quot;, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="photo" /><category term="content" /><category term="photoshop" /><category term="shortcut" /><category term="filter" /><category term="color" /><category term="camera_raw" /><summary type="html"><![CDATA[人物美白]]></summary></entry><entry><title type="html">小面积光流传感器算法测试 (二) – 特征训练</title><link href="https://roshameow.github.io//personal_homepage/docs/algorithm/optical_flow_train2/" rel="alternate" type="text/html" title="小面积光流传感器算法测试 (二) – 特征训练" /><published>2024-04-25T00:00:00+00:00</published><updated>2024-04-28T23:30:43+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/algorithm/optical_flow_train2</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/algorithm/optical_flow_train2/"><![CDATA[<h2 id="数据">数据</h2>

<p>① ② :  <img src="/personal_homepage/docs/attachment/Pasted%20image%2020240426065333.png" alt="Pasted image 20240426065333.png" width="150" />    ③ : <img src="/personal_homepage/docs/attachment/Pasted%20image%2020240426065926.png" alt="Pasted image 20240426065926.png" width="330" /></p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th>采样方式</th>
      <th>具体说明</th>
      <th>特点</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>①</td>
      <td>仿真图像+仿真采样<br />Sample</td>
      <td>在16x16的图像上随机crop得到8x8的patch, <br />再随机用grid_sample提取8x8的patch比对<br /><br />正样本: 和patch距离&lt;0.5的patch<br /></td>
      <td>从采样方法来说, 当前像素只和周围3x3邻域像素相关<br /><br /></td>
    </tr>
    <tr>
      <td>②</td>
      <td>真实图像+仿真采样<br />SampleFromFrame</td>
      <td>用实际sensor提供的图片</td>
      <td> </td>
    </tr>
    <tr>
      <td>③</td>
      <td>真实图像+真实采样<br />SampleFromVideo</td>
      <td>筛选实际sensor提供的图片前后帧,<br />用其他算法确定光流已知的图片对,<br />在图片的其他区域采样</td>
      <td>这是图像配准特征训练中的一般做法</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>代码: <a href="https://gist.github.com/roshameow/a56eaeff6cc8c84aacfce28ba17be0bf#file-local_binary-py"><strong>local_binary.py</strong></a></li>
  <li>结果: 对于究竟学到了哪方面特征, 我很疑惑
    <ul>
      <li>出乎我意料的, 是①  &gt; ② &gt; ③
        <ul>
          <li>可能是我加噪声的方式和真实情况有差距?</li>
          <li>可能是我数据采样中的光流不可靠?</li>
          <li>可能是产生了我不清楚的过拟合?</li>
        </ul>
      </li>
      <li>adaboost的方法比神经网络训练效果好(或者差不多?)</li>
      <li>“最好”的训练结果也没比不训练的结果(sad-mean(diff)的版本)好.
        <ul>
          <li>可能通过匹配patch计算光流的准确度本来已经达到饱和, 再训练patch的描述也没法提升?</li>
        </ul>
      </li>
      <li>用真实数据的loss比仿真数据要大
        <ul>
          <li>说明真实数据更难</li>
          <li>用真实图像插值时, 结果变得超差, 改成crop好了一些</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="torch-grid-sample">torch grid sample</h3>

<p>torch grid 的采样方式有align_corners=True和align_cornes=False两种</p>
<ul>
  <li><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240415160510.png" alt="Pasted image 20240415160510.png" width="250" /></li>
  <li>转换关系: 一般需要对齐的时候选align_corner=True
    <ul>
      <li>pixel -&gt; grid(align_corner=True): <code class="language-plaintext highlighter-rouge">x=x/(n-1)*2-1</code></li>
      <li>x和y的dim可能和crop也不一致, 需要注意</li>
      <li>可以先按转换关系使grid_sample和crop完全对齐, 测试sample代码位置上是否正确</li>
    </ul>
  </li>
</ul>

<h2 id="gradient-descent训练">gradient descent训练</h2>

<p>用对比学习的模式, 输入相同size的dist(N x M: batch_size x compared_sample_num)和 label.</p>
<ul>
  <li>label = 1, 对应match pair, 我们想要使其dist更小(相关性的话更大)</li>
  <li>对batch的dim做平均要在最后, 因为不同sample应该分别排序</li>
</ul>

<h3 id="几种contrastive-loss">几种contrastive loss</h3>

<table>
  <thead>
    <tr>
      <th>loss</th>
      <th>公式</th>
      <th>目的</th>
      <th>特点</th>
      <th> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><a href="https://github.com/naver/r2d2/blob/master/nets/ap_loss.py">AP Loss</a> <br />(Average Precision)<br /><br />按照R2D2<a href="#ref">2</a>的写法</td>
      <td>1. 把dist量化, 变为N x Q x M 的binary tensor<br />2. 和label比对, 统计每个sample对应的match/miss的hist: N x Q<br />3. 对于hist的每个bin, 计算前面k个bin的precision<br /><code class="language-plaintext highlighter-rouge">prec@k=cumsum_k match/(match+miss)</code> <br />4. 对每个bin位置的prec@k 做关于match hist的加权平均, 得到AP<br />5. AP loss = 1-mean(AP)<br /></td>
      <td>减小miss hist排在match hist前面的情况<br />用prec@K 表示混合程度<br /><br />名称里的average, 是对match sample的average<br /><br />说是<a href="https://en.wikipedia.org/wiki/Precision_and_recall#:~:text=Precision%20can%20be%20seen%20as,irrelevant%20ones%20are%20also%20returned">precision</a>, <br />代码里实际写的是accuracy<br /></td>
      <td>要根据dist范围调整量化的min/max<br /><br />量化的过程用到了clamp截断<br /><br />因为所有的正样本都参与了训练<br />这个loss会比较稳定<br /><br />在训练过程中, hist本身就是一个很直观观测分布的指标</td>
      <td> </td>
    </tr>
    <tr>
      <td>InfoNCE<br />(Noise-Contrastive Estimation)</td>
      <td>1. 对dist做softmax<br />2. 计算dist和label的<a href="https://en.wikipedia.org/wiki/Cross-entropy">cross entropy loss</a></td>
      <td>让match dist=0, miss dist = 1</td>
      <td>要保证batch的每个sample只对应一个正样本<br />(因为softmax)</td>
      <td> </td>
    </tr>
    <tr>
      <td>triple loss</td>
      <td>1. 每个batch sample, 分别选取match sample里dist最大(ap), <br />和miss sample里dist最小的(an)<br />2. 计算这两者的ranking loss<br /><br />即 $dist_{ap}-dist_{an}-m$ <br />(m是boundary, 即允许最大的match dist比最小的miss dist稍大一点)</td>
      <td>让match dist尽量小, miss dist尽量大</td>
      <td>只有最极端的正负样本参与了训练</td>
      <td> </td>
    </tr>
    <tr>
      <td>Circle Loss</td>
      <td>1. 对match sample(sp), 和miss sample(sn)做logsumexp<br />2. 对这两个加和做soft_plus<br /><br />参数$\gamma$ 控制logsumexp的光滑度, 参数m还是控制match/miss的dist界限值</td>
      <td>和triple loss差不多, 但是是连续形式</td>
      <td>其他正负样本也参与训练, 越是bad case权重越高</td>
      <td> </td>
    </tr>
  </tbody>
</table>

<ul>
  <li>代码: <a href="https://gist.github.com/roshameow/4ef59b0173b9489c2caa9c0e4712137b#file-loss-py"><strong>loss.py</strong></a>
    <ul>
      <li>当时写的时候我用的是相关性而不是dist(但是变量名写的是dist…), 可能是改r2d2代码的时候糊涂了 💔(代码里在量化的时候把相关性转成了dist)</li>
      <li>AP_loss的中间结果(dist的平均分布): <img src="/personal_homepage/docs/attachment/hist.png" alt="hist.png" width="100" /> 其中蓝色是match, 红色是miss</li>
    </ul>
  </li>
  <li><strong>光流任务</strong> :
    <ul>
      <li>尝试了两种descriptor:
        <ul>
          <li>只用AP loss和64dim的 linear特征loss比较正常</li>
          <li>如果用grid-sample 选一些pixel pair的差值(类似<a href="https://docs.opencv.org/3.4/dc/d7d/tutorial_py_brief.html">BRIEF</a> ), loss完全没有下降</li>
        </ul>
      </li>
      <li>dist计算中, 如果是用内积计算两组sample(N x V, M x V)的相关性(N x M), 只需要用矩阵乘法就可以. 但是如果用其他自定义的dist, 需要先extend到(N x M x V, N x M x V), 去计算距离.</li>
    </ul>
  </li>
</ul>

<h2 id="adaboost训练"><a href="https://en.wikipedia.org/wiki/AdaBoost#:~:text=AdaBoost%20refers%20to%20a%20particular,the%20class%20of%20the%20object.">Adaboost</a>训练</h2>

<p>如果特征是离散的, gradient没有指导意义的时候, 可以用sample+选择的方式训练</p>

<ul>
  <li>特征采用<a href="https://docs.opencv.org/3.4/dc/d7d/tutorial_py_brief.html">BRIEF</a> binary特征, 每个pixel pair做为一个weak-learner
    <ul>
      <li>$h = sign(p_1-p_2)$</li>
    </ul>
  </li>
  <li>步骤是重复: 选特征-&gt; 更新sample权重 -&gt; 计算weighted error 的过程</li>
  <li>代码: <a href="https://gist.github.com/roshameow/4d0792f08724f0bda880b564db04530f#file-boosting_train-py"><strong>boosting_train.py</strong></a></li>
</ul>

<h3 id="adaboost的一个改进">adaboost的一个改进</h3>

<p>在beblid<a href="#ref">2</a>中介绍了<strong>给每个weak-learner加一个boundary的方法</strong>: 当weak-learner结果在boundary的同侧视为匹配(比如 $h_1&lt;T, h_2&lt;T$ ), 异侧视为不匹配</p>
<ul>
  <li><strong>步骤</strong>: 对于每个weak-learner
    <ul>
      <li>给当前weak-learner的结果<strong>排序</strong></li>
      <li>先确定当前weak-learner所对应的可能的boundaries ${T_j}$
        <ul>
          <li>$T_{j-1}$ 到 $T_j$ error的改变有以下几种可能: 假设$(v_1,v_2)$ 是当前weak-learner sample pair的结果
            <ul>
              <li><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240426173943.png" alt="Pasted image 20240426173943.png" width="150" /></li>
              <li>$v_1&lt; T_{j-1}&lt;v_2&lt; T_{j}$ , $v_1, v_2$ 从$T_{j-1}$ 的异侧变为了$T_j$ 的同侧 (1)
                <ul>
                  <li>如果$(v_1, v_2)$ label为1, match, error -1</li>
                  <li>如果$(v_1, v_2)$ label为-1, miss, error +1</li>
                </ul>
              </li>
              <li>$T_{j-1}&lt;v_1&lt; T_{j}&lt;v_2$ , $v_1, v_2$ 从$T_{j-1}$ 的同侧变为了$T_j$ 的异侧 (2)
                <ul>
                  <li>如果$(v_1, v_2)$ label为1, match, error +1</li>
                  <li>如果$(v_1, v_2)$ label为-1, miss, error -1</li>
                </ul>
              </li>
              <li>$v_2&lt; T_{j-1}&lt;v_1&lt; T_{j}$ , $v_1, v_2$ 从$T_{j-1}$ 的异侧变为了$T_j$ 的同侧 (3)
                <ul>
                  <li>如果$(v_1, v_2)$ label为1, match, error -1</li>
                  <li>如果$(v_1, v_2)$ label为-1, miss, error +1</li>
                </ul>
              </li>
              <li>$T_{j-1}&lt;v_2&lt; T_{j}&lt;v_1$ , $v_1, v_2$ 从$T_{j-1}$ 的同侧变为了$T_j$ 的异侧 (4)
                <ul>
                  <li>如果$(v_1, v_2)$ label为1, match, error +1</li>
                  <li>如果$(v_1, v_2)$ label为-1, miss, error -1</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>用 $d_1$ 收集case 2,3, $d_2$ 收集case 1,4.
            <ul>
              <li>当$v_1$ 经过$T_{j-1}\rightarrow T_j$ 就会触发$d_1$,  当$v_2$ 经过$T_{j-1}\rightarrow T_j$ 就会触发$d_2$</li>
            </ul>
          </li>
        </ul>
      </li>
      <li><strong>遍历</strong>boundrary $T_j$ 和其中的sample即可计算出全部带boundary的weak-learner的error</li>
    </ul>
  </li>
  <li><strong>计算量</strong>:
    <ul>
      <li>如果是plain的adaboost, 每个weak-learner的迭代只需要更新weight, 计算量只有O(N)</li>
      <li>如果weak-learner总数很多, 每次迭代只sample一部分weak-learner, 就没法复用weak-learner的结果, 要重新计算O(N x W)
        <ul>
          <li>W 是本轮重新sample出的weak-learner</li>
        </ul>
      </li>
      <li>如果给weak-learner附上boundary, 还要
        <ul>
          <li>对每个weak-learner的结果排序O(PlogP), P=2N 是sample pair里结果的个数, 即O(W x PlogP)</li>
          <li>对每个weak-learner遍历sample 的结果 O(N), 即O(W x N)
            <ul>
              <li>如果不是采用这种遍历sample的方式, 虽然不需要排序了, 但是需要O(T x N)的error计算, 即总共O(W x T x N), 考虑到sample数N是比较大的, 这没法接受</li>
              <li>考虑在boundary同侧的结果本来就没影响, 所以拆成排序+遍历应该是boundary训练的一个常见的方法🤔️</li>
            </ul>
          </li>
          <li>因为每个weak-learner的结果和boundary的选择不一样, 似乎没法在weak-learner那边并行❓</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p>opencv的人脸检测分类器也是这种加boundary(threshold)的形式</p>

<h2 id="reference">reference</h2>
<p><span id="ref"></span>
[1]  He, Kun, Yan Lu, and Stan Sclaroff. “Local Descriptors Optimized for <strong>Average Precision</strong>.” arXiv, April 17, 2018. <a href="http://arxiv.org/abs/1804.05312">http://arxiv.org/abs/1804.05312</a>.</p>

<p>[2] Revaud, Jerome, Philippe Weinzaepfel, César De Souza, Noe Pion, Gabriela Csurka, Yohann Cabon, and Martin Humenberger. “<strong>R2D2</strong>: Repeatable and Reliable Detector and Descriptor.” arXiv, June 17, 2019. <a href="https://doi.org/10.48550/arXiv.1906.06195">https://doi.org/10.48550/arXiv.1906.06195</a>.</p>

<p>[3] Suárez, Iago, Ghesn Sfeir, José M. Buenaposada, and Luis Baumela. “<strong>BEBLID</strong>: Boosted Efficient Binary Local Image Descriptor.” <em>Pattern Recognition Letters</em> 133 (May 2020): 366–72. <a href="https://doi.org/10.1016/j.patrec.2020.04.005">https://doi.org/10.1016/j.patrec.2020.04.005</a>.</p>
<h2 id="其他资源">其他资源</h2>

<p>[1] https://www.htmlsymbols.xyz/number-symbols/circled-numbers html特殊list符号</p>

<p>[2] https://www.zhihu.com/question/382802283 很好的对于Circle loss的解释</p>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;&quot;/docs/images/logo.svg&quot;, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="algorithm" /><category term="content" /><category term="optical_flow" /><category term="deeplearning" /><category term="adaboost" /><category term="contrastive_learning" /><category term="grid_sample" /><summary type="html"><![CDATA[数据]]></summary></entry><entry><title type="html">stable-diffusion中k-sampling的不同版本</title><link href="https://roshameow.github.io//personal_homepage/docs/algorithm/stable-diffusion7/" rel="alternate" type="text/html" title="stable-diffusion中k-sampling的不同版本" /><published>2024-04-16T00:00:00+00:00</published><updated>2024-05-20T22:28:12+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/algorithm/stable-diffusion7</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/algorithm/stable-diffusion7/"><![CDATA[<h2 id="diffusion-process--reverse-diffusion-process">Diffusion Process &amp; Reverse Diffusion Process</h2>

<ul>
  <li>用Markov chain的表示:
    <ul>
      <li>
        <table>
          <tbody>
            <tr>
              <td>$q(x_t</td>
              <td>x_{t-1})=N(x_t;\sqrt{1-\beta_t}x_{t-1},\beta_tI)$ ,  $q(x_t</td>
              <td>x_0)=N(x_t;\sqrt{\bar\alpha_t}x_0,(1-\bar\alpha_t)I)$</td>
            </tr>
          </tbody>
        </table>
      </li>
    </ul>
  </li>
</ul>

<p>diffusion收敛到SDE, SDE离散化得到diffusion
把DPM表示成
<a href="https://en.wikipedia.org/wiki/Stochastic_differential_equation">SDE(stochastic differential equation)</a>:</p>
<ul>
  <li>SDE的一般形式: $dx=f(x,t)dt+g(t)dw$
    <ul>
      <li>$f(\cdot,t)$ 是drift coefficients 表示确定的部分</li>
      <li>$g(\cdot)$ 是diffusion coefficients 表示随机部分</li>
      <li>$\omega$ 是Wiener process(布朗运动)</li>
    </ul>
  </li>
  <li>SDE的reverse: $dx=(f(x,t)-g^2(t)\nabla_x\log p(x,t))dt+g(t)dw$
    <ul>
      <li>score function $\nabla_x\log p(x,t)$   指向higher density of data</li>
      <li>$\nabla_x\log p(x;\sigma)=(D(x;\sigma)-x)/{\sigma^2}$</li>
      <li>前半确定部分: Probability flow ODE: $dx=(f(x,t)-g^2(t)\nabla_x\log p(x,t))dt$</li>
    </ul>
  </li>
  <li>Variance Preserving (VP) SDE, DDPM: $dx=-\frac{1}{2}\beta_t x dt +\sqrt{\beta_t} dw$
    <ul>
      <li>$f(x,t)=f(t)x=\frac{1}{\bar \alpha_t}\frac{d\bar \alpha_t}{dt}$</li>
    </ul>
  </li>
  <li></li>
</ul>

<p>diffusion ODE: $\frac{dx_t}{dt}=f(t)x_t+\frac{g^2(t)}{2\sigma_t}\epsilon_\theta(x_t,t), x_t\sim N(0,)$</p>

<p>diffusion ODE的1,2,3阶solver</p>

<p>Ancestral: 带 a的ksampler, 添加noise</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th>步骤</th>
      <th> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Euler</td>
      <td>noise injection:<br />- increased noise $\hat \sigma$  : $\hat \sigma\leftarrow \sigma_i + \gamma\sigma_i$ <br />-  sample x with increased noise: $\hat x \leftarrow x_i + \sqrt{\hat \sigma^2-\sigma_i^2}\cdot\epsilon$ <br />Take Euler Step: <br />- $dt=\sigma_{i+1}-\hat \sigma$<br />- $denoised=model(\hat x,\hat \sigma)$ <br />- numerical derivative: $d=(\hat x-denoised)/{\hat \sigma}$ <br />- Euler step: $x_{i+1}=\hat x+dt \cdot d$</td>
      <td> </td>
    </tr>
    <tr>
      <td>Euler Ancestral</td>
      <td>Take Euler Step to $\sigma_{down}$:  <br />- $dt=\sigma_{down}-\sigma_i$ <br />- $denoised=model(x,\sigma_i)$ <br />- numerical derivative: $d=(x-denoised)/{\sigma_i}$ <br />- Euler step: $x_{down}=x+dt \cdot d$ <br />Add ancestral noise:<br />- $x_{i+1}=x_{down}+noise*\sigma_{up}$ <br /></td>
      <td><br />$\sigma_{up}=\min(\sigma_{i+1},\eta\cdot(\frac{\sigma_{i+1}^2}{\sigma_i^2}(\sigma_i^2-\sigma_{i+1}^2)))$<br />$\sigma_{down}=\sqrt{\sigma_{i+1}^2-\sigma_{up}^2}$</td>
    </tr>
    <tr>
      <td>DDPM</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>DDIM</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>DPM-Solver</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>dpm_2</td>
      <td>noise injection: <br />得到$\hat\sigma, \hat x, denoised$ <br />DPM-Solver-2: <br />- 在$\sigma_{i+1},\hat \sigma$ 之间取: $\sigma_{mid}=e^{\frac{\log \hat\sigma+\log \sigma_{i+1}}{2}}$ <br />- $dt_1=\sigma_{mid}-\hat \sigma$ <br />- $dt_2=\sigma_{i+1}-\hat \sigma$ <br />- $x_{mid}=\hat x+dt_1\cdot(\hat x-model(\hat x,\hat \sigma))/\hat \sigma$  <br />用$\sigma_{mid}$ 处的numerial derivative<br />- $x=\hat x+dt_2\cdot(x_{mid}-model(x_{mid},\sigma_{mid}))/\sigma_{mid}$ <br /><br /></td>
      <td> </td>
    </tr>
    <tr>
      <td>dpmpp_2m</td>
      <td>- $t=-\log(\sigma_i)$ , $t_{next}=-\log(\sigma_{i+1})$ , $t_{last}=-\log(\sigma_{i-1})$ <br />- $h=t_{next}-t$, $h_{last}=t-t_{last}$ <br />- $r=h_{last}/h$ <br />- $denoised_d=(1+\frac{1}{2r})\cdot denoised-\frac{1}{2r}\cdot denoised_{old}$  <br />- $x_{i+1}=\frac{\sigma_{i+1}}{\sigma_i}\cdot x_i-(e^{-h}-1)\cdot denoised_d$ <br /></td>
      <td> </td>
    </tr>
    <tr>
      <td>lcm</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td><a href="https://en.wikipedia.org/wiki/Heun%27s_method">heun</a></td>
      <td>noise injection:  <br />得到$\hat\sigma, \hat x, denoised$ <br />Take Euler step:<br />- $x_2=\hat x + dt\cdot d$<br />Take Heun step:<br />- $d_2=(x_2-model(x_2,\sigma_{i+1}))/\sigma_{i+1}$ <br />- $d^\prime=\frac{d+d_2}{2}$ <br />- $x_{i+1}=\hat x+d^\prime\cdot dt$</td>
      <td> </td>
    </tr>
  </tbody>
</table>

<p>Euler:</p>

<h2 id="reference">reference</h2>

<p>[1 ] Karras, Tero, Miika Aittala, Timo Aila, and Samuli Laine. “Elucidating the Design Space of Diffusion-Based Generative Models.” arXiv, October 11, 2022. <a href="https://doi.org/10.48550/arXiv.2206.00364">https://doi.org/10.48550/arXiv.2206.00364</a>.</p>

<p>[2] Lu, Cheng, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. “DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps.” arXiv, October 13, 2022. <a href="https://doi.org/10.48550/arXiv.2206.00927">https://doi.org/10.48550/arXiv.2206.00927</a>.</p>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;&quot;/docs/images/logo.svg&quot;, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="algorithm" /><category term="content" /><summary type="html"><![CDATA[Diffusion Process &amp; Reverse Diffusion Process]]></summary></entry><entry><title type="html">小面积光流传感器算法测试 (一)</title><link href="https://roshameow.github.io//personal_homepage/docs/algorithm/optical-flow-train/" rel="alternate" type="text/html" title="小面积光流传感器算法测试 (一)" /><published>2024-04-11T00:00:00+00:00</published><updated>2024-05-09T06:26:59+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/algorithm/optical-flow-train</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/algorithm/optical-flow-train/"><![CDATA[<p>大概分为: preprocess -&gt; instant flow compute -&gt; filter correct 三个步骤</p>

<h2 id="计算连续两帧的光流">计算连续两帧的光流</h2>

<table>
  <thead>
    <tr>
      <th>算法</th>
      <th>改进</th>
      <th>公式</th>
      <th>效果</th>
      <th>存储占用</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>LK<br /><a href="https://en.wikipedia.org/wiki/Lucas–Kanade_method">Lucas-Kanade</a></td>
      <td> </td>
      <td>对图像$I$ 的每个像素, 有 $\frac{\partial I}{\partial x}dx+\frac{\partial I}{\partial y}dy=\frac{dI}{dt}$ <br />即, $\begin{bmatrix}dx \\ dy\end{bmatrix}=\begin{bmatrix}\frac{\partial I}{\partial x}\frac{\partial I}{\partial x} &amp; \frac{\partial I}{\partial x}\frac{\partial I}{\partial y} \\ \frac{\partial I}{\partial x}\frac{\partial I}{\partial y} &amp; \frac{\partial I}{\partial y}\frac{\partial I}{\partial y} \end{bmatrix}^{-1}\begin{bmatrix}\frac{\partial I}{\partial x}\frac{dI}{dt} \\ \frac{\partial I}{\partial y}\frac{dI}{dt}\end{bmatrix}=H^{-1}\begin{bmatrix}\frac{\partial I}{\partial x}\frac{dI}{dt} \\ \frac{\partial I}{\partial y}\frac{dI}{dt}\end{bmatrix}$<br />其中$\frac{\partial I}{\partial x}\approx I(x+1,y)-I(x,y)$  <br /></td>
      <td>只在光流在0-1附近有效(即subpixel的尺度)<br /><br />和$\frac{\partial I}{\partial x}$ 的计算方式有关</td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td>LK_MEAN_NORM<br /></td>
      <td>把$\frac{dI}{dt}$ 改为 $\frac{dI}{dt}-mean(\frac{dI}{dt})$<br />($mean(\frac{dI}{dt})$ 表示整体亮度的变化, 和光流无关)</td>
      <td>解决亮度变化的情况</td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td>@pre_shift</td>
      <td>先对齐到上一次计算的光流位置</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td>@compute_pyd</td>
      <td>把图像分为多层下采样计算</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td>LK-DIS<br />dense inverse search<br />结合两种方法</td>
      <td>分块计算<br />- 每块都迭代计算光流<br />  - 先按整像素移动到ssd最小的位置<br />   - 再用LK_MEAN_NORM的方法不断微调计算光流<br />       - 按照光流计算的方向对齐, 计算ssd<br />        - ssd不再变小就跳出循环<br />用所有分块的平均光流作为最终结果</td>
      <td>比LK更稳定</td>
      <td>需要所有patch的Hessian, dx, dy矩阵<br /><br />对齐patch时移动patch的中间结果<br /></td>
    </tr>
    <tr>
      <td>像素neighbor patch比对<br /></td>
      <td> </td>
      <td>$diff=dist(F(I_1(x,y))-F(I_2(x+dx,y+dy)))$ <br /><br />$argmin_{(dx,dy)}\sum_{x,y}dist(F(I_1(x,y))-F(I_2(x+dx,y+dy)))$ <br />其中, $F$ 是特征提取器, dist是距离函数</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>: 特征</td>
      <td>SAD_SIMPLE<br />sum of average differences</td>
      <td>$sad=L_1(diff)$<br />或者$sad=L_1(diff-mean(diff))$</td>
      <td>第二种更好</td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td>SAD_SUBPIXEL<br />用多项式拟合sad平面<br />(opencv Farneback<br />也是用多项式拟合, <br />不过拟合的是原图像)</td>
      <td>$P(x+\Delta x)\approx P(x)+P^\prime(x)\Delta x+\frac{P^{\prime\prime}(x)}{2}\Delta x^2$ <br />即, $\Delta x=-\frac{P^\prime(x)}{P^{\prime\prime}(x)}$ <br />其中 $P^\prime(x)\approx \frac{dist(x+1,y)-dist(x-1,y)}{2}$ , $P^{\prime\prime}$ 类似<br />-  dx, dy分开计算<br />- 直接用多项式代入也是等价的</td>
      <td>没用<br />可能是因为subpixel的部分不符合多项式</td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td>SAD_BLUR</td>
      <td>先对图像做2x2的blur<br />😮‍💨因为觉得不准的地方是不是因为刚好对齐的地方在像素的中间</td>
      <td>垃圾<br /></td>
      <td> </td>
    </tr>
    <tr>
      <td>: 距离<br /></td>
      <td>SSD_SIMPLE<br />sum of squared differences<br /></td>
      <td>$ssd=Var(diff)$</td>
      <td>比SAD稳定</td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td>SAD_BINARY</td>
      <td>$hamming=popcount(ref\ \hat\ current)$<br />其中图像是binary(image-mean(image))</td>
      <td>效果明显变差<br />binary有没有必要呢?</td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td>BINARY_FEATURE</td>
      <td>用gradient descent训练一个特征</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td>BINARY_FEATURE_BOOST</td>
      <td>用adaboost训练一个特征</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>: neighbor</td>
      <td>SAD_SIMPLE_CROSS<br />neighbor变成cross形状, 节省一些存储</td>
      <td> </td>
      <td>节省存储&amp;计算<br /></td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td>SAD_SPIRAL</td>
      <td>- 从上一次的光流位置向外螺旋状计算<br />- 遇到更小的distance提前结束循环</td>
      <td>节省存储&amp;计算<br />和LK效果类似<br />应该是都用了preshift的原因</td>
      <td> </td>
    </tr>
    <tr>
      <td><a href="https://en.wikipedia.org/wiki/Phase_correlation">phase_correlation</a></td>
      <td> </td>
      <td>$dx,dy=argmax_{x,y} F^{-1}(\frac{F(I_1)\cdot \bar{F(I_2)}}{|F(I_1)\cdot \bar{F(I_2)}|})$ <br />相当于提取图像的phase部分<br />然后用cross correlation的dist</td>
      <td>在位移太大时效果不好<br />光流在0-6时效果可以 <br />另外如果不做phase correlation,<br />直接做cross correlation<br />效果并不好</td>
      <td>需要ref和current的fft频域</td>
    </tr>
  </tbody>
</table>

<h2 id="测试">测试</h2>

<ul>
  <li>仿真数据: 距离仿真数据的位置</li>
  <li>真实数据:
    <ul>
      <li>稳定性: 电机带动匀速转动</li>
      <li>响应速度: 电机急停急转</li>
    </ul>
  </li>
</ul>

<p>一些结果$\downarrow$</p>

<p><img src="/personal_homepage/docs/attachment/result_compare_2000_test.png" alt="result_compare_2000_test.png" width="400" />  <img src="/personal_homepage/docs/attachment/Pasted%20image%2020240425142720.png" alt="Pasted image 20240425142720.png" width="400" /></p>

<ul>
  <li>对于测试场景来说, 所有subpixel的方法似乎都没有必要</li>
  <li>phase correlation &gt; 训练得到的几种特征平移匹配 $\approx$ sad(-mean(diff)) $\approx$ ssd &gt; sad binary » sad spiral $\approx$ LK
    <ul>
      <li><strong>对图像做detail的提取(如image-blur(image,(6x6)))</strong> 之后, sad的结果得到提高, 和训练得到的特征类似</li>
      <li>几个特征平移方法出现错误的地方可能因为超出了搜索范围导致的</li>
      <li>平移后diff的例子: <img src="/personal_homepage/docs/attachment/Pasted%20image%2020240425151045.png" alt="Pasted image 20240425151045.png" width="100" /></li>
    </ul>
  </li>
  <li>响应的测试: 除了lk会反应慢一些, 其他都在可以接受范围内</li>
  <li>这真是个特别枯燥的工作😑, 很多方法觉得, 啊, 应该不会有效果的, 但还是想着, 坚持着写出来测一下究竟差在哪里吧</li>
</ul>

<h2 id="几种opencv支持的方法测试">几种opencv支持的方法测试</h2>

<ul>
  <li>代码: <a href="https://gist.github.com/roshameow/7843f23826791c152ab2ed8c169590b9#file-flow_opencv-py"><strong>flow_opencv.py</strong></a></li>
  <li>复现修改opencv的算法, 有时需要opencv代码的中间结果
    <ul>
      <li>因为我没有下载opencv完整的源码, 而是用pip装opencv的库</li>
      <li>用<code class="language-plaintext highlighter-rouge">pkg-config --cflags --libs opencv4</code> 查看opencv的lib,include path</li>
      <li>单独把要复现的函数复制一个.cpp, 就可以随便打印中间结果了</li>
    </ul>
  </li>
</ul>

<h2 id="有用的链接">有用的链接</h2>

<p>[1] https://dsp.stackexchange.com/questions/16995/image-reconstructionphase-vs-magnitude 关于图像phase部分的提问</p>

<p>[2] https://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/OWENS/LECT7/node2.html 解释图像边缘部分的<a href="https://en.wikipedia.org/wiki/Phase_congruency#:~:text=Phase%20congruency%20is%20a%20measure,changes%20in%20illumination%20and%20contrast.">Phase congruency</a> 更强</p>

<p><img src="https://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/OWENS/LECT7/img31.gif" alt="原图" /> <img src="https://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/OWENS/LECT7/img33.gif" alt="PC图" /> 用Phase Congruency提取图像边缘的结果</p>

<p>[3]  <a href="https://en.wikipedia.org/wiki/Phase_stretch_transform#:~:text=Phase%20stretch%20transform%20(PST)%20is,time%20stretch%20dispersive%20Fourier%20transform.">Phase stretch Transform</a></p>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;&quot;/docs/images/logo.svg&quot;, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="algorithm" /><category term="content" /><category term="optical_flow" /><category term="opencv" /><category term="test" /><summary type="html"><![CDATA[大概分为: preprocess -&gt; instant flow compute -&gt; filter correct 三个步骤]]></summary></entry></feed>