<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://roshameow.github.io//personal_homepage/atom.xml" rel="self" type="application/atom+xml" /><link href="https://roshameow.github.io//personal_homepage/" rel="alternate" type="text/html" /><updated>2024-05-04T23:19:04+00:00</updated><id>https://roshameow.github.io//personal_homepage/atom.xml</id><title type="html">Liu, Wen’s Home Page</title><subtitle>Work, Experiments and Ideas.</subtitle><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;title&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;icon&quot;=&gt;nil}]}</name><email>w.liuatnk@gmail.com</email></author><entry><title type="html">小面积光流传感器算法测试 (三) – 滤波</title><link href="https://roshameow.github.io//personal_homepage/docs/algorithm/optical_flow_train3/" rel="alternate" type="text/html" title="小面积光流传感器算法测试 (三) – 滤波" /><published>2024-04-28T00:00:00+00:00</published><updated>2024-05-05T15:17:18+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/algorithm/optical_flow_train3</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/algorithm/optical_flow_train3/"><![CDATA[<p>在高速场景下, 每次中断收集的数据是光流的累加值, 其实本来就相当于一个滤波…况且在硬件有限的条件下, 复杂的滤波没有什么实用价值.</p>

<h2 id="实验">实验</h2>

<table>
  <thead>
    <tr>
      <th>方法</th>
      <th> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ConfidenceFilter</td>
      <td>输出光流时同时输出一个置信度, 如果置信度较低, 选择历史值而不是测量值</td>
    </tr>
    <tr>
      <td>FIR</td>
      <td>在临近window上的一个线性filter</td>
    </tr>
    <tr>
      <td>Kalman</td>
      <td>在gain值稳定后, kalman滤波其实相当于一个<a href="https://en.wikipedia.org/wiki/Infinite_impulse_response">IIR filter</a></td>
    </tr>
  </tbody>
</table>

<ul>
  <li>对于整像素的光流结果, 相当于Kalman的MeasureNoise有一部分量化噪声, 应该加大MeasureNoiseCov参数的设置, 不过实验中看不出区别</li>
  <li>Kalman在反应速度和平滑度上都要好于FIR的</li>
</ul>

<h2 id="kalman滤波原理"><a href="https://en.wikipedia.org/wiki/Kalman_filter#:~:text=The%20Kalman%20filter%20produces%20an,uncertainty%20are%20%22trusted%22%20more.">Kalman滤波</a>原理</h2>

<h3 id="ssmstate-space-model">SSM(state space model)</h3>

<ul>
  <li><a href="https://en.wikipedia.org/wiki/State-space_representation">状态空间(state space)</a> : 用state vector记录历史的input, 而不是记录所有的历史token
    <ul>
      <li><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240317183044.png" alt="Pasted image 20240317183044.png" width="200" /></li>
    </ul>
  </li>
  <li>连续表示:
    <ul>
      <li>linear state space model的一般形式:
        <ul>
          <li>$\dot x(t)=A(t)x(t)+B(t)u(t)$ (用input u更新状态 x)</li>
          <li>$y(t)=C(t)x(t)+D(t)u(t)$  (用状态x, 生成output y)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="滤波步骤">滤波步骤</h3>

<table>
  <thead>
    <tr>
      <th>变量</th>
      <th>input</th>
      <th>predict</th>
      <th>correct</th>
      <th>output</th>
      <th>使用</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>state</td>
      <td> </td>
      <td>$\hat x_n$</td>
      <td>$x_n$</td>
      <td> </td>
      <td>[x,y,dx,dy]</td>
    </tr>
    <tr>
      <td>measure</td>
      <td>$u_n$</td>
      <td>$\hat y_n$</td>
      <td> </td>
      <td>$y_n$</td>
      <td>[x, y]</td>
    </tr>
  </tbody>
</table>

<table>
  <thead>
    <tr>
      <th>参数</th>
      <th> </th>
      <th>使用</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>transition matrix P</td>
      <td>state -&gt; predict state</td>
      <td>\begin{bmatrix}1&amp;0&amp;1&amp;0\\0&amp;1&amp;0&amp;1\\0&amp;0&amp;1&amp;0\\0&amp;0&amp;0&amp;1\end{bmatrix}</td>
    </tr>
    <tr>
      <td>measure matrix $C$</td>
      <td>state -&gt; measure</td>
      <td>\begin{bmatrix}1&amp;0&amp;0&amp;0\\0&amp;1&amp;0&amp;0\end{bmatrix}</td>
    </tr>
    <tr>
      <td>gain $g_n$</td>
      <td>measure error -&gt; state error</td>
      <td>根据仿真模型和运动模型更新</td>
    </tr>
  </tbody>
</table>

<p>参考<a href="https://github.com/opencv/opencv/blob/4.x/modules/video/src/kalman.cpp">opencv里面的写法</a></p>
<ul>
  <li><strong>predict</strong>:
    <ul>
      <li>$\hat x_n=P(x_{n-1})$ 是state 在当前时间的predict</li>
    </ul>
  </li>
  <li><strong>correct</strong>:
    <ul>
      <li>$x_{n}-\hat x_{n}=g_n\cdot(u_n-\hat y_{n})=g_n\cdot(u_n-C\hat x_n)$
        <ul>
          <li>state的error和measure的error(measure-predict) 是线性关系
            <ul>
              <li><strong>如果measure从predict偏移越少, state越保持predict的结果</strong></li>
            </ul>
          </li>
          <li>写成<strong>SSM</strong>的形式: $x_n=\hat x_{n}+g_n\cdot(u_n-C\hat x_n)=(I-g_n\cdot C)\hat x_n +g_n\cdot u_n=(I-g_n\cdot C)P x_{n-1} +g_n\cdot u_n$</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="通过运动模型更新gain---g_n">通过运动模型更新gain   $g_n$</h3>

<table>
  <thead>
    <tr>
      <th>变量</th>
      <th>仿真</th>
      <th>predict</th>
      <th>correct</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>state</td>
      <td>$x_n^r$</td>
      <td>$\hat x_n^r$</td>
      <td>$\tilde x_n$</td>
    </tr>
    <tr>
      <td>error</td>
      <td> </td>
      <td>$\hat \epsilon_n=x_n^r-\hat x_n^r$</td>
      <td>$\epsilon_n=x_n^r-\tilde x_n$</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>仿真模型:
    <ul>
      <li>$x_n^r$ 是<strong>仿真的state</strong></li>
      <li>$x_{n}^r=P(x_{n-1}^r)+ProcessNoise$(状态转移噪声)</li>
      <li>$u_n^r=C(x_n^r)+MeasureNoise$    测量值仿真结果
        <ul>
          <li>假设ProcessNoise和MeasureNoise都是高斯分布的<a href="https://en.wikipedia.org/wiki/Random_variable">Random Variable</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>计算模型:
    <ul>
      <li>predict:
        <ul>
          <li>$\hat x_n^r=P(\tilde x_{n-1})$  是predict值</li>
          <li>PredictError:
            <ul>
              <li>$\hat \epsilon_n=x_n^r-\hat x_n^r=P(x_{n-1}^r)+ProcessNoise-P(\tilde x_{n-1})=P\epsilon_{n-1}+ProcessNoise$ ①</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>correct:
        <ul>
          <li>$\tilde x_{n}=\hat x_{n}^r+g_n\cdot(u_n^r-C\hat x_n^r)=\hat x_{n}^r+g_n\cdot(C\cdot \hat\epsilon_n+MeasureNoise)$    是<strong>correct之后的state</strong></li>
          <li>CorrectError:
            <ul>
              <li>$\epsilon_n= x_n^r-\tilde x_n =\hat \epsilon_{n}-g_n\cdot( C\cdot \hat \epsilon_{n}+MeasureNoise)$ ②</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Gain: argmin(CorrectError) 
        <ul>
          <li>
\[\begin{align}g_n=\text{argmin} ||x_{n}^r-\tilde x_{n}||&amp;=\text{argmin} ||\hat \epsilon_n-g_n\cdot(C\cdot\hat \epsilon_n + MeasureNoise)||\\&amp;=(\hat \epsilon_n)(C\hat \epsilon_n+MeasureNoise)^T((C\hat \epsilon_n+MeasureNoise)(C\hat \epsilon_n+MeasureNoise)^T)^{-1} &amp; (\text{因为 }\text{argmin}||xA-b||=(bA^T)(AA^T)^{-1})\\&amp;=(\hat \epsilon_n^{cov}C^T)(C\hat \epsilon_n^{cov} C^T+MeasureNoise^{cov})^{-1}&amp;(\text{展开, }mean(MeasureNoise)=0)\end{align}\]
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>特点:
    <ul>
      <li>gain  $g_n$ 和实际采集的数据$u_n$ 无关, 是由仿真过程和计算过程确定的</li>
      <li>① , ② 为$\hat \epsilon_n$ 的更新过程</li>
      <li>在gain的计算中, 每次更新$\hat \epsilon_n^{cov}, \epsilon_n^{cov}$ 就可以了</li>
      <li>gain 在一段时间后会收敛到一个固定的值</li>
    </ul>
  </li>
</ul>

<h3 id="推导least-square">推导(Least Square)</h3>

<p>定义 L2 norm: 
	\(||A|| = ∑_{ij} A_{ij}^2  = trace(A^TA)\)</p>

<ul>
  <li>
    <p>Normal Least Square: 
  \(\min_x ||Ax-b|| = \min_x (Ax-b)^T(Ax-b)= \min_x (x^T(A^TA)x - 2x^T(A^Tb))\)</p>
  </li>
  <li>其中:
    <ul>
      <li>$\partial_x x^T(G)x = 2Gx   \ (\partial_{x_k} \sum G_{ij} x_i\cdot x_j = \sum G_{ik} x_i+\sum G_{kj} x_j\text{ related part are one for k-th row and one for k-th col)}$</li>
      <li>$\partial_x x^T(A^T b) = A^T b$</li>
    </ul>
  </li>
  <li>得到: $(A^TA)^{-1}(A^Tb)$</li>
</ul>

<p>变形:</p>

\[\text{argmin}_x ||xA-b||=\text{argmin}_{x^T} ||A^Tx^T-b^T||=((AA^T)^{-1}(Ab^T))^T=(bA^T)((AA^T)^{-1})^T=(bA^T)(AA^T)^{-1}\]

<h2 id="代码">代码</h2>

<p><a href="https://gist.github.com/roshameow/bee4e4ebe065cc3159094590bd873eb1#file-confidence_filter-py"><strong>confidence_filter.py</strong></a> <a href="https://gist.github.com/roshameow/bee4e4ebe065cc3159094590bd873eb1#file-fir-py"><strong>fir.py</strong></a> <a href="https://gist.github.com/roshameow/bee4e4ebe065cc3159094590bd873eb1#file-kalman-py"><strong>kalman.py</strong></a></p>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;title&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;icon&quot;=&gt;nil}]}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="algorithm" /><category term="content" /><category term="kalman" /><category term="optical_flow" /><category term="filter" /><category term="norm" /><summary type="html"><![CDATA[在高速场景下, 每次中断收集的数据是光流的累加值, 其实本来就相当于一个滤波…况且在硬件有限的条件下, 复杂的滤波没有什么实用价值.]]></summary></entry><entry><title type="html">小红书学到的几种图片调色 (二)</title><link href="https://roshameow.github.io//personal_homepage/docs/photo/photo-color1/" rel="alternate" type="text/html" title="小红书学到的几种图片调色 (二)" /><published>2024-04-27T00:00:00+00:00</published><updated>2024-04-30T05:12:03+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/photo/photo-color1</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/photo/photo-color1/"><![CDATA[<h2 id="人物美白">人物美白</h2>

<ul>
  <li>cameraRaw滤镜
    <ul>
      <li>基本:
        <ul>
          <li>
            <ul>
              <li>色温, + 色调</li>
            </ul>
          </li>
          <li>
            <ul>
              <li>曝光, + 对比度, -高光</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>混色器:
        <ul>
          <li>色相: + 红色, -橙色, –蓝色</li>
          <li>明亮度: -红色</li>
        </ul>
      </li>
      <li>校准:
        <ul>
          <li>绿原色: +色相</li>
          <li>蓝原色: +饱和度</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><a href="https://www.xiaohongshu.com/explore/661ab1fb000000000401bfe2">教程</a></p>

<h2 id="梦幻发光">梦幻发光</h2>

<p><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240428143139.png" alt="Pasted image 20240428143139.png" width="200" /> <img src="/personal_homepage/docs/attachment/Pasted%20image%2020240428143157.png" alt="Pasted image 20240428143157.png" width="200" /> <img src="/personal_homepage/docs/attachment/Pasted%20image%2020240428143227.png" alt="Pasted image 20240428143227.png" width="200" /></p>
<ul>
  <li>调整画面颜色: 可选颜色, 在 <a href="https://en.wikipedia.org/wiki/CMYK_color_model">CMYK 颜色</a> 调整
    <ul>
      <li>黄色: + 青色,黄色,黑色  -洋红</li>
      <li>绿色: + 青色,黄色  -洋红,黑色</li>
    </ul>
  </li>
  <li>给高光部分做高斯模糊</li>
  <li>混合模式改为变亮</li>
  <li>和blender里面的<a href="https://docs.blender.org/manual/en/latest/render/eevee/render_settings/bloom.html">bloom(辉光)</a> 功能原理一样
<a href="https://www.xiaohongshu.com/explore/660fbca9000000001a014c53">教程</a></li>
</ul>

<h2 id="photoshop快捷键">photoshop快捷键</h2>

<ul>
  <li>command+alt+shift+E(盖印)
    <ul>
      <li>把效果和图层合并生成一个新图层</li>
    </ul>
  </li>
  <li>command+alt+2(提取高光)
    <ul>
      <li>command+J 可以把高光变成一个新图层</li>
    </ul>
  </li>
</ul>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;title&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;icon&quot;=&gt;nil}]}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="photo" /><category term="content" /><category term="photoshop" /><category term="shortcut" /><summary type="html"><![CDATA[人物美白]]></summary></entry><entry><title type="html">小面积光流传感器算法测试 (二) – 特征训练</title><link href="https://roshameow.github.io//personal_homepage/docs/algorithm/optical_flow_train2/" rel="alternate" type="text/html" title="小面积光流传感器算法测试 (二) – 特征训练" /><published>2024-04-25T00:00:00+00:00</published><updated>2024-04-28T23:30:43+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/algorithm/optical_flow_train2</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/algorithm/optical_flow_train2/"><![CDATA[<h2 id="数据">数据</h2>

<p>① ② :  <img src="/personal_homepage/docs/attachment/Pasted%20image%2020240426065333.png" alt="Pasted image 20240426065333.png" width="150" />    ③ : <img src="/personal_homepage/docs/attachment/Pasted%20image%2020240426065926.png" alt="Pasted image 20240426065926.png" width="330" /></p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th>采样方式</th>
      <th>具体说明</th>
      <th>特点</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>①</td>
      <td>仿真图像+仿真采样<br />Sample</td>
      <td>在16x16的图像上随机crop得到8x8的patch, <br />再随机用grid_sample提取8x8的patch比对<br /><br />正样本: 和patch距离&lt;0.5的patch<br /></td>
      <td>从采样方法来说, 当前像素只和周围3x3邻域像素相关<br /><br /></td>
    </tr>
    <tr>
      <td>②</td>
      <td>真实图像+仿真采样<br />SampleFromFrame</td>
      <td>用实际sensor提供的图片</td>
      <td> </td>
    </tr>
    <tr>
      <td>③</td>
      <td>真实图像+真实采样<br />SampleFromVideo</td>
      <td>筛选实际sensor提供的图片前后帧,<br />用其他算法确定光流已知的图片对,<br />在图片的其他区域采样</td>
      <td>这是图像配准特征训练中的一般做法</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>代码: <a href="https://gist.github.com/roshameow/a56eaeff6cc8c84aacfce28ba17be0bf#file-local_binary-py"><strong>local_binary.py</strong></a></li>
  <li>结果: 对于究竟学到了哪方面特征, 我很疑惑
    <ul>
      <li>出乎我意料的, 是①  &gt; ② &gt; ③
        <ul>
          <li>可能是我加噪声的方式和真实情况有差距?</li>
          <li>可能是我数据采样中的光流不可靠?</li>
          <li>可能是产生了我不清楚的过拟合?</li>
        </ul>
      </li>
      <li>adaboost的方法比神经网络训练效果好(或者差不多?)</li>
      <li>“最好”的训练结果也没比不训练的结果(sad-mean(diff)的版本)好.
        <ul>
          <li>可能通过匹配patch计算光流的准确度本来已经达到饱和, 再训练patch的描述也没法提升?</li>
        </ul>
      </li>
      <li>用真实数据的loss比仿真数据要大
        <ul>
          <li>说明真实数据更难</li>
          <li>用真实图像插值时, 结果变得超差, 改成crop好了一些</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="torch-grid-sample">torch grid sample</h3>

<p>torch grid 的采样方式有align_corners=True和align_cornes=False两种</p>
<ul>
  <li><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240415160510.png" alt="Pasted image 20240415160510.png" width="250" /></li>
  <li>转换关系: 一般需要对齐的时候选align_corner=True
    <ul>
      <li>pixel -&gt; grid(align_corner=True): <code class="language-plaintext highlighter-rouge">x=x/(n-1)*2-1</code></li>
      <li>x和y的dim可能和crop也不一致, 需要注意</li>
      <li>可以先按转换关系使grid_sample和crop完全对齐, 测试sample代码位置上是否正确</li>
    </ul>
  </li>
</ul>

<h2 id="gradient-descent训练">gradient descent训练</h2>

<p>用对比学习的模式, 输入相同size的dist(N x M: batch_size x compared_sample_num)和 label.</p>
<ul>
  <li>label = 1, 对应match pair, 我们想要使其dist更小(相关性的话更大)</li>
  <li>对batch的dim做平均要在最后, 因为不同sample应该分别排序</li>
</ul>

<h3 id="几种contrastive-loss">几种contrastive loss</h3>

<table>
  <thead>
    <tr>
      <th>loss</th>
      <th>公式</th>
      <th>目的</th>
      <th>特点</th>
      <th> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><a href="https://github.com/naver/r2d2/blob/master/nets/ap_loss.py">AP Loss</a> <br />(Average Precision)<br /><br />按照R2D2<a href="#ref">2</a>的写法</td>
      <td>1. 把dist量化, 变为N x Q x M 的binary tensor<br />2. 和label比对, 统计每个sample对应的match/miss的hist: N x Q<br />3. 对于hist的每个bin, 计算前面k个bin的precision<br /><code class="language-plaintext highlighter-rouge">prec@k=cumsum_k match/(match+miss)</code> <br />4. 对每个bin位置的prec@k 做关于match hist的加权平均, 得到AP<br />5. AP loss = 1-mean(AP)<br /></td>
      <td>减小miss hist排在match hist前面的情况<br />用prec@K 表示混合程度<br /><br />名称里的average, 是对match sample的average<br /><br />说是<a href="https://en.wikipedia.org/wiki/Precision_and_recall#:~:text=Precision%20can%20be%20seen%20as,irrelevant%20ones%20are%20also%20returned">precision</a>, <br />代码里实际写的是accuracy<br /></td>
      <td>要根据dist范围调整量化的min/max<br /><br />量化的过程用到了clamp截断<br /><br />因为所有的正样本都参与了训练<br />这个loss会比较稳定<br /><br />在训练过程中, hist本身就是一个很直观观测分布的指标</td>
      <td> </td>
    </tr>
    <tr>
      <td>InfoNCE<br />(Noise-Contrastive Estimation)</td>
      <td>1. 对dist做softmax<br />2. 计算dist和label的<a href="https://en.wikipedia.org/wiki/Cross-entropy">cross entropy loss</a></td>
      <td>让match dist=0, miss dist = 1</td>
      <td>要保证batch的每个sample只对应一个正样本<br />(因为softmax)</td>
      <td> </td>
    </tr>
    <tr>
      <td>triple loss</td>
      <td>1. 每个batch sample, 分别选取match sample里dist最大(ap), <br />和miss sample里dist最小的(an)<br />2. 计算这两者的ranking loss<br /><br />即 $dist_{ap}-dist_{an}-m$ <br />(m是boundary, 即允许最大的match dist比最小的miss dist稍大一点)</td>
      <td>让match dist尽量小, miss dist尽量大</td>
      <td>只有最极端的正负样本参与了训练</td>
      <td> </td>
    </tr>
    <tr>
      <td>Circle Loss</td>
      <td>1. 对match sample(sp), 和miss sample(sn)做logsumexp<br />2. 对这两个加和做soft_plus<br /><br />参数$\gamma$ 控制logsumexp的光滑度, 参数m还是控制match/miss的dist界限值</td>
      <td>和triple loss差不多, 但是是连续形式</td>
      <td>其他正负样本也参与训练, 越是bad case权重越高</td>
      <td> </td>
    </tr>
  </tbody>
</table>

<ul>
  <li>代码: <a href="https://gist.github.com/roshameow/4ef59b0173b9489c2caa9c0e4712137b#file-loss-py"><strong>loss.py</strong></a>
    <ul>
      <li>当时写的时候我用的是相关性而不是dist(但是变量名写的是dist…), 可能是改r2d2代码的时候糊涂了 💔(代码里在量化的时候把相关性转成了dist)</li>
      <li>AP_loss的中间结果(dist的平均分布): <img src="/personal_homepage/docs/attachment/hist.png" alt="hist.png" width="100" /> 其中蓝色是match, 红色是miss</li>
    </ul>
  </li>
  <li><strong>光流任务</strong> :
    <ul>
      <li>尝试了两种descriptor:
        <ul>
          <li>只用AP loss和64dim的 linear特征loss比较正常</li>
          <li>如果用grid-sample 选一些pixel pair的差值(类似<a href="https://docs.opencv.org/3.4/dc/d7d/tutorial_py_brief.html">BRIEF</a> ), loss完全没有下降</li>
        </ul>
      </li>
      <li>dist计算中, 如果是用内积计算两组sample(N x V, M x V)的相关性(N x M), 只需要用矩阵乘法就可以. 但是如果用其他自定义的dist, 需要先extend到(N x M x V, N x M x V), 去计算距离.</li>
    </ul>
  </li>
</ul>

<h2 id="adaboost训练"><a href="https://en.wikipedia.org/wiki/AdaBoost#:~:text=AdaBoost%20refers%20to%20a%20particular,the%20class%20of%20the%20object.">Adaboost</a>训练</h2>

<p>如果特征是离散的, gradient没有指导意义的时候, 可以用sample+选择的方式训练</p>

<ul>
  <li>特征采用<a href="https://docs.opencv.org/3.4/dc/d7d/tutorial_py_brief.html">BRIEF</a> binary特征, 每个pixel pair做为一个weak-learner
    <ul>
      <li>$h = sign(p_1-p_2)$</li>
    </ul>
  </li>
  <li>步骤是重复: 选特征-&gt; 更新sample权重 -&gt; 计算weighted error 的过程</li>
  <li>代码: <a href="https://gist.github.com/roshameow/4d0792f08724f0bda880b564db04530f#file-boosting_train-py"><strong>boosting_train.py</strong></a></li>
</ul>

<h3 id="adaboost的一个改进">adaboost的一个改进</h3>

<p>在beblid<a href="#ref">2</a>中介绍了<strong>给每个weak-learner加一个boundary的方法</strong>: 当weak-learner结果在boundary的同侧视为匹配(比如 $h_1&lt;T, h_2&lt;T$ ), 异侧视为不匹配</p>
<ul>
  <li><strong>步骤</strong>: 对于每个weak-learner
    <ul>
      <li>给当前weak-learner的结果<strong>排序</strong></li>
      <li>先确定当前weak-learner所对应的可能的boundaries ${T_j}$
        <ul>
          <li>$T_{j-1}$ 到 $T_j$ error的改变有以下几种可能: 假设$(v_1,v_2)$ 是当前weak-learner sample pair的结果
            <ul>
              <li><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240426173943.png" alt="Pasted image 20240426173943.png" width="150" /></li>
              <li>$v_1&lt; T_{j-1}&lt;v_2&lt; T_{j}$ , $v_1, v_2$ 从$T_{j-1}$ 的异侧变为了$T_j$ 的同侧 (1)
                <ul>
                  <li>如果$(v_1, v_2)$ label为1, match, error -1</li>
                  <li>如果$(v_1, v_2)$ label为-1, miss, error +1</li>
                </ul>
              </li>
              <li>$T_{j-1}&lt;v_1&lt; T_{j}&lt;v_2$ , $v_1, v_2$ 从$T_{j-1}$ 的同侧变为了$T_j$ 的异侧 (2)
                <ul>
                  <li>如果$(v_1, v_2)$ label为1, match, error +1</li>
                  <li>如果$(v_1, v_2)$ label为-1, miss, error -1</li>
                </ul>
              </li>
              <li>$v_2&lt; T_{j-1}&lt;v_1&lt; T_{j}$ , $v_1, v_2$ 从$T_{j-1}$ 的异侧变为了$T_j$ 的同侧 (3)
                <ul>
                  <li>如果$(v_1, v_2)$ label为1, match, error -1</li>
                  <li>如果$(v_1, v_2)$ label为-1, miss, error +1</li>
                </ul>
              </li>
              <li>$T_{j-1}&lt;v_2&lt; T_{j}&lt;v_1$ , $v_1, v_2$ 从$T_{j-1}$ 的同侧变为了$T_j$ 的异侧 (4)
                <ul>
                  <li>如果$(v_1, v_2)$ label为1, match, error +1</li>
                  <li>如果$(v_1, v_2)$ label为-1, miss, error -1</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>用 $d_1$ 收集case 2,3, $d_2$ 收集case 1,4.
            <ul>
              <li>当$v_1$ 经过$T_{j-1}\rightarrow T_j$ 就会触发$d_1$,  当$v_2$ 经过$T_{j-1}\rightarrow T_j$ 就会触发$d_2$</li>
            </ul>
          </li>
        </ul>
      </li>
      <li><strong>遍历</strong>boundrary $T_j$ 和其中的sample即可计算出全部带boundary的weak-learner的error</li>
    </ul>
  </li>
  <li><strong>计算量</strong>:
    <ul>
      <li>如果是plain的adaboost, 每个weak-learner的迭代只需要更新weight, 计算量只有O(N)</li>
      <li>如果weak-learner总数很多, 每次迭代只sample一部分weak-learner, 就没法复用weak-learner的结果, 要重新计算O(N x W)
        <ul>
          <li>W 是本轮重新sample出的weak-learner</li>
        </ul>
      </li>
      <li>如果给weak-learner附上boundary, 还要
        <ul>
          <li>对每个weak-learner的结果排序O(PlogP), P=2N 是sample pair里结果的个数, 即O(W x PlogP)</li>
          <li>对每个weak-learner遍历sample 的结果 O(N), 即O(W x N)
            <ul>
              <li>如果不是采用这种遍历sample的方式, 虽然不需要排序了, 但是需要O(T x N)的error计算, 即总共O(W x T x N), 考虑到sample数N是比较大的, 这没法接受</li>
              <li>考虑在boundary同侧的结果本来就没影响, 所以拆成排序+遍历应该是boundary训练的一个常见的方法🤔️</li>
            </ul>
          </li>
          <li>因为每个weak-learner的结果和boundary的选择不一样, 似乎没法在weak-learner那边并行❓</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p>opencv的人脸检测分类器也是这种加boundary(threshold)的形式</p>

<h2 id="reference">reference</h2>
<p><span id="ref"></span>
[1]  He, Kun, Yan Lu, and Stan Sclaroff. “Local Descriptors Optimized for <strong>Average Precision</strong>.” arXiv, April 17, 2018. <a href="http://arxiv.org/abs/1804.05312">http://arxiv.org/abs/1804.05312</a>.</p>

<p>[2] Revaud, Jerome, Philippe Weinzaepfel, César De Souza, Noe Pion, Gabriela Csurka, Yohann Cabon, and Martin Humenberger. “<strong>R2D2</strong>: Repeatable and Reliable Detector and Descriptor.” arXiv, June 17, 2019. <a href="https://doi.org/10.48550/arXiv.1906.06195">https://doi.org/10.48550/arXiv.1906.06195</a>.</p>

<p>[3] Suárez, Iago, Ghesn Sfeir, José M. Buenaposada, and Luis Baumela. “<strong>BEBLID</strong>: Boosted Efficient Binary Local Image Descriptor.” <em>Pattern Recognition Letters</em> 133 (May 2020): 366–72. <a href="https://doi.org/10.1016/j.patrec.2020.04.005">https://doi.org/10.1016/j.patrec.2020.04.005</a>.</p>
<h2 id="其他资源">其他资源</h2>

<p>[1] https://www.htmlsymbols.xyz/number-symbols/circled-numbers html特殊list符号</p>

<p>[2] https://www.zhihu.com/question/382802283 很好的对于Circle loss的解释</p>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;title&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;icon&quot;=&gt;nil}]}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="algorithm" /><category term="content" /><category term="optical_flow" /><category term="deeplearning" /><category term="adaboost" /><category term="contrastive_learning" /><category term="grid_sample" /><summary type="html"><![CDATA[数据]]></summary></entry><entry><title type="html">stable-diffusion中k-sampling的不同版本</title><link href="https://roshameow.github.io//personal_homepage/docs/algorithm/stable-diffusion7/" rel="alternate" type="text/html" title="stable-diffusion中k-sampling的不同版本" /><published>2024-04-16T00:00:00+00:00</published><updated>2024-04-29T23:35:24+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/algorithm/stable-diffusion7</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/algorithm/stable-diffusion7/"><![CDATA[<p>把DPM表示成SDE(stochastic differential equation): $dx=f(x,t)dt+g(t)dw$ 
score function</p>

<p>Ancestral: 带 a的ksampler, 添加noise</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th>步骤</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Euler</td>
      <td>noise injection:<br />- increased noise $\hat \sigma$  : $\hat \sigma\leftarrow \sigma_i + \gamma\sigma_i$ <br />-  sample x with increased noise: $\hat x \leftarrow x_i + \sqrt{\hat \sigma^2-\sigma_i^2}\cdot\epsilon$ <br />Take Euler Step: <br />- $dt=\sigma_{i+1}-\hat \sigma$<br />- $denoised=model(\hat x,\hat \sigma)$ <br />- numerical derivative: $d=(x-denoised)/{\hat \sigma}$ <br />- Euler step: $x_{i+1}=\hat x+dt \cdot d$</td>
    </tr>
    <tr>
      <td>Euler Ancestral</td>
      <td> </td>
    </tr>
    <tr>
      <td>DDPM</td>
      <td> </td>
    </tr>
    <tr>
      <td>DDIM</td>
      <td> </td>
    </tr>
    <tr>
      <td>DPM-Solver</td>
      <td> </td>
    </tr>
  </tbody>
</table>

<p>Euler:</p>

<h2 id="reference">reference</h2>

<p>[1 ] Karras, Tero, Miika Aittala, Timo Aila, and Samuli Laine. “Elucidating the Design Space of Diffusion-Based Generative Models.” arXiv, October 11, 2022. <a href="https://doi.org/10.48550/arXiv.2206.00364">https://doi.org/10.48550/arXiv.2206.00364</a>.</p>

<p>[2] Lu, Cheng, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. “DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps.” arXiv, October 13, 2022. <a href="https://doi.org/10.48550/arXiv.2206.00927">https://doi.org/10.48550/arXiv.2206.00927</a>.</p>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;title&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;icon&quot;=&gt;nil}]}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="algorithm" /><category term="content" /><summary type="html"><![CDATA[把DPM表示成SDE(stochastic differential equation): $dx=f(x,t)dt+g(t)dw$ score function]]></summary></entry><entry><title type="html">小面积光流传感器算法测试 (一)</title><link href="https://roshameow.github.io//personal_homepage/docs/algorithm/optical-flow-train/" rel="alternate" type="text/html" title="小面积光流传感器算法测试 (一)" /><published>2024-04-11T00:00:00+00:00</published><updated>2024-04-29T18:40:10+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/algorithm/optical-flow-train</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/algorithm/optical-flow-train/"><![CDATA[<p>大概分为: preprocess -&gt; instant flow compute -&gt; filter correct 三个步骤</p>

<h2 id="计算连续两帧的光流">计算连续两帧的光流</h2>

<table>
  <thead>
    <tr>
      <th>算法</th>
      <th>改进</th>
      <th>公式</th>
      <th>效果</th>
      <th>存储占用</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>LK<br /><a href="https://en.wikipedia.org/wiki/Lucas–Kanade_method">Lucas-Kanade</a></td>
      <td> </td>
      <td>对图像$I$ 的每个像素, 有 $\frac{\partial I}{\partial x}dx+\frac{\partial I}{\partial y}dy=\frac{dI}{dt}$ <br />即, $\begin{bmatrix}dx \\ dy\end{bmatrix}=\begin{bmatrix}\frac{\partial I}{\partial x}\frac{\partial I}{\partial x} &amp; \frac{\partial I}{\partial x}\frac{\partial I}{\partial y} \\ \frac{\partial I}{\partial x}\frac{\partial I}{\partial y} &amp; \frac{\partial I}{\partial y}\frac{\partial I}{\partial y} \end{bmatrix}^{-1}\begin{bmatrix}\frac{\partial I}{\partial x}\frac{dI}{dt} \\ \frac{\partial I}{\partial y}\frac{dI}{dt}\end{bmatrix}=H^{-1}\begin{bmatrix}\frac{\partial I}{\partial x}\frac{dI}{dt} \\ \frac{\partial I}{\partial y}\frac{dI}{dt}\end{bmatrix}$<br />其中$\frac{\partial I}{\partial x}\approx I(x+1,y)-I(x,y)$  <br /></td>
      <td>只在光流在0-1附近有效(即subpixel的尺度)<br /><br />和$\frac{\partial I}{\partial x}$ 的计算方式有关</td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td>LK_MEAN_NORM<br /></td>
      <td>把$\frac{dI}{dt}$ 改为 $\frac{dI}{dt}-mean(\frac{dI}{dt})$<br />($mean(\frac{dI}{dt})$ 表示整体亮度的变化, 和光流无关)</td>
      <td>解决亮度变化的情况</td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td>@pre_shift</td>
      <td>先对齐到上一次计算的光流位置</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td>@compute_pyd</td>
      <td>把图像分为多层下采样计算</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td>LK-DIS<br />dense inverse search<br />结合两种方法</td>
      <td>分块计算<br />- 每块都迭代计算光流<br />  - 先按整像素移动到ssd最小的位置<br />   - 再用LK_MEAN_NORM的方法不断微调计算光流<br />       - 按照光流计算的方向对齐, 计算ssd<br />        - ssd不再变小就跳出循环<br />用所有分块的平均光流作为最终结果</td>
      <td>比LK更稳定</td>
      <td>需要所有patch的Hessian, dx, dy矩阵<br /><br />对齐patch时移动patch的中间结果<br /></td>
    </tr>
    <tr>
      <td>像素neighbor patch比对<br /></td>
      <td> </td>
      <td>$diff=dist(F(I_1(x,y))-F(I_2(x+dx,y+dy)))$ <br /><br />$argmin_{(dx,dy)}\sum_{x,y}dist(F(I_1(x,y))-F(I_2(x+dx,y+dy)))$ <br />其中, $F$ 是特征提取器, dist是距离函数</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>: 特征</td>
      <td>SAD_SIMPLE<br />sum of average differences</td>
      <td>$sad=L_1(diff)$<br />或者$sad=L_1(diff-mean(diff))$</td>
      <td>第二种更好</td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td>SAD_SUBPIXEL<br />用多项式拟合sad平面<br />(opencv Farneback<br />也是用多项式拟合, <br />不过拟合的是原图像)</td>
      <td>$P(x+\Delta x)\approx P(x)+P^\prime(x)\Delta x+\frac{P^{\prime\prime}(x)}{2}\Delta x^2$ <br />即, $\Delta x=-\frac{P^\prime(x)}{P^{\prime\prime}(x)}$ <br />其中 $P^\prime(x)\approx \frac{dist(x+1,y)-dist(x-1,y)}{2}$ , $P^{\prime\prime}$ 类似<br />-  dx, dy分开计算<br />- 直接用多项式代入也是等价的</td>
      <td>没用<br />可能是因为subpixel的部分不符合多项式</td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td>SAD_BLUR</td>
      <td>先对图像做2x2的blur<br />😮‍💨因为觉得不准的地方是不是因为刚好对齐的地方在像素的中间</td>
      <td>垃圾<br /></td>
      <td> </td>
    </tr>
    <tr>
      <td>: 距离<br /></td>
      <td>SSD_SIMPLE<br />sum of squared differences<br /></td>
      <td>$ssd=Var(diff)$</td>
      <td>比SAD稳定</td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td>SAD_BINARY</td>
      <td>$hamming=popcount(ref\ \hat\ current)$<br />其中图像是binary(image-mean(image))</td>
      <td>效果明显变差<br />binary有没有必要呢?</td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td>BINARY_FEATURE</td>
      <td>用gradient descent训练一个特征</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td>BINARY_FEATURE_BOOST</td>
      <td>用adaboost训练一个特征</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>: neighbor</td>
      <td>SAD_SIMPLE_CROSS<br />neighbor变成cross形状, 节省一些存储</td>
      <td> </td>
      <td>节省存储&amp;计算<br /></td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td>SAD_SPIRAL</td>
      <td>- 从上一次的光流位置向外螺旋状计算<br />- 遇到更小的distance提前结束循环</td>
      <td>节省存储&amp;计算<br />和LK效果类似<br />应该是都用了preshift的原因</td>
      <td> </td>
    </tr>
    <tr>
      <td><a href="https://en.wikipedia.org/wiki/Phase_correlation">phase_correlation</a></td>
      <td> </td>
      <td>$dx,dy=argmax_{x,y} F^{-1}(\frac{F(I_1)\cdot \bar{F(I_2)}}{|F(I_1)\cdot \bar{F(I_2)}|})$ <br />相当于提取图像的phase部分<br />然后用cross correlation的dist</td>
      <td><strong>效果最好</strong> <br />另外如果不做phase correlation,<br />直接做cross correlation<br />效果并不好</td>
      <td>需要ref和current的fft频域</td>
    </tr>
  </tbody>
</table>

<h2 id="测试">测试</h2>

<ul>
  <li>仿真数据: 距离仿真数据的位置</li>
  <li>真实数据:
    <ul>
      <li>稳定性: 电机带动匀速转动</li>
      <li>响应速度: 电机急停急转</li>
    </ul>
  </li>
</ul>

<p>一些结果$\downarrow$</p>

<p><img src="/personal_homepage/docs/attachment/result_compare_2000_test.png" alt="result_compare_2000_test.png" width="400" />  <img src="/personal_homepage/docs/attachment/Pasted%20image%2020240425142720.png" alt="Pasted image 20240425142720.png" width="400" /></p>

<ul>
  <li>对于测试场景来说, 所有subpixel的方法似乎都没有必要</li>
  <li>phase correlation &gt; 训练得到的几种特征平移匹配 $\approx$ sad(-mean(diff)) $\approx$ ssd &gt; sad binary » sad spiral $\approx$ LK
    <ul>
      <li><strong>对图像做detail的提取(如image-blur(image,(6x6)))</strong> 之后, sad的结果得到提高, 和训练得到的特征类似</li>
      <li>几个特征平移方法出现错误的地方可能因为超出了搜索范围导致的</li>
      <li>平移后diff的例子: <img src="/personal_homepage/docs/attachment/Pasted%20image%2020240425151045.png" alt="Pasted image 20240425151045.png" width="100" /></li>
    </ul>
  </li>
  <li>响应的测试: 除了lk会反应慢一些, 其他都在可以接受范围内</li>
  <li>这真是个特别枯燥的工作😑, 很多方法觉得, 啊, 应该不会有效果的, 但还是想着, 坚持着写出来测一下究竟差在哪里吧</li>
</ul>

<h2 id="几种opencv支持的方法测试">几种opencv支持的方法测试</h2>

<ul>
  <li>代码: <a href="https://gist.github.com/roshameow/7843f23826791c152ab2ed8c169590b9#file-flow_opencv-py"><strong>flow_opencv.py</strong></a></li>
  <li>复现修改opencv的算法, 有时需要opencv代码的中间结果
    <ul>
      <li>因为我没有下载opencv完整的源码, 而是用pip装opencv的库</li>
      <li>用<code class="language-plaintext highlighter-rouge">pkg-config --cflags --libs opencv4</code> 查看opencv的lib,include path</li>
      <li>单独把要复现的函数复制一个.cpp, 就可以随便打印中间结果了</li>
    </ul>
  </li>
</ul>

<h2 id="有用的链接">有用的链接</h2>

<p>[1] https://dsp.stackexchange.com/questions/16995/image-reconstructionphase-vs-magnitude 关于图像phase部分的提问</p>

<p>[2] https://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/OWENS/LECT7/node2.html 解释图像边缘部分的<a href="https://en.wikipedia.org/wiki/Phase_congruency#:~:text=Phase%20congruency%20is%20a%20measure,changes%20in%20illumination%20and%20contrast.">Phase congruency</a> 更强</p>

<p><img src="https://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/OWENS/LECT7/img31.gif" alt="原图" /> <img src="https://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/OWENS/LECT7/img33.gif" alt="PC图" /> 用Phase Congruency提取图像边缘的结果</p>

<p>[3]  <a href="https://en.wikipedia.org/wiki/Phase_stretch_transform#:~:text=Phase%20stretch%20transform%20(PST)%20is,time%20stretch%20dispersive%20Fourier%20transform.">Phase stretch Transform</a></p>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;title&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;icon&quot;=&gt;nil}]}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="algorithm" /><category term="content" /><category term="optical_flow" /><category term="opencv" /><category term="test" /><summary type="html"><![CDATA[大概分为: preprocess -&gt; instant flow compute -&gt; filter correct 三个步骤]]></summary></entry><entry><title type="html">画一个环形的重复图样</title><link href="https://roshameow.github.io//personal_homepage/docs/geometry/ring-pattern/" rel="alternate" type="text/html" title="画一个环形的重复图样" /><published>2024-04-10T00:00:00+00:00</published><updated>2024-04-11T03:16:48+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/geometry/ring-pattern</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/geometry/ring-pattern/"><![CDATA[<p><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240410165502.png" alt="Pasted image 20240410165502.png" width="200" /></p>

<ul>
  <li>公司需要画一个这样的图像, 本来想法是先画一个方形渐变, 复制需要的份数, 极坐标变换.</li>
  <li>想全部在photoshop里面完成的, 但是发现不知道怎么复制</li>
  <li>转向了python的pil画渐变和复制</li>
  <li>用photoshop的极坐标变换和python opencv都可以完成
    <ul>
      <li>opencv是图到图的变换</li>
    </ul>
  </li>
  <li>又想到直接画出2d的坐标meshgrid再应用变换好像更容易?</li>
</ul>

<p>代码: <a href="https://gist.github.com/roshameow/24e05cc4f336ca61c93ec2bc8e75ae39#file-ring-py"><strong>ring.py</strong></a></p>

<h2 id="其他链接">其他链接</h2>

<p>[1] https://zhuanlan.zhihu.com/p/518229060 ps插件</p>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;title&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;icon&quot;=&gt;nil}]}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="geometry" /><category term="content" /><category term="python" /><category term="photoshop" /><category term="opencv" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">blender学习: 几何节点做摄像头移动阵列</title><link href="https://roshameow.github.io//personal_homepage/docs/blender/blender-learning11/" rel="alternate" type="text/html" title="blender学习: 几何节点做摄像头移动阵列" /><published>2024-04-08T00:00:00+00:00</published><updated>2024-04-10T23:13:14+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/blender/blender-learning11</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/blender/blender-learning11/"><![CDATA[<p>参考<a href="https://www.bilibili.com/video/BV1t5411v7b1/">这个教学</a></p>

<h3 id="建模">建模</h3>

<p>直接复制作者的模型和材质</p>
<ul>
  <li>箭头</li>
  <li>摄像头</li>
</ul>

<h3 id="步骤">步骤:</h3>

<ul>
  <li>制作摄像头阵列: 用<a href="https://docs.blender.org/manual/en/latest/modeling/geometry_nodes/instances/instance_on_points.html#">Instance on Points节点</a>
    <ul>
      <li>添加一个Plane mesh, 在modifier添加几何节点</li>
      <li>在Points的地方制作一个meshgrid:
        <ul>
          <li>用Grid节点调整Grid大小和距离: 相对plane平面</li>
          <li>用<a href="https://docs.blender.org/manual/en/latest/render/shader_nodes/vector/vector_rotate.html#">Vector Rotate节点</a> 批量调整Plane里面顶点的位置</li>
        </ul>
      </li>
      <li>把摄像头主体和摄像机臂分别设置成为Plane顶点的instance: 用Join Geometry节点连接</li>
    </ul>
  </li>
  <li>设置摄像头主体追踪箭头: 分为 箭头在xz平面平移(看向箭头)和箭头在x轴旋转(跟随箭头点头) 两部分
    <ul>
      <li>平移-&gt;关于y轴旋转:
        <ul>
          <li>计算Plane里面顶点到箭头的vector: 这里面<a href="https://docs.blender.org/manual/en/latest/modeling/geometry_nodes/geometry/read/position.html">Position节点</a>给出的是Plane每个顶点的location</li>
          <li>用<a href="https://docs.blender.org/manual/en/latest/modeling/geometry_nodes/utilities/rotation/align_euler_to_vector.html">Align Euler to Vector节点</a> 设置成关于y轴旋转(因为y轴是摄像机头本来的朝向?)</li>
        </ul>
      </li>
      <li>旋转-&gt; 旋转:
        <ul>
          <li>提取箭头Rotation的x轴反向旋转, 用Rotate Euler节点的local模式添加到Plane的Rotation(plane每个顶点的rotation)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>制作箭头绕圈和点头动画:
    <ol>
      <li>绕圈: 让箭头围绕一个圈移动
        <ul>
          <li>添加一个Circle曲线</li>
          <li>给箭头添加Constraint-&gt; <a href="https://docs.blender.org/manual/en/4.1/animation/constraints/relationship/follow_path.html">Follow Path</a>
            <ul>
              <li>Target选择刚才的Circle</li>
              <li>Option+G清除位置: 加了follow path constraint之后, position会变成相对path的, 所以要清除position保证箭头在path上</li>
              <li>在起始和结束打上关键帧
                <ul>
                  <li>起始设置offset=0</li>
                  <li>结束设置offset=100: 按照文档的说法似乎应该是0到1? 但是实际用的是0到100?</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>点头: 打rotation的关键帧
        <ul>
          <li>打3个关键帧: 原始位置-&gt; 最低位置 -&gt; 原始位置
        - 在graph editor调整运动曲线</li>
        </ul>
      </li>
    </ol>
  </li>
  <li>结果: 感觉这是个很有用的互动功能, 但是这个场景有点意义不明? 是在表达什么?
    <ul>
      <li><img src="/personal_homepage/docs/attachment/camera_follow.mp4" alt="camera_follow.mp4" width="400" /></li>
    </ul>
  </li>
</ul>

<h3 id="geometry-nodes">geometry nodes</h3>

<p><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240410145231.png" alt="Pasted image 20240410145231.png" width="800" /></p>
<h3 id="用到的blender的一些快捷键">用到的blender的一些快捷键</h3>

<ul>
  <li>Option+G/S/R (<a href="https://docs.blender.org/manual/en/latest/scene_layout/object/editing/clear.html#">清除</a> )
    <ul>
      <li>把object位置变成默认的0</li>
    </ul>
  </li>
  <li>G/S (移动/缩放)的多种功能:
    <ul>
      <li>可以在Timeline移动缩放关键帧</li>
      <li>可以在graph editor里面调整运动曲线</li>
    </ul>
  </li>
  <li>I (<a href="https://docs.blender.org/manual/en/latest/animation/keyframes/editing.html#insert-keyframe">打关键帧</a> )
    <ul>
      <li>在Layout界面Object Mode 中选中</li>
    </ul>
  </li>
  <li>Ctrl+手势: 缩放graph editor面板</li>
</ul>

<h3 id="blender-script的用法">blender Script的用法</h3>

<p>看到有人用blender内置的python script编辑器做追踪的效果, 但是这个编辑器实在很难用, 而且没有自动提示的情况下, 要一直关心传参数的格式很麻烦, 远不如几何节点好用.</p>
<ul>
  <li>如果用vs code编辑再同步呢? 需要一些联动的功能:
    <ul>
      <li>安装blender的代码包以便自动提示</li>
      <li>blender执行外部代码的功能</li>
      <li>一个可以自动识别选中blender物体转换为代码的插件.</li>
    </ul>
  </li>
</ul>

<p>网上一个blender和vscode联动的方法: https://blog.csdn.net/qq_43331089/article/details/124490171</p>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;title&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;icon&quot;=&gt;nil}]}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="blender" /><category term="content" /><category term="geometry_node" /><category term="track" /><category term="shortcut" /><category term="script" /><summary type="html"><![CDATA[参考这个教学]]></summary></entry><entry><title type="html">EMVA1288 sensor测试</title><link href="https://roshameow.github.io//personal_homepage/docs/sensor/EMVA1288-sensor/" rel="alternate" type="text/html" title="EMVA1288 sensor测试" /><published>2024-04-05T00:00:00+00:00</published><updated>2024-04-11T17:20:55+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/sensor/EMVA1288-sensor</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/sensor/EMVA1288-sensor/"><![CDATA[<p><img src="/personal_homepage/docs/attachment/Pasted%20image%2020231116154316.png" alt="Pasted image 20231116154316.png" width="600" /></p>

<p>参数: QE $K, \eta$</p>

<table>
  <thead>
    <tr>
      <th>成像模型</th>
      <th>input</th>
      <th>中间结果</th>
      <th>output</th>
      <th>参数</th>
      <th>参数</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>变量下标</td>
      <td>p</td>
      <td>e</td>
      <td>y</td>
      <td>qe或<br />$\eta$</td>
      <td>$K$</td>
    </tr>
    <tr>
      <td>含义</td>
      <td>光子</td>
      <td>电子</td>
      <td>读数</td>
      <td>QE(Quantum Efficiency)</td>
      <td>System Gain</td>
    </tr>
    <tr>
      <td>测量方式</td>
      <td>由积分时间+ sensor面积 得出<br />公式: <br /><br />$\mu_p=\frac{\text{辐射能}}{\text{单个光子的辐射能}}=\frac{A(sensor面积)\cdot t(曝光时间)\cdot E(辐射照度)}{h(\text{普朗克常数})c(\text{光速})/\lambda(\text{波长})}$</td>
      <td> </td>
      <td>直接测量$y(t)$</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>分布</td>
      <td>Poisson分布<br /></td>
      <td> </td>
      <td> </td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>公式关系</td>
      <td> </td>
      <td>$\mu_e=\sigma_e^2$ <br />$K\mu_e=\mu_y-\mu_{y.dark}$</td>
      <td> </td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>统计变量</td>
      <td>$\mu_p, \sigma_p$</td>
      <td> </td>
      <td>dark noise: $\mu_{y.dark},\sigma_{y.dark}$<br />$\mu_y, \sigma_y$<br />spatial $s_y$</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
      <td> </td>
      <td> </td>
      <td> </td>
      <td> </td>
    </tr>
  </tbody>
</table>

<p>测试:</p>

<table>
  <thead>
    <tr>
      <th>测试项名称</th>
      <th>测试</th>
      <th>理论值</th>
      <th>含义</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>PTC(Photon Transfer)曲线</td>
      <td>$(\mu_y-\mu_{y.dark},\sigma_y^2)$</td>
      <td>$\sigma_y^2=K(\mu_y-\mu_{y.dark})+\sigma_{y.dark}^2$<br />是一条直线</td>
      <td>斜率 = System gain $K$</td>
    </tr>
    <tr>
      <td>SNR曲线</td>
      <td>($\mu_p$, SNR$=\frac{\mu_y-\mu_{y.dark}}{\sigma_y}$ )</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>Sensitive Curve</td>
      <td>$(\mu_p,\mu_y-\mu_{y.dark})$</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>Linearity Curve</td>
      <td>?</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>Linear error</td>
      <td>linearity curve上data值和拟合值的距离</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>Deviation Linearity Curve</td>
      <td>$(\mu_p,LE(\text{Linearity Error}))$<br />Linearity Error是<br /></td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>Stablity Check</td>
      <td>两项<br /> $(\mu_y-\mu_{y.dark},\sigma_y)$<br /> $(\mu_y-\mu_{y.dark},\mu[0]-\mu[1])$</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>darkcurrent</td>
      <td>两项<br />$(t,\mu_{y.dark})$ <br />$(t,\sigma_{y.dark})$</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>Temporal dark noise</td>
      <td> </td>
      <td> </td>
      <td> </td>
    </tr>
  </tbody>
</table>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;title&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;icon&quot;=&gt;nil}]}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="sensor" /><category term="content" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">pyside6一些功能的用法</title><link href="https://roshameow.github.io//personal_homepage/docs/tool/pyside6-tech/" rel="alternate" type="text/html" title="pyside6一些功能的用法" /><published>2024-04-01T00:00:00+00:00</published><updated>2024-04-07T20:09:32+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/tool/pyside6-tech</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/tool/pyside6-tech/"><![CDATA[<p>pyside是qt的python封装, API的调用方法基本差不多. 用pyside从零开始写一个gui用于标注或测试(调参数或者看中间结果), 每次花费时间都比想象的要少的多. 功能方便而且代码的可读性非常好.</p>

<h2 id="工具">工具</h2>

<ul>
  <li>designer和vscode
    <ul>
      <li>desginer主要用到promote, 加载资源, 配置qss的功能</li>
    </ul>
  </li>
  <li>vscode的PYQT Integration, 配置好uic, rcc路径后, 可以右键编译</li>
</ul>

<h2 id="事例">事例</h2>
<h3 id="视频播放">视频播放</h3>

<p>用QTimer和opencv实现</p>

<p><img src="/personal_homepage/docs/attachment/Screen%20Recording%202024-04-07%20at%2009.30.20.mp4" alt="Screen Recording 2024-04-07 at 09.30.20.mp4" width="200" /></p>

<ol>
  <li>把label提升到自定义可以drop file的LabelImage</li>
  <li>用timer设置play, pause功能, 进度条拖动功能</li>
</ol>

<ul>
  <li>代码: <a href="https://gist.github.com/roshameow/c1c27989df0ac90a89ee9d99b87d6d59#file-main-py"><strong>main.py</strong></a> , <a href="https://gist.github.com/roshameow/c1c27989df0ac90a89ee9d99b87d6d59#file-ui_labelimage-py"><strong>Ui_LabelImage.py</strong></a></li>
</ul>

<h3 id="动态折线图">动态折线图</h3>

<p>用QtCharts实现</p>

<p><img src="/personal_homepage/docs/attachment/Screen%20Recording%202024-04-07%20at%2009.46.44.mp4" alt="Screen Recording 2024-04-07 at 09.46.44.mp4" width="800" /></p>

<ol>
  <li>界面画出QWidget并提升到自定义的LineChart, 继承QChartView</li>
  <li>定义chart和series</li>
  <li>修改QChartView的样式: 在designer里用qss实现</li>
  <li>修改QChart的样式:
    <ul>
      <li><a href="https://stackoverflow.com/questions/39146502/how-to-remove-margin-from-qchartview-or-qchart">QChart调整和QChartView之间的Margin</a></li>
      <li><a href="https://stackoverflow.com/questions/51398463/qt-chart-remove-space-for-title-legend">QChart调整axis和边界之间的Margin</a></li>
      <li><a href="https://doc.qt.io/qt-6/qtcharts-customchart-example.html">QChart设置样式</a></li>
    </ul>
  </li>
  <li>添加series update代码</li>
</ol>

<ul>
  <li>代码: <a href="https://gist.github.com/roshameow/d1e0892205fa832aeb930a75130864e7#file-update_frame-py"><strong>update_frame.py</strong></a> , <a href="https://gist.github.com/roshameow/d1e0892205fa832aeb930a75130864e7#file-line_chart-py"><strong>line_chart.py</strong></a></li>
</ul>

<h3 id="图像标记">图像标记</h3>

<iframe src="//player.bilibili.com/player.html?aid=1452824219&amp;bvid=BV1bq421F7sF&amp;cid=1496585736&amp;p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>

<ol>
  <li>选择文件: 在QTreeView上设置model为QFileSystemModel</li>
  <li>互动标记图片
    <ul>
      <li>用paintEvent和QPainter实现标记</li>
    </ul>
  </li>
  <li>切换label
    <ul>
      <li>用QFile替换svg的颜色</li>
    </ul>
  </li>
  <li>显示位置数据: 在QTableView上设置model为自定义的PandasModel
    <ul>
      <li>给tableView设置Delegate更改颜色和行为</li>
    </ul>
  </li>
</ol>

<ul>
  <li>代码: <a href="https://gist.github.com/roshameow/7d45d536dd4ab8ff6ff618b7911b5890#file-main-py"><strong>main.py</strong></a> , <a href="https://gist.github.com/roshameow/7d45d536dd4ab8ff6ff618b7911b5890#file-mask_model-py">tableView添加互动: <strong>mask_model.py</strong></a> , <a href="https://gist.github.com/roshameow/7d45d536dd4ab8ff6ff618b7911b5890#file-pyside_util-py">更改svg颜色: <strong>pyside_util.py</strong></a> , <a href="https://gist.github.com/roshameow/7d45d536dd4ab8ff6ff618b7911b5890#file-ui_labelimage-py">图片添加互动: <strong>Ui_LabelImage.py</strong></a></li>
</ul>

<h2 id="资源">资源</h2>

<p>讲的很好的入门视频:</p>

<iframe src="//player.bilibili.com/player.html?aid=610679490&amp;bvid=BV1c84y1N7iL&amp;cid=1098863799&amp;p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>

<p>其他:</p>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;title&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;icon&quot;=&gt;nil}]}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="tool" /><category term="content" /><category term="pyside6" /><category term="gui" /><category term="python" /><summary type="html"><![CDATA[pyside是qt的python封装, API的调用方法基本差不多. 用pyside从零开始写一个gui用于标注或测试(调参数或者看中间结果), 每次花费时间都比想象的要少的多. 功能方便而且代码的可读性非常好.]]></summary></entry><entry><title type="html">blender学习: 用粒子系统做毛毡效果</title><link href="https://roshameow.github.io//personal_homepage/docs/blender/blender-learning10/" rel="alternate" type="text/html" title="blender学习: 用粒子系统做毛毡效果" /><published>2024-03-25T00:00:00+00:00</published><updated>2024-03-26T19:01:05+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/blender/blender-learning10</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/blender/blender-learning10/"><![CDATA[<h3 id="步骤">步骤:</h3>

<p>参考<a href="https://www.xiaohongshu.com/explore/65eb070500000000030320e2/">这个教学</a></p>
<ul>
  <li>Paticles添加毛发粒子: Particle type选择<a href="https://docs.blender.org/manual/en/latest/physics/particles/hair/index.html">hair</a>
    <ul>
      <li>Emission
        <ul>
          <li>number=5000 发根的总数量</li>
          <li>hair length=0.03</li>
          <li>Segments = 5 卷曲?</li>
        </ul>
      </li>
      <li><a href="https://docs.blender.org/manual/en/latest/physics/particles/emitter/render.html">Render</a> -&gt;Path-&gt;Steps=5
        <ul>
          <li>对hair做subdivision的次数</li>
        </ul>
      </li>
      <li><a href="https://docs.blender.org/manual/en/latest/physics/particles/hair/display.html">Viewport Display</a>-&gt; Strand Step=5</li>
      <li><a href="https://docs.blender.org/manual/en/latest/physics/particles/emitter/children.html">Children</a> 选择simple
        <ul>
          <li>display amount=100, render amount=100</li>
          <li>Roughness
            <ul>
              <li>Random = 0.08</li>
              <li>Size = 0.851</li>
            </ul>
          </li>
          <li>Kink(纽结)选择Curl
            <ul>
              <li>Amplitude(振幅) = 0.03</li>
              <li>Hair shape
                <ul>
                  <li>Diameter Root &gt; Tip</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>shader:
    <ul>
      <li>给Particles添加一个Principle Hair BSDF的material
        <ul>
          <li>roughness= 0.86</li>
          <li>Radial Roughness=0.95</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>结果: 没法复刻例图的效果, 比起毛毡更像搓澡巾😣
    <ul>
      <li><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240326101412.png" alt="Pasted image 20240326101412.png" width="300" />  局部: <img src="/personal_homepage/docs/attachment/Pasted%20image%2020240326101810.png" alt="Pasted image 20240326101810.png" width="200" /></li>
      <li>是不是还需要设置其他参数?</li>
    </ul>
  </li>
</ul>

<h3 id="模型">模型</h3>

<p>[1] https://free3d.com/3d-model/pumpkin-57117.html</p>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;title&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;icon&quot;=&gt;nil}]}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="blender" /><category term="content" /><category term="粒子系统" /><category term="hair" /><summary type="html"><![CDATA[步骤:]]></summary></entry></feed>