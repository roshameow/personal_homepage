<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://roshameow.github.io//personal_homepage/atom.xml" rel="self" type="application/atom+xml" /><link href="https://roshameow.github.io//personal_homepage/" rel="alternate" type="text/html" /><updated>2024-02-27T16:08:26+00:00</updated><id>https://roshameow.github.io//personal_homepage/atom.xml</id><title type="html">Liu, Wen’s Home Page</title><subtitle>Work, Experiments and Ideas.</subtitle><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;title&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;icon&quot;=&gt;nil}]}</name><email>w.liuatnk@gmail.com</email></author><entry><title type="html">stable-diffusion的用法: 常用插件</title><link href="https://roshameow.github.io//personal_homepage/docs/tool/stable-diffusion5/" rel="alternate" type="text/html" title="stable-diffusion的用法: 常用插件" /><published>2024-02-27T00:00:00+00:00</published><updated>2024-02-28T07:57:30+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/tool/stable-diffusion5</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/tool/stable-diffusion5/"><![CDATA[<p>comfyui manager</p>

<p>impact pack</p>

<p>comfyroll_customnodes</p>

<p>inpaint_nodes</p>

<p>rgthree-comfy</p>

<p>Marigold</p>

<p>WD14-tagger</p>

<p>resize</p>

<p>image-saver</p>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;title&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;icon&quot;=&gt;nil}]}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="tool" /><category term="content" /><category term="ComfyUI" /><summary type="html"><![CDATA[comfyui manager]]></summary></entry><entry><title type="html">stable-diffusion的用法: 常用的作图功能-提升细节</title><link href="https://roshameow.github.io//personal_homepage/docs/photo/stable-diffusion4/" rel="alternate" type="text/html" title="stable-diffusion的用法: 常用的作图功能-提升细节" /><published>2024-02-24T00:00:00+00:00</published><updated>2024-02-28T07:55:46+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/photo/stable-diffusion4</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/photo/stable-diffusion4/"><![CDATA[<p>和inpaint一样要利用stable-diffusion的图生图功能, 想让图像忠于原图的情况下提升图像细节, 解决图像模糊, 扭曲, 不合理的部分.</p>

<h2 id="使用">使用</h2>

<ul>
  <li>模型:
    <ul>
      <li>一般模型: 对于模糊的图, encode+ksample+decode就可以提升细节</li>
      <li><a href="https://iceclear.github.io/projects/stablesr/">StableSR</a>: 基于sd2.1, 关于细节修复任务重新训练的模型</li>
      <li><a href="https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0">sdxl refiner模型</a>: 输入latent image, 官方建议接在sdxl base模型的后面用
        <ul>
          <li>sdxl refiner一般只能配和sdxl base模型, 不能配合其他风格的sdxl模型, 所以不是很实用</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>控制:
    <ul>
      <li>更像原图:
        <ul>
          <li>风格控制: 和inpaint一样, 可以用text promt和ipadpater控制</li>
          <li>微调模型控制:
            <ul>
              <li>controlnet tile(配合v1.5模型): 这是个非常好用的模型👍, 可以在分块(输入的不是一整张图)的时候, 更倾向理解图像部分而不是text prompt</li>
              <li><a href="https://huggingface.co/lllyasviel/sd_control_collection/discussions/1">关于sdxl模型没有一个tile模型的讨论</a></li>
            </ul>
          </li>
        </ul>
      </li>
      <li>提升细节:
        <ul>
          <li>从流程上控制:
            <ul>
              <li><a href="https://ku-cvlab.github.io/Self-Attention-Guidance">Self-attention Guidance(SAG)</a>
                <ul>
                  <li>和cfg(class free guidance)类似, 是更改reverse diffusion process的过程</li>
                  <li>让图片变清晰原理:  认为self-attention值更大的位置是更重要的位置, 让其离blurry的方向更远, 即更清晰
                    <ul>
                      <li>从文章里的例子看, 这种做法达成了一种神奇的自纠正的效果, 可能是因为在reverse diffusion process中重要的地方更快的变清晰, 引导了其他部分的生成</li>
                    </ul>
                  </li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>方便的节点: stable-diffusion本身的图生图功能input和output等大的, 图像大小和模型不匹配的时候就需要分块处理或放大
    <ul>
      <li>分块: 下面两个功能类似
        <ul>
          <li><a href="https://github.com/ltdrdata/ComfyUI-Impact-Pack/blob/5f73ac55c02b588bf151c355522e8c402723e134/modules/impact/core.py#L329">ComfyUI Impact Pack 提供的detailer节点</a> : 包含一些方便的功能
            <ul>
              <li>可以分块图生图</li>
              <li>可以设置cycle次数, 即图生图的次数 (如果comfyUI本身提供循环功能就好了)</li>
            </ul>
          </li>
          <li><a href="https://github.com/ssitu/ComfyUI_UltimateSDUpscale">UltimateSDUpscale</a>
            <ul>
              <li>支持分块图生图</li>
              <li>改变tile的一些融合方式</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>放大: 放大经常伴随细节损失, 一般是和refine功能接力使用: upscale-&gt;refine-&gt;upscale-&gt;refine-&gt;upscale-&gt;refine…</li>
    </ul>
  </li>
</ul>

<h2 id="效果">效果</h2>

<ul>
  <li>使用时要注意:
    <ul>
      <li>输入图像的尺寸: 如果输入图像太小也是无法生成正常图片的, 尽量用外部的resize和upscale调整到和模型训练的大小相近</li>
      <li>图像包含的语义信息的尺寸</li>
      <li>reverse diffusion process的参数: cfg, denoise, steps等</li>
    </ul>
  </li>
  <li>对一些常见的噪声效果很好, 例如: jpg压缩的高频alias, 摩尔纹
    <ul>
      <li>但是对于其他噪声就不认识, 可能被认作图像本身的风格? 例如我用其他prior搭配<a href="https://en.wikipedia.org/wiki/Augmented_Lagrangian_method">admm算法</a>恢复图像的中间结果</li>
    </ul>
  </li>
  <li>恢复出的图像风格会受选择的主要模型风格影响, 很难保证细节不变风格还原</li>
  <li>恢复真实人物的效果很差, 可能是本身我们对人脸细节要求就更严格? 可能需要加载专门控制人脸的微调模型</li>
</ul>

<h2 id="comfyui-workflow">ComfyUI workflow</h2>

<p>用sdxl的refiner$\downarrow$ : <a href="https://gist.github.com/roshameow/7a46bcdb4d7bdc5b2c0e758e4aa85a9f#file-refine_sdxl_refiner-json"><strong>refine_sdxl_refiner.json</strong></a></p>

<p><img src="/personal_homepage/docs/attachment/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202024-02-27%20231030.png" alt="屏幕截图 2024-02-27 231030.png" width="800" /></p>

<p>用detailer节点和 <a href="https://github.com/ssitu/ComfyUI_UltimateSDUpscale">UltimateSDUpscale</a> 节点$\downarrow$ : <a href="https://gist.github.com/roshameow/7a46bcdb4d7bdc5b2c0e758e4aa85a9f#file-refine_impact_pack_detailer_sag_ultimatesd-json"><strong>refine_impact_pack_detailer_SAG_UltimateSD.json</strong></a></p>
<ul>
  <li>参考<a href="https://comfyworkflows.com/workflows/f9ed8191-b2e1-4417-ac26-2ecc7ff2a484">这个comfyworkflows上分享的workflow</a></li>
</ul>

<p><img src="/personal_homepage/docs/attachment/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202024-02-27%20221824.png" alt="屏幕截图 2024-02-27 221824.png" width="800" /></p>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;title&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;icon&quot;=&gt;nil}]}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="photo" /><category term="content" /><category term="ComfyUI" /><category term="sdxl" /><category term="refine" /><summary type="html"><![CDATA[和inpaint一样要利用stable-diffusion的图生图功能, 想让图像忠于原图的情况下提升图像细节, 解决图像模糊, 扭曲, 不合理的部分.]]></summary></entry><entry><title type="html">stable-diffusion的用法: 常用的作图功能-抠图, inpainting</title><link href="https://roshameow.github.io//personal_homepage/docs/photo/stable-diffusion3/" rel="alternate" type="text/html" title="stable-diffusion的用法: 常用的作图功能-抠图, inpainting" /><published>2024-02-18T00:00:00+00:00</published><updated>2024-02-26T19:40:13+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/photo/stable-diffusion3</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/photo/stable-diffusion3/"><![CDATA[<h2 id="用sam抠图">用SAM抠图</h2>

<p>以下两种方式都是对segment-anything二次封装</p>
<ul>
  <li>在ComfyUI里用<a href="https://github.com/storyicon/comfyui_segment_anything">segment anything节点</a>
    <ul>
      <li>特点: 任何shape都可以, 输入text prompt, 但是没法输入位置</li>
      <li>步骤:
        <ol>
          <li><a href="https://github.com/IDEA-Research/GroundingDINO">groundingDINO</a>模型: image, text prompt-&gt;目标检测box</li>
          <li><a href="https://github.com/facebookresearch/segment-anything">SAM</a>模型: image, box-&gt; mask
            <ol>
              <li>SAM模型本体可以接受多种形式的prompt</li>
              <li>SAM模式的特点是对image只encode一次, 这里box是作为prompt形式输入的, 不是在图像上crop然后处理cropped image</li>
            </ol>
          </li>
        </ol>
      </li>
      <li>🤔️: SAM本来就可以输入text prompt, 加入groundingDINO模型是因为SAM的text promt效果不好吗?</li>
    </ul>
  </li>
  <li><a href="https://github.com/geekyutao/Inpaint-Anything">inpaint-anything</a>
    <ul>
      <li>特点: 输入point prompt和text prompt(optional), 对输入图像大小有要求, 还接入了一个stable-diffusion的inpaint模块</li>
    </ul>
  </li>
</ul>

<h2 id="inpainting--outpainting">inpainting &amp; outpainting</h2>

<ul>
  <li>图生图功能: 利用stable-diffusion的sampler流程, noise mask之外的部分网络就不会去更改, 利用VAE decoder的填补能力
    <ul>
      <li>VAE encode成latent: 根据情况选择图像输入
        <ol>
          <li>输入原图: 直接用VAE Encode-&gt;latent-&gt;Set Latent Noise Mask-&gt;Ksampler
            <ul>
              <li>适合想保留部分原图元素的</li>
            </ul>
          </li>
          <li>输入用灰色填补mask的图: 用 <a href="https://blenderneko.github.io/ComfyUI-docs/Core%20Nodes/Latent/inpaint/VAEEncodeForInpainting/">VAE Encode(for inpainting)</a> -&gt;latent-&gt;Ksampler
            <ul>
              <li><a href="https://blenderneko.github.io/ComfyUI-docs/Core%20Nodes/Latent/inpaint/VAEEncodeForInpainting/">VAE Encode(for inpainting)</a> 相当于 用灰色mask的图<a href="https://blenderneko.github.io/ComfyUI-docs/Core%20Nodes/Latent/VAEEncode/">VAE Encode</a> + <a href="https://blenderneko.github.io/ComfyUI-docs/Core%20Nodes/Latent/inpaint/SetLatentNoiseMask/">Set Latent Noise Mask</a> 两个节点的效果</li>
              <li>适合想要在mask处新生成物体的</li>
            </ul>
          </li>
          <li>输入LaMa inpaint的图: 用<a href="https://github.com/Acly/comfyui-inpaint-nodes">fooocus comfyUI版本</a> 提供的inpaint from model节点, 然后再走VAE Encode-&gt;latent-&gt;Set Latent Noise Mask-&gt;Ksampler的流程
            <ul>
              <li><strong>一定要用GrowMask加大mask的范围</strong>(最好把图像中受物体影响的部分全mask掉,例如mask人物的影子, 或是被挡住一半的文字. 不要留下不和谐的部分. <a href="#ref">3</a> )</li>
              <li>适合有重复pattern的场景效果</li>
            </ul>
          </li>
        </ol>
      </li>
      <li>用 <a href="https://blenderneko.github.io/ComfyUI-docs/Core%20Nodes/Latent/inpaint/SetLatentNoiseMask/">Set Latent Noise Mask</a> 在做sampler的流程中对mask以外的地方进行保留
        <ul>
          <li>sdxl模型latent image可以对应到8x8的pixel patch, 输入的mask会resize到latent大小成为latent mask</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>控制: inpainting任务是想让生成的部分风格接近原图, 和原图边界融合的更好
    <ul>
      <li>用ipadapter控制风格
        <ul>
          <li>ipadapter会参考原图物体的位置关系, 使这个控制在outpainting里非常鸡肋, 我们的目的是生成和原图风格一致的新物体, 而不是把原来的物体叠加在图片上</li>
          <li>ipadapter的attention mask输入是 整张图-&gt; 部分 的流程, 而不是我们在inpainting任务想要的 部分 -&gt; 整张图
            <ul>
              <li>attention mask可以用来把两张图融合在同一张图的不同位置, 例如让两个人合影</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>用text prompt控制
        <ul>
          <li><a href="https://github.com/pythongosssss/ComfyUI-WD14-Tagger">WD14 Tagger</a> 推导提示词</li>
          <li>用<a href="https://github.com/ZHO-ZHO-ZHO/ComfyUI-Gemini">Gemini</a> 的API的打标功能</li>
        </ul>
      </li>
      <li>微调模型控制: 如果不加任何模型, 生成出来的结果和原图看起来完全不融合
        <ul>
          <li>controlnet inpaint:(配合v1.5)
            <ul>
              <li>inpaintpreprocessor输入image, mask</li>
              <li>inpaintpreprocessor处理过的图像输入inpaint controlnet</li>
              <li>controlnet连接text prompt</li>
              <li>用controlnet的时候甚至可以用空的latent, 因为controlnet会更改sampler流程为生成整张图?
                <ul>
                  <li>但是一般还是输入image latent 参考, 不然的话颜色风格会变奇怪</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>fooocus inpaint: (配合sdxl)
            <ul>
              <li><a href="https://github.com/lllyasviel/Fooocus/discussions/414">fooocus自己的ui界面做inpaint</a>网上评价很好</li>
              <li><a href="https://github.com/Acly/comfyui-inpaint-nodes">fooocus comfyUI版本</a>的一些使用问题: README里给出的工作流都不太work
                <ul>
                  <li>输入model, patch(包含一个head模型和一个lora模型), latent(必须包含latent image和noise mask)</li>
                  <li>使用时感觉好像一定要配合VAE Encode(for inpainting), 如果输入lama补充后的, 结果会出现奇怪的偏色, 是模型内部对mask部分进行了识别吗?</li>
                  <li>用fooocus 开发的dpmpp sampler方式迭代, 也会出现奇怪的东西, 是comfyui的实现有问题吗?</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>结果: 远没有达到一键生成的效果, 但是比起lama只会填入重复花纹还是有很大进步
    <ul>
      <li>新生成的部分和原图融合的不好
        <ul>
          <li>按这个流程原图mask之外的内容不会重新生成, 融合的不好可以理解, 应该是生成整张图才对. 这个工作流应该是有问题, 没有发挥stable diffusion的全部能力</li>
        </ul>
      </li>
      <li>需要修改seed多次生成(抽卡), 人工选好一点的效果做为图生图VAE部分的输入继续</li>
      <li>尤其是人物, 动物, 补全的地方可以看出很明显的问题, 是多次抽卡解决不了的, 必须要配合人工实时修改</li>
    </ul>
  </li>
</ul>

<h2 id="comfyui-workflow">ComfyUI workflow</h2>
<h3 id="换背景">换背景</h3>

<p>工作流: <a href="https://gist.github.com/roshameow/041450c340e56d39f12c93bd7d2d8606#file-inpainting_background_sam_fooocus-json"><strong>inpainting_background_SAM_fooocus.json</strong></a> $\downarrow$</p>

<p><img src="/personal_homepage/docs/attachment/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202024-02-23%20232215.png" alt="屏幕截图 2024-02-23 232215.png" width="800" /></p>
<h3 id="去掉不想要的人物">去掉不想要的人物</h3>

<p><a href="https://gist.github.com/roshameow/041450c340e56d39f12c93bd7d2d8606#file-inpaint_delete_person_sam_lama_fooocus-json"><strong>inpaint_delete_person_SAM_Lama_fooocus.json</strong></a> $\downarrow$</p>

<p><img src="/personal_homepage/docs/attachment/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202024-02-23%20231325.png" alt="屏幕截图 2024-02-23 231325.png" width="800" /></p>

<h3 id="修改局部">修改局部</h3>

<p><a href="https://gist.github.com/roshameow/041450c340e56d39f12c93bd7d2d8606#file-inpaint_edit_fooocus_ipadapter-json"><strong>inpaint_edit_fooocus_ipadapter.json</strong></a> $\downarrow$</p>

<p><img src="/personal_homepage/docs/attachment/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202024-02-23%20235229.png" alt="屏幕截图 2024-02-23 235229.png" width="800" /></p>

<p>和inpaint类似, 用Pad for Outpainting生成mask和灰色填补的图像</p>

<h3 id="outpaint-更改图片尺寸">outpaint 更改图片尺寸</h3>

<p><a href="https://gist.github.com/roshameow/041450c340e56d39f12c93bd7d2d8606#file-outpainting_lama_fooocus_ipadapter-json"><strong>outpainting_lama_fooocus_ipadapter.json</strong></a> $\downarrow$</p>

<p><img src="/personal_homepage/docs/attachment/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202024-02-23%20230951.png" alt="屏幕截图 2024-02-23 230951.png" width="800" /></p>

<ul>
  <li>想生成简单纯色的背景似乎不可能, 网络总会自己加些东西: 是不是网络认为纯色背景是一种未完成的状态?</li>
</ul>

<h3 id="注意事项">注意事项</h3>

<ul>
  <li>白色是想要网络生成的部分</li>
  <li>如果在一个工作流里同时用SAM, foocus inpaint, ipadapter等模型, 我16G的显存会不够用: 其实SAM在生成mask之后就可以移出显存了, 但是不清楚comfyui怎么操作</li>
</ul>

<h2 id="reference">reference</h2>
<p><span id="ref"></span></p>

<p>[1] https://github.com/Sanster/IOPaint inpaint+outpaint合集</p>

<p>[2] https://github.com/comfyanonymous/ComfyUI/blob/master/nodes.py#L331 ComfyUI 节点代码</p>

<p>[3] https://medium.com/gliacloud/removing-any-object-from-your-photo-with-lama-7e966765fadc lama实际使用感受</p>

<p>[4] https://huggingface.co/blog/TimothyAlexisVass/explaining-the-sdxl-latent-space sdxl的latent image的解释. sdxl的latent image和原图像素对应.</p>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;title&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;icon&quot;=&gt;nil}]}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="photo" /><category term="content" /><category term="ComfyUI" /><category term="sdxl" /><category term="inpaint" /><summary type="html"><![CDATA[用SAM抠图]]></summary></entry><entry><title type="html">photoshop: 酸性风格海报</title><link href="https://roshameow.github.io//personal_homepage/docs/photo/photoshop1/" rel="alternate" type="text/html" title="photoshop: 酸性风格海报" /><published>2024-02-14T00:00:00+00:00</published><updated>2024-02-19T00:21:24+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/photo/photoshop1</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/photo/photoshop1/"><![CDATA[<p><a href="https://aesthetics.fandom.com/wiki/Acid_Design">酸性(acid)风格</a> 迷幻感 . 设计元素包含: 高饱和, 流动性, 未来复古元素, 格纹, 镭射金属, 霓虹色. 
<a href="https://www.bilibili.com/video/BV1gD4y127m1/">这个链接解释的比较详细</a></p>

<h2 id="制作液态效果-流动感-金属感">制作液态效果, 流动感, 金属感</h2>

<p>在网上看到的两个简易教程$\downarrow$</p>
<ol>
  <li>物体只保留纹理
    <ol>
      <li>选取素材变成gray image: 图像-&gt; 调整 -&gt; 黑白</li>
    </ol>
  </li>
  <li>制作物体中间类似流动金属的效果
    <ol>
      <li>教程1的做法:
        <ol>
          <li>复制物体</li>
          <li>gaussian滤波
            <ul>
              <li>不想有显得特别不光滑的地方</li>
            </ul>
          </li>
          <li>反色</li>
          <li>下层用铬黄渐变, 细节,平滑度调到最高</li>
          <li>上层混合模式改成正片叠底(multiply): $I_1*I_2$
            <ul>
              <li>保留原物体的细节</li>
            </ul>
          </li>
          <li>上下两个图层合并</li>
          <li>混合模式改为滤色(screen): 是$clip(I_1+I_2,0,1)$ 吗? 因为看到结果有全白部分
            <ul>
              <li>仿玻璃的质感: 透明, 并比原图层亮</li>
            </ul>
          </li>
        </ol>
      </li>
      <li>教程2的做法:
        <ol>
          <li>复制物体</li>
          <li>上层反色, 用差值模式(difference): $abs(I_1-I_2)$
            <ul>
              <li>这个结果是越偏离中间值128的像素越亮</li>
            </ul>
          </li>
          <li>上下两个图层合并</li>
          <li>重复2-3次
            <ul>
              <li>这个的结果可能是在0-255范围内设置了几个量化点, 越靠近其中一个点就会越亮
                <ul>
                  <li><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240218142334.png" alt="Pasted image 20240218142334.png" width="400" /></li>
                  <li>所以完全可以直接拉曲线的. 步骤1-4都有点多余. 只要有那种相近的地方明暗变化很快的感觉就可以了.</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>用曲线压缩动态范围</li>
          <li>把一个金属色流动的背景叠加(overlay)在上层
            <ul>
              <li>overlay是screen和multiply的混合</li>
            </ul>
          </li>
        </ol>
      </li>
    </ol>
  </li>
</ol>

<p><a href="https://www.bilibili.com/video/BV18w411B75n/">教程1</a> : <img src="/personal_homepage/docs/attachment/Pasted%20image%2020240217233149.png" alt="Pasted image 20240217233149.png" width="300" />   <a href="https://www.bilibili.com/video/BV1684y1T7t6/">教程2</a> : <img src="/personal_homepage/docs/attachment/Pasted%20image%2020240218102040.png" alt="Pasted image 20240218102040.png" width="300" /></p>

<ul>
  <li>这两个教程看起来都有点随便, 顶多算一种特殊效果滤镜.
    <ul>
      <li>看起来具体的步骤并不重要. 因为里面确实有些操作意义不明可以简化, 奇怪的不是教程里绕了弯路, 而是不明白每一步的动机. 难道是随便试了试发现这样可以?</li>
    </ul>
  </li>
  <li>在<a href="https://www.bilibili.com/video/BV18w411B75n/">教程1</a> 里用铬黄渐变的filter制作金属感,  <a href="https://www.bilibili.com/video/BV1684y1T7t6/">教程2</a> 更是直接叠了一个金属色的图
    <ul>
      <li><a href="https://www.youtube.com/watch?v=RHgT4Q4dDY8">这个教程</a> 很详细的用bevel(倒角)和emboss(浮雕) 制作金属反光效果. 铬黄渐变的滤镜可能也是类似原理的一组滤镜?</li>
    </ul>
  </li>
  <li>photoshop的blending mode有些模式找不到公式…感觉很难理解用法
    <ul>
      <li>比如overlay和screen…</li>
    </ul>
  </li>
</ul>

<h2 id="reference">reference</h2>

<p>[1] http://pigs-blood-cake.blogspot.com/p/photoshop.html ps中英文对照</p>

<p>[2] https://photoshoptrainingchannel.com/blending-modes-explained/#multiply ps图层混合</p>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;title&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;icon&quot;=&gt;nil}]}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="photo" /><category term="content" /><category term="photoshop" /><category term="filter" /><category term="design" /><summary type="html"><![CDATA[酸性(acid)风格 迷幻感 . 设计元素包含: 高饱和, 流动性, 未来复古元素, 格纹, 镭射金属, 霓虹色. 这个链接解释的比较详细]]></summary></entry><entry><title type="html">blender学习: 玻璃杯</title><link href="https://roshameow.github.io//personal_homepage/docs/blender/blender-learning6/" rel="alternate" type="text/html" title="blender学习: 玻璃杯" /><published>2024-02-10T00:00:00+00:00</published><updated>2024-02-16T09:12:32+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/blender/blender-learning6</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/blender/blender-learning6/"><![CDATA[<p>由cylinder变形得到水杯. 用cycles渲染玻璃材质.</p>
<h2 id="步骤">步骤</h2>

<p>参考<a href="https://www.bilibili.com/video/BV1Fg4y127c7/">这个b站的教学视频</a></p>

<ul>
  <li>制作水杯:
    <ol>
      <li>添加一个cylinder</li>
      <li>删除上下两个底: 在Edit Mode, 开启透视, 按3选中面删除</li>
      <li>把下底缩小一点: 按1选中顶点</li>
      <li>填充底面: 用Ctrl+F填充, 选择<a href="https://docs.blender.org/manual/en/latest/modeling/meshes/editing/face/grid_fill.html">Grid Fill</a>, 调节Offset
        <ul>
          <li><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240212082748.png" alt="Pasted image 20240212082748.png" width="100" /></li>
        </ul>
      </li>
      <li>做出玻璃杯的厚度: 选中上面的顶点, 用E+S向内
        <ul>
          <li><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240212082705.png" alt="Pasted image 20240212082705.png" width="100" /></li>
        </ul>
      </li>
      <li>做出玻璃杯的内层: 继续E+Y向下, S向内</li>
      <li>做出玻璃杯的内底: Ctrl+F填充, 选择<a href="https://docs.blender.org/manual/en/latest/modeling/meshes/editing/face/grid_fill.html">Grid Fill</a> , 调节Offset</li>
      <li>把杯子变光滑:
        <ol>
          <li>在边缘处添加loop cut(卡线): Ctrl+R 增加loop cut, Ctrl+B向上下拉伸
            <ul>
              <li><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240212133706.png" alt="Pasted image 20240212133706.png" width="100" /></li>
              <li>添加loop cut是为了之后把杯子边缘变成弧形, 而不影响杯壁和杯底平的部分</li>
            </ul>
          </li>
          <li>Shift+option 长按出现选项, 选中loop, 按G移动loop进行微调
            <ol>
              <li><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240212141056.png" alt="Pasted image 20240212141056.png" width="100" /></li>
            </ol>
          </li>
        </ol>
      </li>
      <li>改成shade smooth</li>
      <li>加一个subdivision modifier</li>
    </ol>
  </li>
  <li>制作拍摄场景:
    <ol>
      <li>添加一个plane mesh</li>
      <li>在Edit Mode选中一个边, E+Z复制一份, 向上拉
        <ol>
          <li><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240212141942.png" alt="Pasted image 20240212141942.png" width="100" /></li>
        </ol>
      </li>
      <li>选中中间的edge, 拉成弧线: Shift+Option选中, Ctrl+B拉伸, 按S改变段数
        <ol>
          <li><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240212142443.png" alt="Pasted image 20240212142443.png" width="150" /></li>
        </ol>
      </li>
      <li>选择shade smooth</li>
    </ol>
  </li>
  <li>打光
    <ul>
      <li>一个area light做主光, 另一个area light做辅光: 形状调节成disk
        <ul>
          <li><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240212143950.png" alt="Pasted image 20240212143950.png" width="150" /> <img src="/personal_homepage/docs/attachment/Pasted%20image%2020240212152018.png" alt="Pasted image 20240212152018.png" width="130" /></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>shader:
    <ul>
      <li>用Refraction和Glossy混合</li>
      <li>用<a href="https://docs.blender.org/manual/en/4.0/render/shader_nodes/input/fresnel.html">Fresnel node</a> 调节折射和反射的比例</li>
      <li>设置IOR(折射率)为1.52</li>
      <li>设置粗糙度为0</li>
    </ul>
  </li>
  <li>render:
    <ul>
      <li>用cycles, 把light Paths调成Full Global Illumation: 即所有反射次数为32</li>
    </ul>
  </li>
  <li>结果:
    <ul>
      <li><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240212152142.png" alt="Pasted image 20240212152142.png" width="400" /></li>
    </ul>
  </li>
</ul>

<h2 id="用到的blender的一些快捷键功能">用到的blender的一些快捷键功能</h2>

<ul>
  <li>G/R/S (移动/旋转/放缩) 按了之后会提示方向快捷键</li>
  <li>E (<a href="https://docs.blender.org/manual/en/2.80/modeling/meshes/editing/duplicating/extrude.html">Extrude</a>, 挤出): 保持原顶点不变, 复制一份, 复制的和原来的用edge连接
    <ul>
      <li>selected face: <img src="https://docs.blender.org/manual/en/2.80/_images/modeling_meshes_editing_duplicating_extrude_face-before.png" alt="drawing" width="150" />  During extrude:  <img src="https://docs.blender.org/manual/en/2.80/_images/modeling_meshes_editing_duplicating_extrude_face-after.png" alt="drawing" width="150" />  </li>
    </ul>
  </li>
  <li><a href="https://docs.blender.org/manual/en/latest/modeling/meshes/tools/loop.html">Loop Cut</a>(卡线): 在edge中间添新的顶点
    <ul>
      <li>添加loop cut: Ctrl+R</li>
      <li>选中已有的loop cut: Shift+Option+长按</li>
    </ul>
  </li>
  <li>Ctrl+B( <a href="https://docs.blender.org/manual/en/2.81/modeling/meshes/editing/subdividing/bevel.html#:~:text=The%20Bevel%20tool%20smooths%20the,above%20to%20run%20the%20tool.">Bevel</a>, 拉伸, 倒角): 把一个edge变成多个edge, 使物体边缘光滑
    <ul>
      <li><img src="https://docs.blender.org/manual/zh-hans/2.81/_images/modeling_meshes_editing_subdividing_bevel_example-4.png" alt="drawing" width="150" /></li>
      <li>按S可以改变段数</li>
    </ul>
  </li>
  <li>Edit Mode 1/2/3 (选vertex, edge, face)</li>
</ul>

<h2 id="frensel反射系数">Frensel反射系数</h2>

<p>Frensel node确定折射和反射比例, 从blender的参数看, 受IOR(折射率)和平面法向的影响</p>

<ul>
  <li>推导:
    <ul>
      <li>根据<a href="https://en.wikipedia.org/wiki/Fresnel_equations">Fresnel equation</a> 的dielectric(绝缘体)的版本: $R=0.5*(r_{\perp}^2+r_{\parallel}^2)$
        <ul>
          <li>其中: $r_{\perp}=\frac{n_i\cos\theta_i-n_t\cos\theta_t}{n_i\cos\theta_i+n_t\cos\theta_t}$  , $r_{\parallel}=\frac{n_t\cos\theta_i-n_i\cos\theta_t}{n_t\cos\theta_i+n_i\cos\theta_t}$
            <ul>
              <li>$n_i$ 是入射介质(即空气=1)的折射率</li>
              <li>$n_t$ 是传播介质(即玻璃=1.52)的折射率</li>
              <li>$\cos\theta_i=(n\cdot w_o)$ 表示平面法向和观察方向的夹角：观察方向越平行于平面，$\theta_i$ 越大，折射越少, 反射越强
                <ul>
                  <li>pathtracing的方法里, 观察方向就是入射方向</li>
                </ul>
              </li>
              <li>$\cos\theta_t$ 表示平面法向和传播方向的夹角, 根据<a href="https://en.wikipedia.org/wiki/Snell%27s_law">Snell’s law</a>: $\frac{\sin\theta_t}{\sin\theta_i}=\frac{n_i}{n_t}=\frac{1}{\eta}$
                <ul>
                  <li>$\eta=n_t/n_i$ : ratio of IOR, 正面是$n$, 反面是$1/n$</li>
                  <li>则$\cos\theta_t=\sqrt{1-\sin^2\theta_t}=\sqrt{1-\frac{1}{\eta^2}(1-\cos^2\theta_i)}=\frac{1}{\eta}\sqrt{\eta^2-1+\cos^2\theta_i}$</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>代码中的表达方式:
  \(\begin{align}R &amp;=0.5*((\frac{n_i\cos\theta_i-n_t\cos\theta_t}{n_i\cos\theta_i+n_t\cos\theta_t})^2+(\frac{n_t\cos\theta_i-n_i\cos\theta_t}{n_t\cos\theta_i+n_i\cos\theta_t})^2)\\\\ &amp;=0.5*((\frac{\cos\theta_i-\eta\cos\theta_t}{\cos\theta_i+\eta\cos\theta_t})^2+(\frac{\eta\cos\theta_i-\cos\theta_t}{\eta\cos\theta_i+\cos\theta_t})^2)\; (\eta=n_t/n_i\text{是ratio of IOR, 正面是n, 反面是1/n)}\\\\ &amp;=0.5*((\frac{c-g}{c+g})^2+(\frac{\eta^2c-g}{\eta^2c+g})^2)\;\; (\text{让 }c=\cos\theta_i,\  g=\sqrt{\eta^2-1+\cos^2\theta}=\eta\cos\theta_t, \text{ 这步已经足够化简了, 为什么要把}\eta\text{完全替换掉呢?})\\\\ &amp;=0.5*((\frac{c-g}{c+g})^2+(\frac{(1+g^2-c^2)c-g}{(1+g^2-c^2)c+g})^2)\;\; (\text{根据 }\eta^2=1+g^2-c^2)\\\\  &amp;=0.5*((\frac{c-g}{c+g})^2+(\frac{c-g}{c+g})^2(\frac{1-c(c+g)}{1+c(g-c)})^2)\\\\  &amp;=0.5*(A^2+A^2B^2)=0.5*A^2(1+B^2)\;\; (\text{让 }A=\frac{g-c}{g+c}, B=\frac{c(g+c)-1}{c(g-c)+1})\end{align}\)</li>
    </ul>
  </li>
  <li>blender代码:
    <ul>
      <li><a href="https://projects.blender.org/blender/cycles/src/branch/main/src/kernel/osl/shaders/node_fresnel.osl">cycles/node_fresnel.osl at main</a></li>
      <li><a href="https://projects.blender.org/blender/cycles/src/branch/main/src/kernel/osl/shaders/node_fresnel.h">cycles/node_fresnel.h at main</a></li>
    </ul>
  </li>
</ul>

<h2 id="reference">reference</h2>
<p><span id="ref"></span></p>

<p>[1] https://seblagarde.wordpress.com/2013/04/29/memo-on-fresnel-equations/ 一些版本的公式推导和reference</p>

<p>[2] https://blenderartists.org/t/whats-the-math-behind-fresnel-node/1502809/11 Fensel node讨论</p>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;title&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;icon&quot;=&gt;nil}]}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="blender" /><category term="content" /><category term="shader" /><category term="3d_model" /><category term="shortcut" /><summary type="html"><![CDATA[由cylinder变形得到水杯. 用cycles渲染玻璃材质. 步骤]]></summary></entry><entry><title type="html">stable-diffusion的用法: 用 lora+controlnet做风格转换</title><link href="https://roshameow.github.io//personal_homepage/docs/photo/stable-diffusion2/" rel="alternate" type="text/html" title="stable-diffusion的用法: 用 lora+controlnet做风格转换" /><published>2024-02-09T00:00:00+00:00</published><updated>2024-02-24T21:25:27+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/photo/stable-diffusion2</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/photo/stable-diffusion2/"><![CDATA[<p>尝试只用ipadapter做猫的风格转换, 非常不成功, ipadapter还是无法控制输入的元素. 必须要用多张图片训练的LoRA控制. 
另外, 猫脸上颜色分布的特征, 没有controlnet可以直接表示.</p>

<h2 id="comfyui步骤">ComfyUI步骤</h2>

<ol>
  <li><strong>训练Lora</strong>
    <ul>
      <li>用<a href="https://github.com/bmaltais/kohya_ss">kohya_ss</a> 的gui训练
        <ul>
          <li>network rank设置64</li>
          <li>打开Gradient checkpoint, 不然我16G的显存不够用</li>
          <li>另外, 我的wsl2需要解决一下找不到cuda toolkit的问题
            <ul>
              <li>检查发现是ubuntu的requirement里面指定的torch和bitsandbytes版本不匹配</li>
              <li>在虚拟环境里, 升级到最新版2.2后代码又出问题</li>
              <li>最后参考windows版本的requirement.txt, 改成torch=2.1.0+cu118解决了</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>连上Lora节点</strong>
    <ul>
      <li>ComfyUI里连接顺序是: Checkpoint: model, clip -&gt; Lora</li>
      <li>多个Lora顺序连接就行</li>
      <li>ip的lora的weigt设为1.22, 风格化lora的weight不用设那么大</li>
    </ul>
  </li>
  <li><strong>写text prompt</strong>
    <ul>
      <li>写了包括描述内容的, lora配套的, 描述想要风格的positive prompt</li>
    </ul>
  </li>
  <li><strong>连上Controlnet节点</strong>
    <ul>
      <li>用到了<a href="https://huggingface.co/collections/diffusers/sdxl-controlnets-64f9c35846f3f06f5abe351f">canny模型, depth模型</a>
        <ul>
          <li>下载<a href="https://github.com/kijai/ComfyUI-Marigold">Marigold</a>的深度识别模型节点</li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<h2 id="结果">结果</h2>

<p><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240209124721.png" alt="Pasted image 20240209124721.png" width="150" /> -&gt; <img src="/personal_homepage/docs/attachment/Pasted%20image%2020240209124343.png" alt="Pasted image 20240209124343.png" width="600" /></p>

<ul>
  <li><strong>风格:</strong> 3种方法都可以
    <ol>
      <li>text prompt里输入</li>
      <li>风格LoRA
        <ul>
          <li>在prompt也要加上对应风格关键词</li>
        </ul>
      </li>
      <li>用ipadpter输入风格图片控制
        <ul>
          <li>ipadpter的weight需要调整: 不能太低(风格化没用), 也不能太高(和风格图太像)</li>
          <li>一直有和风格图太像, 或者风格图提供我们不想要元素的风险…毕竟一张图片的信息不像文字和lora那样有明确的指向性</li>
          <li>我用的不是换脸模型, 但是模型好像特别执着于换脸? 的确模型本来的目的是换ip…</li>
        </ul>
      </li>
      <li>另外, 可以直接用一个特定风格的大模型</li>
    </ol>
  </li>
  <li><strong>控制:</strong>
    <ul>
      <li>用canny或depth模型都能做到控制猫的位置
        <ul>
          <li>Marigold的识别特别慢, 尝试的时候计算完深度图要先把那个节点断开</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>问题:</strong>
    <ul>
      <li>有些风格单独可以出图, 但是加了我训练的猫的LoRA就没法出对应风格的图了</li>
      <li>用LoRA训练的模型没有完全理解我的猫的所有特征: 尤其是鼻子和嘴附近的毛色
        <ul>
          <li>我用了30多张照片, 也没有改label的caption, 所以可能还有质量增加的空间</li>
        </ul>
      </li>
      <li>用ipadpter的失败经验:
        <ul>
          <li>如果用一张猫的图片和一张风格图
            <ul>
              <li><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240209133511.png" alt="Pasted image 20240209133511.png" width="100" /></li>
              <li>无法控制输入元素</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="comfyui-workflow">ComfyUI workflow</h2>

<p>用Lora调整风格$\downarrow$ : <a href="https://gist.github.com/roshameow/d952bf0157a25ab3e9e724df1449b160#file-rosha_lora_controlnet_canny_depth_style_change-json"><strong>rosha_lora_controlnet_canny_depth_style_change.json</strong></a></p>

<p><img src="/personal_homepage/docs/attachment/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202024-02-08%20230728.png" alt="屏幕截图 2024-02-08 230728.png" width="800" /></p>

<p>用ipadator调整风格$\downarrow$ : <a href="https://gist.github.com/roshameow/d952bf0157a25ab3e9e724df1449b160#file-rosha_lora_ipadapter_style_tranformation-json"><strong>rosha_lora_ipadapter_style_tranformation.json</strong></a></p>

<p><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240209135354.png" alt="Pasted image 20240209135354.png" width="800" /></p>

<h2 id="资源">资源</h2>

<p>艺术风格prompt:</p>

<p>[1] https://medium.com/phygital/top-40-useful-prompts-for-stable-diffusion-xl-008c03dd0557</p>

<p>[2] https://docs.midjourney.com/docs/explore-prompting</p>

<p>[3] https://www.urania.ai/top-sd-artists</p>

<p>[4] https://github.com/SupaGruen/StableDiffusion-CheatSheet 是v1.5插件</p>

<p>Lora训练:</p>

<p>[5]  <a href="https://www.youtube.com/watch?v=NaHtI0u3Am4">https://www.youtube.com/watch?v=NaHtI0u3Am4</a> kohya_ss的参数设置,</p>

<p>[6] https://zhuanlan.zhihu.com/p/654322145.   下面讨论说optimizer还是默认比较好?</p>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;title&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;icon&quot;=&gt;nil}]}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="photo" /><category term="content" /><category term="ComfyUI" /><category term="ipadapter" /><category term="controlnet" /><category term="canny" /><category term="sdxl" /><summary type="html"><![CDATA[尝试只用ipadapter做猫的风格转换, 非常不成功, ipadapter还是无法控制输入的元素. 必须要用多张图片训练的LoRA控制. 另外, 猫脸上颜色分布的特征, 没有controlnet可以直接表示.]]></summary></entry><entry><title type="html">blender学习: 做火焰效果</title><link href="https://roshameow.github.io//personal_homepage/docs/blender/blender-learning5/" rel="alternate" type="text/html" title="blender学习: 做火焰效果" /><published>2024-02-08T00:00:00+00:00</published><updated>2024-02-12T03:33:52+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/blender/blender-learning5</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/blender/blender-learning5/"><![CDATA[<p>火焰的形状用圆形变形一半获得. 用<a href="https://en.wikipedia.org/wiki/Perlin_noise">Perlin noise</a>使坐标变形制作随机抖动. 用Emission node渲染发光效果.</p>

<h2 id="用graph-editor和noise-texture---飞行器火焰">用graph editor和noise texture -&gt; 飞行器火焰</h2>
<h3 id="步骤">步骤</h3>

<p>参考RuiHuang_art在<a href="https://www.bilibili.com/video/BV1HH4y1Y7Vv/">b站的教学视频</a> , 和做车流用到的功能差不多</p>
<ul>
  <li>world property: 加入一个背景的贴图</li>
  <li>制做一个平面的火焰:
    <ul>
      <li>添加一个Plane mesh</li>
      <li>分成两半: 在Edit编辑模式下, 按Ctrl+R加<a href="https://docs.blender.org/manual/en/latest/modeling/meshes/tools/loop.html">loop cut</a> , 在edge中间添加新的顶点,</li>
      <li>把一半拉长: 选中顶点后按G+Y在Y轴拉伸</li>
    </ul>
  </li>
  <li>shader: 目的是制作一个自发光, 半透明, 抖动的效果
    <ul>
      <li>颜色效果:
        <ol>
          <li>用<a href="https://docs.blender.org/manual/en/latest/render/shader_nodes/input/texture_coordinate.html">texture Coordinate</a> 生成uv坐标
            <ul>
              <li>在mapping里把x,y的location调到-2, scale调到4</li>
              <li>uv坐标系好像范围是(-scale/2, scale/2)</li>
            </ul>
          </li>
          <li>用<a href="https://docs.blender.org/manual/en/latest/render/shader_nodes/textures/gradient.html">Gradient Texture</a> 的spherical: 制作从外到内的渐变</li>
          <li>自发光: 用<a href="https://docs.blender.org/manual/en/latest/editors/texture_node/types/converter/color_ramp.html">Color Ramp</a> 做一个 蓝-&gt;白-&gt;红-&gt;黄 的渐变</li>
          <li>调整透明度: 用<a href="https://docs.blender.org/manual/en/latest/editors/texture_node/types/converter/color_ramp.html">Color Ramp</a> 做一个 透明-&gt;半透明-&gt;透明 的渐变</li>
          <li>连接bsdf
            <ul>
              <li>把Emission和transparent bsdf用Mix shader连接在一起</li>
            </ul>
          </li>
        </ol>
      </li>
      <li>抖动效果:
        <ol>
          <li>用<a href="https://docs.blender.org/manual/en/latest/render/shader_nodes/input/texture_coordinate.html">texture Coordinate</a> 生成object坐标</li>
          <li>用<a href="https://docs.blender.org/manual/en/latest/render/shader_nodes/textures/noise.html">Noise Texture</a> 扭曲坐标, 制作抖动效果: noise texture和displacement的cloudy texture效果差不多
            <ul>
              <li>用<a href="https://docs.blender.org/manual/en/latest/compositing/types/color/mix/mix_color.html">Mix Node</a> 的Multiply把两个坐标混合</li>
            </ul>
          </li>
          <li>用<a href="https://docs.blender.org/manual/zh-hans/dev/editors/graph_editor/fcurves/editing.html">Graph Editor</a> 调整mapping的y坐标的location, 形成动态的扭曲</li>
        </ol>
      </li>
    </ul>
  </li>
  <li>把火焰的plane mesh旋转复制出多个绕y轴一圈
    <ul>
      <li>在shader里选中一个materail<a href="https://blender.stackexchange.com/questions/7044/copy-material-to-another-object">复制给所有mesh</a></li>
      <li>复制完想修改所有的mesh形状但是没找到方法</li>
    </ul>
  </li>
  <li>结果: 我复制的次数比较少, 而且颜色也不好看,  形状也拉的不太对
    <ul>
      <li><img src="/personal_homepage/docs/attachment/fire.mp4" alt="fire.mp4" width="320" /></li>
    </ul>
  </li>
</ul>

<h3 id="shader流程">shader流程</h3>

<p><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240208164250.png" alt="Pasted image 20240208164250.png" width="600" /></p>

<h2 id="用displacement粒子系统---卡通火焰">用displacement+粒子系统 -&gt; 卡通火焰</h2>
<h3 id="步骤-1">步骤:</h3>

<p>参考小红书<a href="https://www.xiaohongshu.com/explore/658636a9000000003c010c5d">KurTips的教学</a></p>
<ul>
  <li>制作火焰形状
    <ul>
      <li>添加一个cube mesh</li>
      <li>在modifier里加subdivision surface</li>
      <li>在Edit Mode, 打开Propotional Editing(衰减编辑), 选择最上面vertex, 按G+V可以在z轴拉伸</li>
      <li>制作焰心: 复制一个更小的放在前面</li>
      <li>选中两个object, 右键<a href="https://docs.blender.org/manual/en/latest/scene_layout/object/editing/shading.html">shade smooth</a>
        <ul>
          <li>shader计算时, 三角形内的normal vector由三个顶点的normal vector插值得到</li>
        </ul>
      </li>
      <li>合并两个object: <a href="https://docs.blender.org/manual/en/4.2/scene_layout/object/editing/join.html">Ctrl+J</a></li>
    </ul>
  </li>
  <li>shader: 设置<a href="https://docs.blender.org/manual/en/latest/render/shader_nodes/input/texture_coordinate.html">texture Coordinate</a>  generate-&gt;Separate XYZ-&gt;Gradient Texture-&gt;ColorRamp-&gt; Emission
    <ul>
      <li>把大小两块用不同的材质
        <ul>
          <li>进入Edit Mode, 随便选中一个点, Ctrl+L可以把原本的大小两个object分别选中</li>
          <li>在Material添加材质</li>
        </ul>
      </li>
      <li>ColorRamp颜色: 外焰: 红-&gt;黄 ; 焰心: 红-&gt;橙红</li>
    </ul>
  </li>
  <li>抖动效果: 用displacement实现
    <ol>
      <li>在Modifier添加displace: 选Clouds材质, 调整Size, Depth=1, displacement的Stength</li>
      <li>让displacement的强度从上到下渐变: 用顶点组实现
        <ol>
          <li>定义顶点组: 在Edit Mode开透视模式, 选中上面一部分顶点; 在Oject的Data里添加vetex group, 把刚才选中的部分顶点assign给这个顶点组</li>
          <li>在<a href="https://docs.blender.org/manual/en/latest/sculpt_paint/weight_paint/index.html">Weight Paint</a>模式, 增加weight-&gt;<a href="https://docs.blender.org/manual/en/2.79/sculpt_paint/painting/weight_paint/weight_tools.html">smooth-&gt;iteration</a>: 让 in Vertex Group-&gt; outside Vertex Group 的渐变更光滑
            <ul>
              <li>smooth操作的每个iteration都是和邻域的顶点做平滑</li>
            </ul>
          </li>
          <li>在displace的vertex group选择顶点组</li>
        </ol>
      </li>
      <li>制作displacement的动态: 通过绑定一个空坐标轴实现
        <ol>
          <li>在displace的Coordinates选择object, 物体选择我们的空坐标轴</li>
          <li>修改空坐标轴的Oject-&gt;Transform的z轴的值, 改为: <code class="language-plaintext highlighter-rouge">#frame/100(帧数/100)</code>
            <ul>
              <li>这样做比用<a href="https://docs.blender.org/manual/zh-hans/dev/editors/graph_editor/fcurves/editing.html">Graph Editor</a> 方便不少👍</li>
            </ul>
          </li>
        </ol>
      </li>
    </ol>
  </li>
  <li>制作火星: 用粒子系统
    <ol>
      <li>在火焰里添加一个圆环mesh</li>
      <li>进入Edit Mode, 填充: 快捷键F</li>
      <li>修改Particles参数:
        <ul>
          <li>Field Weight-&gt; Gravity改为0: 让粒子向上</li>
          <li>Velocity-&gt; normal改为2</li>
          <li>Emission: 调整Frame_start, Frame_end, lifetime(和velocity共同控制粒子移动的距离)</li>
          <li>Physics-&gt; Brownian改为10.5: 让粒子做布朗运动</li>
        </ul>
      </li>
      <li>调整火星材质:
        <ol>
          <li>制作一个多面体mesh, subdivision改成1: 减小计算量</li>
          <li>把多面体的shader改成自发光, 调整color, strength适当增大</li>
          <li><strong>打开eevee渲染器的<a href="https://docs.blender.org/manual/en/latest/render/eevee/render_settings/bloom.html">bloom(辉光)</a> 功能</strong></li>
          <li>制定Particles-&gt;Render参数:
            <ol>
              <li>Render As调成Object模式, Render-&gt;Instance选择我们的制作的多面体mesh</li>
              <li>scale调小, scale Randomness调大: 让粒子大小不一</li>
            </ol>
          </li>
        </ol>
      </li>
    </ol>
  </li>
  <li>结果: 因为焰心的制作方法, 只能在一定角度看.
    <ul>
      <li><img src="/personal_homepage/docs/attachment/fire_cartoon_cut.mp4" alt="fire_cartoon_cut.mp4" width="200" /></li>
    </ul>
  </li>
</ul>

<h2 id="火焰效果的片段">火焰效果的片段</h2>

<p>[1] 冰海战记s2ep5的放火场景</p>

<h2 id="reference">reference</h2>

<p>[1] https://zhuanlan.zhihu.com/p/484222862 辉光效果</p>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;title&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;icon&quot;=&gt;nil}]}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="blender" /><category term="content" /><category term="render" /><category term="shader" /><summary type="html"><![CDATA[火焰的形状用圆形变形一半获得. 用Perlin noise使坐标变形制作随机抖动. 用Emission node渲染发光效果.]]></summary></entry><entry><title type="html">stable-diffusion的用法: 用 ipadatper+controlnet canny做风格转换</title><link href="https://roshameow.github.io//personal_homepage/docs/photo/stable-diffusion1/" rel="alternate" type="text/html" title="stable-diffusion的用法: 用 ipadatper+controlnet canny做风格转换" /><published>2024-02-07T00:00:00+00:00</published><updated>2024-02-24T19:16:16+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/photo/stable-diffusion1</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/photo/stable-diffusion1/"><![CDATA[<p>用ipadapter和canny做风格转换</p>

<h2 id="comfyui使用">ComfyUI使用</h2>

<ol>
  <li>调用ComfyUI中default的工作流
    <ul>
      <li>下载<a href="https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0">sdxl模型</a></li>
    </ul>
  </li>
  <li><strong>连上IPAdapter节点</strong>
    <ul>
      <li>安装<a href="https://github.com/cubiq/ComfyUI_IPAdapter_plus">IPAdapter_plus插件</a>
        <ul>
          <li>下载需要的ipadater微调模型和clip vision编码器: 把图像用clip vision编码后输入base model的cross-attention层</li>
        </ul>
      </li>
      <li>ComfyUI里连接顺序是: Checkpoint: model-&gt; IPAdapter: model -&gt; KSampler: model</li>
      <li>weigt设为1, text prompt都空着, latent prompt也空着</li>
    </ul>
  </li>
  <li><strong>连上Controlnet节点</strong>
    <ul>
      <li>下载<a href="https://huggingface.co/collections/diffusers/sdxl-controlnets-64f9c35846f3f06f5abe351f">canny模型</a>
        <ul>
          <li><del>找不到其他适合sdxl的controlnet模型</del>因为我们的原图是个线稿适合用canny</li>
        </ul>
      </li>
      <li>ComfyUI里连接顺序是: Checkpoint-&gt;prompt-&gt;controlnet-&gt;Ksampler: text prompt
        <ul>
          <li>controlnet最后是作用在condition(positive, negative)上的, 和text promt一样</li>
        </ul>
      </li>
      <li>调整controlnet权重和影响步数: stength=0.5, start_percent=0, end_percent=0.6
        <ul>
          <li>如果设置过大就只能得到一张有颜色的线稿图</li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<h2 id="结果">结果</h2>

<p>原图: 
<img src="/personal_homepage/docs/attachment/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202024-02-07%20224110.png" alt="屏幕截图 2024-02-07 224110.png" width="600" /> + <img src="/personal_homepage/docs/attachment/2e2b76d160ca4317bb8e095ffd8ff878.jpg.png" alt="2e2b76d160ca4317bb8e095ffd8ff878.jpg.png" width="150" /></p>

<p>合成图: 
<img src="/personal_homepage/docs/attachment/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202024-02-07%20224141.png" alt="屏幕截图 2024-02-07 224141.png" width="600" /></p>

<ul>
  <li>竟然可以识别龙的眼睛啊…有好几幅图都把猫眼替换到了龙眼上
    <ul>
      <li>如果两种图有同样的元素(眼睛之类的), 可能可以比较好的替换</li>
      <li>最后的龙和人, 龙和山, 都不能对应起来</li>
    </ul>
  </li>
  <li>ipadapter没法主动控制合成哪个图像中的哪种元素:
    <ul>
      <li>虽然替换了颜色, 背景, 但是没法替换材质, 笔触</li>
      <li>有几张图还替换了原图的色块分布, 这是我们不想要的</li>
    </ul>
  </li>
  <li>另外, controlnet-canny和ipadapter的龙都不能省略
    <ul>
      <li>如果ipadapter只用风格图:
        <ul>
          <li><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240208003655.png" alt="Pasted image 20240208003655.png" width="100" /> <img src="/personal_homepage/docs/attachment/Pasted%20image%2020240208003712.png" alt="Pasted image 20240208003712.png" width="100" /></li>
          <li>增加control-net的权重也是没用的, weight=0.8时出现了噪声图案</li>
        </ul>
      </li>
      <li>如果不加controlnet:
        <ul>
          <li><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240208003804.png" alt="Pasted image 20240208003804.png" width="100" /> <img src="/personal_homepage/docs/attachment/Pasted%20image%2020240208003819.png" alt="Pasted image 20240208003819.png" width="100" /></li>
        </ul>
      </li>
      <li>看起来controlnet保持了龙的外形结构, ipadpter加龙提高了龙的权重, 不过ipadapter从龙图中究竟学到了什么? 是扁平的画风吗? 只用风格图的情况下, 确实画风更接近风格图, 但是内容也更接近了
        <ul>
          <li>可以通过text prompt控制吗? 但是clip又无法理解简笔画的龙.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="comfyui-workflow">ComfyUI workflow</h2>

<p><a href="https://gist.github.com/roshameow/d952bf0157a25ab3e9e724df1449b160#file-ipadapter_controlnet-canny_loong_style_change-json"><strong>ipadapter_controlnet-canny_loong_style_change.json</strong></a></p>

<p><img src="/personal_homepage/docs/attachment/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202024-02-07%20224530.png" alt="屏幕截图 2024-02-07 224530.png" width="800" /></p>

<h2 id="comfyui难用的地方">ComfyUI难用的地方</h2>

<ul>
  <li>UI的奇怪地方
    <ul>
      <li>有参数的地方字体相对整个节点特别小, 而且没法调节. 输入text prompt的时候是根本看不清..</li>
      <li>鼠标的十字倒是特别大…</li>
    </ul>
  </li>
  <li>节点连接非常反常识
    <ul>
      <li>load节点
        <ul>
          <li>输入都在右侧, 一般会觉得应该把prompt输入模型</li>
          <li>load节点的存在就很奇怪, 为什么不把load和apply合并到一起? 或者在右侧用菜单栏切换</li>
          <li>model里面还有个model_name? 模型的名字一长全混在一起真的分不清啊…</li>
        </ul>
      </li>
      <li>preview Image节点
        <ul>
          <li>为什么不直接做成一个下拉选项…</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>节点划分不明确: ComfyUI里面一个节点会用dict的形式传递多种参数
    <ul>
      <li>latent里面可能包含latent image, noise mask, 也可能不包含noise mask, 此时连接到需要noise mask key的latent节点就会报错</li>
      <li>condition(positive, negative)也包含多种condition…</li>
    </ul>
  </li>
  <li>为什么不能做成一个Chromium内核的本地app..
    <ul>
      <li>据说有一些方法可以打包成一个app?</li>
      <li>凡需要inpaint输入的地方, 输入方式很不方便, 能和其他专门用来绘图的软件结合就好了</li>
    </ul>
  </li>
  <li>虽然如此, 可以用节点和分享工作流就比webui好用太多了</li>
</ul>

<h2 id="其他讨论这个链接">其他讨论这个链接</h2>

<p>[1] https://zhuanlan.zhihu.com/p/664201523 里面介绍了一个换画风的工作流, 不过例子是人像, 可能本身网络对人像的理解能力更强吧?　</p>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;title&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;icon&quot;=&gt;nil}]}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="photo" /><category term="content" /><category term="ComfyUI" /><category term="ipadapter" /><category term="controlnet" /><category term="canny" /><category term="sdxl" /><summary type="html"><![CDATA[用ipadapter和canny做风格转换]]></summary></entry><entry><title type="html">神经网络attention结构理解</title><link href="https://roshameow.github.io//personal_homepage/docs/deeplearning/attention/" rel="alternate" type="text/html" title="神经网络attention结构理解" /><published>2024-02-04T00:00:00+00:00</published><updated>2024-02-28T00:20:55+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/deeplearning/attention</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/deeplearning/attention/"><![CDATA[<p>在网络中, block是把input信息转换成output信息的过程: 一般, output(position, vector)是input(token, embedding vector)的线性组合, 组合的weight (position, token) 由input和output两方关系确定. 把着重强调这种信息交互的模块叫attention.</p>
<h2 id="convolution">convolution</h2>

<p><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240207151034.png" alt="Pasted image 20240207151034.png" width="400" /></p>

<p>convolution在神经网络流行之前就已经在图像任务里广泛使用了</p>
<ul>
  <li><strong>目的:</strong>
    <ul>
      <li>对spatial information进行特征的提取和转换</li>
    </ul>
  </li>
  <li><strong>特点:</strong>
    <ul>
      <li>pixel的weight只和(input, output)的相对位置有关, 因此也是平移不变的. 对每个输出像素有影响的只有input里kernel覆盖到的区域, 也就是response field</li>
      <li>每个不同的相对位置对应vector mapping不同
        <ul>
          <li>这个符合图像处理的直观, 左边有条线和右边有条线当然要映射成不同的结果</li>
          <li>有时也会把卷积拆分成1x1 conv和spatial conv(通道无关) 的形式</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="gate-attention">gate attention</h2>

<p><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240207151122.png" alt="Pasted image 20240207151122.png" width="400" /> <img src="/personal_homepage/docs/attachment/Pasted%20image%2020240207151136.png" alt="Pasted image 20240207151136.png" width="400" /></p>

<ul>
  <li><strong>目的:</strong>
    <ul>
      <li>提取channel<a href="#ref">1</a>或spatial的权重, 让网络关注更重要的信息</li>
    </ul>
  </li>
  <li><strong>特点:</strong>
    <ul>
      <li>spatial 信息对人类来说更有可读性, 所以可以把spatial weight可视化, 看看图片什么位置更加重要</li>
    </ul>
  </li>
</ul>

<h2 id="self-attention--cross-attention">self-attention &amp; cross-attention</h2>

<p><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240207151214.png" alt="Pasted image 20240207151214.png" width="400" /></p>

<ul>
  <li><strong>目的:</strong>
    <ul>
      <li>信息的交互: 在我的图示中, 是把文字信息加入视觉信息</li>
    </ul>
  </li>
  <li><strong>特点:</strong>
    <ul>
      <li>self-attention和cross-attention结构类似, 只是变成了一种输入</li>
      <li>cross-attention是现在常见的把一种信息加入另一种信息的方法
        <ul>
          <li>也用到了图像信息和文字信息的统一形式, 即(position, embedding) , 在图像信息中, position(S=H x W)是像素或patch的位置; 文字信息中, position是token在句中的前后位置</li>
        </ul>
      </li>
      <li>relative weight:
        <ul>
          <li>我们可以对relative weight可视化, 从而知道某个文字token和图像哪个位置最相关</li>
          <li>对relative weight的一个维度做average pooling, 就能知道哪个位置是更重要的<a href="#ref">4</a>(和gate-attention的用法一样)</li>
        </ul>
      </li>
      <li>Q,K,V(query, key, value) 的叫法是nlp搜索(匹配)任务的术语, 额, 其实我一直没法对这个望文生义…
        <ul>
          <li>其实从计算过程可以看出, 不管它们的原义, Q, K交换一下也没差</li>
          <li>query和key的embedding channel数应该相同, 为了之后计算他们relation的目的</li>
          <li>和卷积不一样, 放了更大的计算量在交互部分, 相对的, 每个input token对应vector mapping相同, 也就是用”value”部分解决了channel的映射: 这样导致同样的文字信息不论放在前面还是后面, 对应同一种output信息, 区别的只是他们权重可能不同, 或在图像的不同位置</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="attention的一些变形和优化">attention的一些变形和优化</h3>

<ul>
  <li>mulit-head attention: 映射多组QKV, 计算得到的多组output再concat
    <ul>
      <li>在实际实现过程中,  先映射成一个大的QKV, 再把Q, K, V切分是完全等价的</li>
      <li>做mulit-head可以得到多组不同的映射方式, 或减小inner_dimension(T)的大小</li>
    </ul>
  </li>
  <li>分层attention: 在做attention之前对数据排序, 切分
    <ul>
      <li>可能有些pixel之间关联不大, 这样的话把它们分到不同的bucket, 分别做attention, 可以减少计算量<a href="#ref">2</a></li>
    </ul>
  </li>
  <li>多个attention合成:
    <ul>
      <li>在最近流行的ipadapter里面也讨论了多种不同来源的数据在cross-attention 里合成的问题, 结论是分别与原数据做attention最后再合成比较好<a href="#ref">3</a>, 这个也是我们一般naive的想法</li>
    </ul>
  </li>
</ul>

<h2 id="光流估计网络">光流估计网络</h2>

<p>视觉匹配任务</p>

<h2 id="代码">代码</h2>

<p><a href="https://gist.github.com/roshameow/503ec3769d75c47b82f2a7372e8c2dab#file-attention_block-py"><strong>attention_block.py</strong></a></p>

<h2 id="reference">reference</h2>
<p><span id="ref"></span></p>

<p>[1] Woo, Sanghyun, Jongchan Park, Joon-Young Lee, and In So Kweon. “CBAM: Convolutional Block Attention Module.” arXiv, July 18, 2018. <a href="https://doi.org/10.48550/arXiv.1807.06521">https://doi.org/10.48550/arXiv.1807.06521</a>. 介绍gate attention</p>

<p>[2] Cai, Yuanhao, Jing Lin, Xiaowan Hu, Haoqian Wang, Xin Yuan, Yulun Zhang, Radu Timofte, and Luc Van Gool. “Coarse-to-Fine Sparse Transformer for Hyperspectral Image Reconstruction.” arXiv, July 10, 2022. <a href="https://doi.org/10.48550/arXiv.2203.04845">https://doi.org/10.48550/arXiv.2203.04845</a>. 介绍了Spectra-aware hashing attention block的结构</p>

<p>[3] https://github.com/tencent-ailab/IP-Adapter</p>

<p>[4] Hong, Susung, Gyuseong Lee, Wooseok Jang, and Seungryong Kim. “Improving Sample Quality of Diffusion Models Using Self-Attention Guidance.” arXiv, August 24, 2023. <a href="https://doi.org/10.48550/arXiv.2210.00939">https://doi.org/10.48550/arXiv.2210.00939</a>.</p>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;title&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;icon&quot;=&gt;nil}]}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="deeplearning" /><category term="content" /><category term="network" /><category term="attention" /><category term="block" /><summary type="html"><![CDATA[在网络中, block是把input信息转换成output信息的过程: 一般, output(position, vector)是input(token, embedding vector)的线性组合, 组合的weight (position, token) 由input和output两方关系确定. 把着重强调这种信息交互的模块叫attention. convolution]]></summary></entry><entry><title type="html">常用的图像 reconstruction loss</title><link href="https://roshameow.github.io//personal_homepage/docs/deeplearning/restruction-loss/" rel="alternate" type="text/html" title="常用的图像 reconstruction loss" /><published>2024-02-01T00:00:00+00:00</published><updated>2024-02-26T19:22:46+00:00</updated><id>https://roshameow.github.io//personal_homepage/docs/deeplearning/restruction-loss</id><content type="html" xml:base="https://roshameow.github.io//personal_homepage/docs/deeplearning/restruction-loss/"><![CDATA[<p>输出为图像的任务, 比如enhance, deblur, super-resolution, generation等用到的loss, 主要分为以下两类</p>
<h2 id="output和label相近">output和label相近</h2>

<table>
  <thead>
    <tr>
      <th>loss</th>
      <th>公式</th>
      <th>目的</th>
      <th>特点<br /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>L1 loss</td>
      <td>$||I_1-I_2||_1$</td>
      <td>大体相近</td>
      <td>最常用的loss</td>
    </tr>
    <tr>
      <td>L2 loss<br />(MSE)</td>
      <td>$||I_1-I_2||_2$<br />或$MSE=\overline{(I_1-I_2)^2}$</td>
      <td> </td>
      <td>因为导数是线性所以计算最快</td>
    </tr>
    <tr>
      <td><a href="[https://github.com/CVMI-Lab/UHDM/blob/main/utils/common.py](https://github.com/CVMI-Lab/UHDM/blob/main/utils/common.py)">SSIM</a> <br />(stuctural similarity<br /> index measure)</td>
      <td>$\frac{2\mu_1\mu_2+C_1}{\mu_1^2+\mu_2^2+C_1}\cdot \frac{2\sigma_{12}+C_2}{\sigma_1^2+\sigma_2^2+C_2}$ <br />$\mu, \sigma$ 分别为mean, variance<br />$\sigma_{12}$是covariance<br />$C_1, C_2$ 是常数</td>
      <td>纹理相近</td>
      <td>要分patch计算<br />$C_1, C_2$ 的值要根据图像的范围调整</td>
    </tr>
    <tr>
      <td>PSNR<br />(Peak signal-<br />to-noise ratio)</td>
      <td>$-10\log_{10}(MSE(I_1,I_2))$</td>
      <td> </td>
      <td>经常是用来验证</td>
    </tr>
    <tr>
      <td><a href="[https://github.com/varun19299/deep-atrous-guided-filter/tree/master/PerceptualSimilarity](https://github.com/varun19299/deep-atrous-guided-filter/tree/master/PerceptualSimilarity)">PerceptualLoss</a></td>
      <td>一个分类网络</td>
      <td>语义相近</td>
      <td>一般用vgg16, 输入RGB图像<br />一般会用后几层的语义特征对比<br /></td>
    </tr>
    <tr>
      <td><a href="https://github.com/richzhang/PerceptualSimilarity">LPIPS</a> <br />(Learned Perceptual<br />image patch similarity)</td>
      <td> </td>
      <td> </td>
      <td>perceptualLoss+分块</td>
    </tr>
    <tr>
      <td>DeltaE</td>
      <td>转换成Lab空间的MSE</td>
      <td>颜色相近</td>
      <td> </td>
    </tr>
    <tr>
      <td>Gan Loss</td>
      <td>在训练过程中学习到的loss<br />用determinate网络表示<br /></td>
      <td>determinate网络<br />区别不出</td>
      <td> </td>
    </tr>
    <tr>
      <td>KL散度</td>
      <td> </td>
      <td> </td>
      <td> </td>
    </tr>
  </tbody>
</table>

<h2 id="output图像符合自然图像的性质image-prior">output图像符合自然图像的性质(image prior)</h2>

<table>
  <thead>
    <tr>
      <th>loss</th>
      <th>公式</th>
      <th>目的</th>
      <th>特点</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Total Variant(TV)</td>
      <td>anisotropic定义:<br />$||D_x I||_1+||D_y I||_1$</td>
      <td>图像gradient稀疏性</td>
      <td> </td>
    </tr>
    <tr>
      <td>FID<br />(Freachet Inception<br /> Distance)</td>
      <td> </td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>sFID</td>
      <td> </td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>IS<br />(Inception Score)</td>
      <td> </td>
      <td> </td>
      <td> </td>
    </tr>
  </tbody>
</table>

<h2 id="代码">代码</h2>

<p><a href="https://gist.github.com/roshameow/c59d5708610ae30eb4329b140ccab3a7#file-reconstruction_loss-py"><strong>reconstruction_loss.py</strong></a></p>]]></content><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;nil, &quot;email&quot;=&gt;&quot;w.liuatnk@gmail.com&quot;, &quot;twitter&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;title&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;icon&quot;=&gt;nil}]}</name><email>w.liuatnk@gmail.com</email></author><category term="docs" /><category term="deeplearning" /><category term="content" /><category term="loss" /><category term="image" /><summary type="html"><![CDATA[输出为图像的任务, 比如enhance, deblur, super-resolution, generation等用到的loss, 主要分为以下两类 output和label相近]]></summary></entry></feed>