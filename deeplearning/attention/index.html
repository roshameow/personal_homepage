<!DOCTYPE html>
<!--
    So Simple Jekyll Theme 3.2.0
    Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
    Free for personal and commercial use under the MIT license
    https://github.com/mmistakes/so-simple-theme/blob/master/LICENSE
-->
<html lang="en-US" class="no-js">
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  

  
  
  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>神经网络attention结构理解 | Liu, Wen’s Home Page</title>
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="神经网络attention结构理解" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="在网络中, block是把input信息转换成output信息的过程: 一般, output(position, vector)是input(token, embedding vector)的线性组合, 组合的weight (position, token) 由input和output两方关系确定. 把着重强调这种信息交互的模块叫attention. convolution" />
<meta property="og:description" content="在网络中, block是把input信息转换成output信息的过程: 一般, output(position, vector)是input(token, embedding vector)的线性组合, 组合的weight (position, token) 由input和output两方关系确定. 把着重强调这种信息交互的模块叫attention. convolution" />
<link rel="canonical" href="https://roshameow.github.io//personal_homepage/deeplearning/attention/" />
<meta property="og:url" content="https://roshameow.github.io//personal_homepage/deeplearning/attention/" />
<meta property="og:site_name" content="Liu, Wen’s Home Page" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-02-04T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="神经网络attention结构理解" />
<meta name="twitter:site" content="@" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-03-27T23:26:32+00:00","datePublished":"2024-02-04T00:00:00+00:00","description":"在网络中, block是把input信息转换成output信息的过程: 一般, output(position, vector)是input(token, embedding vector)的线性组合, 组合的weight (position, token) 由input和output两方关系确定. 把着重强调这种信息交互的模块叫attention. convolution","headline":"神经网络attention结构理解","mainEntityOfPage":{"@type":"WebPage","@id":"https://roshameow.github.io//personal_homepage/deeplearning/attention/"},"url":"https://roshameow.github.io//personal_homepage/deeplearning/attention/"}</script>
<!-- End Jekyll SEO tag -->


  

  <script>
    /* Cut the mustard */
    if ('querySelector' in document && 'addEventListener' in window) {
      document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + 'js';
    }
  </script>

  <link rel="stylesheet" href="/personal_homepage/assets/css/main.css">
  <link rel="stylesheet" href="/personal_homepage/assets/css/skins/default.css">
  
  <link rel="stylesheet"
    href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,400i,700,700i|Lora:400,400i,700,700i">
  <link rel="alternate" type="application/atom+xml" title="Liu, Wen&#39;s Home Page" href="/personal_homepage/atom.xml">
<!-- start custom head snippets -->

<!-- insert favicons. use http://realfavicongenerator.net/ -->
<link rel="shortcut icon" type="image/x-icon" href="/personal_homepage/docs/images/logo.ico">
<!-- end custom head snippets -->

</head>

  <body class="layout--post  神经网络attention结构理解">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#primary-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    
  <div class="navigation-wrapper">
    <a href="#menu-toggle" id="menu-toggle">Menu</a>
    <nav id="primary-nav" class="site-nav animated drop">
      <ul><li><a href="/personal_homepage/">Home</a></li><li><a href="/personal_homepage/posts/">Posts</a></li><li><a href="/personal_homepage/categories/">Categories</a></li><li><a href="/personal_homepage/tags/">Tags</a></li><li><a href="/personal_homepage/family/">Family</a></li><li><a href="/personal_homepage/search/">Search</a></li></ul>
    </nav>
  </div><!-- /.navigation-wrapper -->


    <header class="masthead">
  <div class="wrap">
    
    
    
      
        <div class="site-title animated fadeIn"><a href="/personal_homepage/">Liu, Wen's Home Page</a></div>
      
      <p class="site-description animated fadeIn" itemprop="description">Work, Experiments and Ideas.</p>
    
  </div>
</header><!-- /.masthead -->


    <main id="main" class="main-content" aria-label="Content">
  <article class="h-entry">
    

    <div class="page-wrapper">
      <header class="page-header">
        
        
        <h1 id="page-title" class="page-title p-name">神经网络attention结构理解
</h1>
        
      </header>

      <div class="page-sidebar">
        <div class="page-author h-card p-author"><img src="/personal_homepage/docs/images/logo.svg" class="author-avatar u-photo" alt=""><div class="author-info">

<span class="read-time">1 min read</span>

    <time class="page-date dt-published" datetime="2024-02-04T00:00:00+00:00"><a class="u-url" href="">February 4, 2024</a>
</time>

  </div>
</div>

        
  <h3 class="page-taxonomies-title">Categories</h3>
  
  <ul class="page-taxonomies"><li class="page-taxonomy"><a class="p-category" href="/personal_homepage/categories/#deeplearning" title="Pages filed under deeplearning">deeplearning</a></li>
  </ul>


        
  <h3 class="page-taxonomies-title">Tags</h3>
  
  <ul class="page-taxonomies"><li class="page-taxonomy"><a href="/personal_homepage/tags/#attention" title="Pages tagged attention" rel="tag">attention</a></li><li class="page-taxonomy"><a href="/personal_homepage/tags/#block" title="Pages tagged block" rel="tag">block</a></li><li class="page-taxonomy"><a href="/personal_homepage/tags/#content" title="Pages tagged content" rel="tag">content</a></li><li class="page-taxonomy"><a href="/personal_homepage/tags/#network" title="Pages tagged network" rel="tag">network</a></li>
  </ul>


        <!-- {::options parse_block_html="true" /} -->
<div id="entry-table-of-contents" class="toc-wrapper">
  <h2 id="toc-toggle">
    Table of Contents <i
      class="toc-toggle-icon fas fa-chevron-down"></i>
  </h2>
  <!-- 1. toc
{:toc} -->
  <!-- <div id="markdown-toc"> -->

    <ul class="inline_toc" id="my_toc"><li><a href="#convolution">convolution</a></li><li><a href="#gate-attention">gate attention</a></li><li><a href="#self-attention--cross-attention">self-attention &amp; cross-attention</a><ul><li><a href="#attention的一些变形和优化">attention的一些变形和优化</a></li></ul></li><li><a href="#光流估计网络">光流估计网络</a></li><li><a href="#代码">代码</a></li><li><a href="#reference">reference</a></li></ul>
  <!-- </div> -->

</div>
<!-- {::options parse_block_html="false" /} -->
        

<div id="entry-table-of-contents" class="toc-wrapper">
    <h2 id="toc-toggle" class="no_toc">
        Related Posts <i class="toc-toggle-icon fas fa-chevron-down"></i>
    </h2>
    <!-- <div id="markdown-toc"> -->
    <ul>
        
        <li><a href="/personal_homepage/house/wall-problem/">墙皮脱落问题</a></li>
        
        <li><a href="/personal_homepage/game/card/">掼蛋规则</a></li>
        
        <li><a href="/personal_homepage/affair/cat-airline/">搬家相关准备</a></li>
        
        <li><a href="/personal_homepage/math/conformal-geometry9/">conformal geometry学习 (九 ) -- atlas</a></li>
        
        <li><a href="/personal_homepage/affair/cat-can/">猫罐头调查</a></li>
        
    </ul>
    <!-- </div> -->

</div>





      </div>

      <div class="page-content">
        <div class="e-content">
          <p>在网络中, block是把input信息转换成output信息的过程: 一般, output(position, vector)是input(token, embedding vector)的线性组合, 组合的weight (position, token) 由input和output两方关系确定. 把着重强调这种信息交互的模块叫attention.</p>
<h2 id="convolution">convolution</h2>

<p><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240207151034.png" alt="Pasted image 20240207151034.png" width="400" /></p>

<p>convolution在神经网络流行之前就已经在图像任务里广泛使用了</p>
<ul>
  <li><strong>目的:</strong>
    <ul>
      <li>对spatial information进行特征的提取和转换</li>
    </ul>
  </li>
  <li><strong>特点:</strong>
    <ul>
      <li>pixel的weight只和(input, output)的相对位置有关, 因此也是平移不变的. 对每个输出像素有影响的只有input里kernel覆盖到的区域, 也就是response field</li>
      <li>每个不同的相对位置对应vector mapping不同
        <ul>
          <li>这个符合图像处理的直观, 左边有条线和右边有条线当然要映射成不同的结果</li>
          <li>有时也会把卷积拆分成1x1 conv和spatial conv(通道无关) 的形式</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="gate-attention">gate attention</h2>

<p><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240207151122.png" alt="Pasted image 20240207151122.png" width="400" /> <img src="/personal_homepage/docs/attachment/Pasted%20image%2020240207151136.png" alt="Pasted image 20240207151136.png" width="400" /></p>

<ul>
  <li><strong>目的:</strong>
    <ul>
      <li>提取channel<a href="#ref">1</a>或spatial的权重, 让网络关注更重要的信息</li>
    </ul>
  </li>
  <li><strong>特点:</strong>
    <ul>
      <li>spatial 信息对人类来说更有可读性, 所以可以把spatial weight可视化, 看看图片什么位置更加重要</li>
    </ul>
  </li>
</ul>

<h2 id="self-attention--cross-attention">self-attention &amp; cross-attention</h2>

<p><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240207151214.png" alt="Pasted image 20240207151214.png" width="400" /></p>

<ul>
  <li><strong>目的:</strong>
    <ul>
      <li>信息的交互: 在我的图示中, 是把文字信息加入视觉信息</li>
    </ul>
  </li>
  <li><strong>特点:</strong>
    <ul>
      <li>self-attention和cross-attention结构类似, 只是变成了一种输入</li>
      <li>cross-attention是现在常见的把一种信息加入另一种信息的方法
        <ul>
          <li>也用到了图像信息和文字信息的统一形式, 即(position, embedding) , 在图像信息中, position(S=H x W)是像素或patch的位置; 文字信息中, position是token在句中的前后位置</li>
        </ul>
      </li>
      <li>relative weight:
        <ul>
          <li>我们可以对relative weight可视化, 从而知道某个文字token和图像哪个位置最相关</li>
          <li>对relative weight的一个维度做average pooling, 就能知道哪个位置是更重要的<a href="#ref">4</a>(和gate-attention的用法一样)</li>
        </ul>
      </li>
      <li>Q,K,V(query, key, value) 的叫法是nlp搜索(匹配)任务的术语, 额, 其实我一直没法对这个望文生义…
        <ul>
          <li>其实从计算过程可以看出, 不管它们的原义, Q, K交换一下也没差</li>
          <li>query和key的embedding channel数应该相同, 为了之后计算他们relation的目的</li>
          <li>和卷积不一样, 放了更大的计算量在交互部分, 相对的, 每个input token对应vector mapping相同, 也就是用”value”部分解决了channel的映射: 这样导致同样的文字信息不论放在前面还是后面, 对应同一种output信息, 区别的只是他们权重可能不同, 或在图像的不同位置
            <ul>
              <li>把position embedding预先和text embedding在成为”value”之前编码在一起可以解决这个问题</li>
              <li>在文本模型里, 其实token的相对位置也很重要, 虽然不会像卷积一样relation完全由相对位置决定. 比如, 相对位置编码就把相对位置加到relation里</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="attention的一些变形和优化">attention的一些变形和优化</h3>

<ul>
  <li>mulit-head attention: 映射多组QKV, 计算得到的多组output再concat
    <ul>
      <li>在实际实现过程中,  先映射成一个大的QKV, 再把Q, K, V切分是完全等价的</li>
      <li>做mulit-head可以得到多组不同的映射方式, 因为两个token之间的关系可能由多种关系构成(比如猫和老鼠既可能是捕食关系, 也可能是同为宠物, 也可能同为动画片的角色)</li>
    </ul>
  </li>
  <li>分层attention: 在做attention之前对数据排序, 切分
    <ul>
      <li>可能有些pixel之间关联不大, 这样的话把它们分到不同的bucket, 分别做attention, 可以减少计算量<a href="#ref">2</a></li>
    </ul>
  </li>
  <li>多个attention合成:
    <ul>
      <li>在最近流行的ipadapter里面也讨论了多种不同来源的数据在cross-attention 里合成的问题, 结论是分别与原数据做attention最后再合成比较好<a href="#ref">3</a>, 这个也是我们一般naive的想法</li>
    </ul>
  </li>
</ul>

<h2 id="光流估计网络">光流估计网络</h2>

<p>视觉匹配任务</p>

<h2 id="代码">代码</h2>

<p><a href="https://gist.github.com/roshameow/503ec3769d75c47b82f2a7372e8c2dab#file-attention_block-py"><strong>attention_block.py</strong></a></p>

<h2 id="reference">reference</h2>
<p><span id="ref"></span></p>

<p>[1] Woo, Sanghyun, Jongchan Park, Joon-Young Lee, and In So Kweon. “CBAM: Convolutional Block Attention Module.” arXiv, July 18, 2018. <a href="https://doi.org/10.48550/arXiv.1807.06521">https://doi.org/10.48550/arXiv.1807.06521</a>. 介绍gate attention</p>

<p>[2] Cai, Yuanhao, Jing Lin, Xiaowan Hu, Haoqian Wang, Xin Yuan, Yulun Zhang, Radu Timofte, and Luc Van Gool. “Coarse-to-Fine Sparse Transformer for Hyperspectral Image Reconstruction.” arXiv, July 10, 2022. <a href="https://doi.org/10.48550/arXiv.2203.04845">https://doi.org/10.48550/arXiv.2203.04845</a>. 介绍了Spectra-aware hashing attention block的结构</p>

<p>[3] https://github.com/tencent-ailab/IP-Adapter</p>

<p>[4] Hong, Susung, Gyuseong Lee, Wooseok Jang, and Seungryong Kim. “Improving Sample Quality of Diffusion Models Using Self-Attention Guidance.” arXiv, August 24, 2023. <a href="https://doi.org/10.48550/arXiv.2210.00939">https://doi.org/10.48550/arXiv.2210.00939</a>.</p>

        </div>

        
        <div class="page-share">
  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Froshameow.github.io%2F%2Fpersonal_homepage%2Fdeeplearning%2Fattention%2F" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" class="btn btn--facebook btn--small"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i> <span>Share</span></a>
  <a href="https://twitter.com/intent/tweet?text=%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Cattention%E7%BB%93%E6%9E%84%E7%90%86%E8%A7%A3%20https%3A%2F%2Froshameow.github.io%2F%2Fpersonal_homepage%2Fdeeplearning%2Fattention%2F" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" class="btn btn--twitter btn--small"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i> <span>Tweet</span></a>
  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Froshameow.github.io%2F%2Fpersonal_homepage%2Fdeeplearning%2Fattention%2F" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" class="btn btn--linkedin btn--small"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> <span>LinkedIn</span></a>
  <a href="https://reddit.com/submit?title=%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Cattention%E7%BB%93%E6%9E%84%E7%90%86%E8%A7%A3&url=https%3A%2F%2Froshameow.github.io%2F%2Fpersonal_homepage%2Fdeeplearning%2Fattention%2F" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" class="btn btn--reddit btn--small"><i class="fab fa-fw fa-reddit" aria-hidden="true"></i> <span>Reddit</span></a>
</div>

        

        
        
  <div class="page-comments">
    <div id="disqus_thread"></div>
    <script>
      var disqus_config = function () {
        this.page.url = 'https://roshameow.github.io//personal_homepage/deeplearning/attention/';
        this.page.identifier = 'https://roshameow.github.io//personal_homepage/deeplearning/attention/';
      };

      (function() {
        var d = document, s = d.createElement('script');
        s.src = 'https://https-roshameow-github-io-personal-homepage.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
      })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
  </div>


        

        <nav class="page-pagination" role="navigation">
  
    <a class="page-previous" href="/personal_homepage/deeplearning/restruction-loss/">
      <h4 class="page-pagination-label">Previous</h4>
      <span class="page-pagination-title">
        <i class="fas fa-arrow-left"></i> 常用的图像 reconstruction loss

      </span>
    </a>
  

  
    <a class="page-next" href="/personal_homepage/photo/stable-diffusion1/">
      <h4 class="page-pagination-label">Next</h4>
      <span class="page-pagination-title">
        stable-diffusion的用法: 用 ipadatper+controlnet canny做风格转换
 <i class="fas fa-arrow-right"></i>
      </span>
    </a>
  
</nav>

      </div>

    </div>
  </article>
</main>

    <footer id="footer" class="site-footer">
  <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
<div class="social-icons"><a class="social-icon" href="/personal_homepage/atom.xml"><i class="fas fa-rss-square fa-2x" title="Feed"></i></a><a class="social-icon" href="https://github.com/roshameow"><i class="fab fa-github-square fa-2x" title="GitHub"></i></a><a class="social-icon" href="https://www.linkedin.com/in/wen-liu-06503a7b/"><i class="fab fa-linkedin fa-2x" title="LinkedIn"></i></a><a class="social-icon" href="mailto:w.liuatnk@gmail.com"><i class="fas fa-envelope-square fa-2x" title="e-mail"></i></a></div><div class="copyright">
    
      <p>&copy; 2024 Liu, Wen's Home Page. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://github.com/mmistakes/so-simple-theme" rel="nofollow">So Simple</a>.</p>
    
  </div>
</footer>

    <script src="https://code.jquery.com/jquery-3.3.1.min.js"
  integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script>
<script src="/personal_homepage/assets/js/main.min.js"></script>
<script src="https://use.fontawesome.com/releases/v5.0.12/js/all.js"></script><!-- <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'G-9TMSCDTBGN', 'auto');
  ga('send', 'pageview');
</script> -->

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-9TMSCDTBGN"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-9TMSCDTBGN');
</script>



<!-- MathJax -->

<script>
    // http://docs.mathjax.org/en/latest/upgrading/v2.html
    MathJax = {
        tex: {
            tags: "ams",    // eq numbering options: none, ams, all
            inlineMath: [['$', '$'], ["\\(", "\\)"]],  // 设置行内数学公式的开始和结束标记
            displayMath: [['$$', '$$'], ["\\[", "\\]"]]  // 设置行间数学公式的开始和结束标记
        },
        // options: {
        //   renderActions: {
        //     // for mathjax 3, handle <script "math/tex"> blocks inserted by kramdown
        //     find: [10, function (doc) {
        //       for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
        //         const display = !!node.type.match(/; *mode=display/);
        //         const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
        //         const text = document.createTextNode('');
        //         node.parentNode.replaceChild(text, node);
        //         math.start = { node: text, delim: '', n: 0 };
        //         math.end = { node: text, delim: '', n: 0 };
        //         doc.math.push(math);
        //       }
        //     }, '']
        //   }
        // }
    }
</script>

<script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  </body>

</html>
