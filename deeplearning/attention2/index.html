<!DOCTYPE html>
<!--
    So Simple Jekyll Theme 3.2.0
    Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
    Free for personal and commercial use under the MIT license
    https://github.com/mmistakes/so-simple-theme/blob/master/LICENSE
-->
<html lang="en-US" class="no-js">
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  

  
  
  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>attentionçš„ä¼˜åŒ–â€“ å¼•è¿›æ—¶åºç»“æ„ | Liu, Wenâ€™s Home Page</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="attentionçš„ä¼˜åŒ–â€“ å¼•è¿›æ—¶åºç»“æ„" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="åœ¨tokenå¾ˆå¤š(å¤§æ¨¡å‹ç”¨çš„è¶…é•¿æ–‡æœ¬), æˆ–è€…æœ¬èº«æ•°æ®æ˜¯æ—¶é—´åºåˆ—(æ¯”å¦‚è¯­éŸ³, è§†é¢‘æµ)çš„æƒ…å†µä¸‹, attentioné‡Œé¢çš„weightä¼šå¸¦æ¥$O(token^2)$ çš„å†…å­˜æ¶ˆè€—. state space modelå¯ä»¥è§£å†³è¿™ä¸ªé—®é¢˜." />
<meta property="og:description" content="åœ¨tokenå¾ˆå¤š(å¤§æ¨¡å‹ç”¨çš„è¶…é•¿æ–‡æœ¬), æˆ–è€…æœ¬èº«æ•°æ®æ˜¯æ—¶é—´åºåˆ—(æ¯”å¦‚è¯­éŸ³, è§†é¢‘æµ)çš„æƒ…å†µä¸‹, attentioné‡Œé¢çš„weightä¼šå¸¦æ¥$O(token^2)$ çš„å†…å­˜æ¶ˆè€—. state space modelå¯ä»¥è§£å†³è¿™ä¸ªé—®é¢˜." />
<link rel="canonical" href="https://roshameow.github.io//personal_homepage/deeplearning/attention2/" />
<meta property="og:url" content="https://roshameow.github.io//personal_homepage/deeplearning/attention2/" />
<meta property="og:site_name" content="Liu, Wenâ€™s Home Page" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-03-09T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="attentionçš„ä¼˜åŒ–â€“ å¼•è¿›æ—¶åºç»“æ„" />
<meta name="twitter:site" content="@" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-03-26T01:23:36+00:00","datePublished":"2024-03-09T00:00:00+00:00","description":"åœ¨tokenå¾ˆå¤š(å¤§æ¨¡å‹ç”¨çš„è¶…é•¿æ–‡æœ¬), æˆ–è€…æœ¬èº«æ•°æ®æ˜¯æ—¶é—´åºåˆ—(æ¯”å¦‚è¯­éŸ³, è§†é¢‘æµ)çš„æƒ…å†µä¸‹, attentioné‡Œé¢çš„weightä¼šå¸¦æ¥$O(token^2)$ çš„å†…å­˜æ¶ˆè€—. state space modelå¯ä»¥è§£å†³è¿™ä¸ªé—®é¢˜.","headline":"attentionçš„ä¼˜åŒ–â€“ å¼•è¿›æ—¶åºç»“æ„","mainEntityOfPage":{"@type":"WebPage","@id":"https://roshameow.github.io//personal_homepage/deeplearning/attention2/"},"url":"https://roshameow.github.io//personal_homepage/deeplearning/attention2/"}</script>
<!-- End Jekyll SEO tag -->


  

  <script>
    /* Cut the mustard */
    if ('querySelector' in document && 'addEventListener' in window) {
      document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + 'js';
    }
  </script>

  <link rel="stylesheet" href="/personal_homepage/assets/css/main.css">
  <link rel="stylesheet" href="/personal_homepage/assets/css/skins/default.css">
  
  <link rel="stylesheet"
    href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,400i,700,700i|Lora:400,400i,700,700i">
  <link rel="alternate" type="application/atom+xml" title="Liu, Wen&#39;s Home Page" href="/personal_homepage/atom.xml">
<!-- start custom head snippets -->

<!-- insert favicons. use http://realfavicongenerator.net/ -->
<link rel="shortcut icon" type="image/x-icon" href="/personal_homepage/docs/images/logo.ico">
<!-- end custom head snippets -->

</head>

  <body class="layout--post  attentionçš„ä¼˜åŒ–-å¼•è¿›æ—¶åºç»“æ„">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#primary-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    
  <div class="navigation-wrapper">
    <a href="#menu-toggle" id="menu-toggle">Menu</a>
    <nav id="primary-nav" class="site-nav animated drop">
      <ul><li><a href="/personal_homepage/">Home</a></li><li><a href="/personal_homepage/posts/">Posts</a></li><li><a href="/personal_homepage/categories/">Categories</a></li><li><a href="/personal_homepage/tags/">Tags</a></li><li><a href="/personal_homepage/family/">Family</a></li><li><a href="/personal_homepage/search/">Search</a></li></ul>
    </nav>
  </div><!-- /.navigation-wrapper -->


    <header class="masthead">
  <div class="wrap">
    
    
    
      
        <div class="site-title animated fadeIn"><a href="/personal_homepage/">Liu, Wen's Home Page</a></div>
      
      <p class="site-description animated fadeIn" itemprop="description">Work, Experiments and Ideas.</p>
    
  </div>
</header><!-- /.masthead -->


    <main id="main" class="main-content" aria-label="Content">
  <article class="h-entry">
    

    <div class="page-wrapper">
      <header class="page-header">
        
        
        <h1 id="page-title" class="page-title p-name">attentionçš„ä¼˜åŒ–â€“ å¼•è¿›æ—¶åºç»“æ„
</h1>
        
      </header>

      <div class="page-sidebar">
        <div class="page-author h-card p-author"><img src="/personal_homepage/docs/images/logo.svg" class="author-avatar u-photo" alt=""><div class="author-info">

<span class="read-time">2 min read</span>

    <time class="page-date dt-published" datetime="2024-03-09T00:00:00+00:00"><a class="u-url" href="">March 9, 2024</a>
</time>

  </div>
</div>

        
  <h3 class="page-taxonomies-title">Categories</h3>
  
  <ul class="page-taxonomies"><li class="page-taxonomy"><a class="p-category" href="/personal_homepage/categories/#deeplearning" title="Pages filed under deeplearning">deeplearning</a></li>
  </ul>


        
  <h3 class="page-taxonomies-title">Tags</h3>
  
  <ul class="page-taxonomies"><li class="page-taxonomy"><a href="/personal_homepage/tags/#mamba" title="Pages tagged Mamba" rel="tag">Mamba</a></li><li class="page-taxonomy"><a href="/personal_homepage/tags/#ssm" title="Pages tagged SSM" rel="tag">SSM</a></li><li class="page-taxonomy"><a href="/personal_homepage/tags/#attention" title="Pages tagged attention" rel="tag">attention</a></li><li class="page-taxonomy"><a href="/personal_homepage/tags/#content" title="Pages tagged content" rel="tag">content</a></li><li class="page-taxonomy"><a href="/personal_homepage/tags/#linear-attention" title="Pages tagged linear attention" rel="tag">linear attention</a></li><li class="page-taxonomy"><a href="/personal_homepage/tags/#state" title="Pages tagged state" rel="tag">state</a></li><li class="page-taxonomy"><a href="/personal_homepage/tags/#state-space-model" title="Pages tagged state space model" rel="tag">state space model</a></li><li class="page-taxonomy"><a href="/personal_homepage/tags/#time-series" title="Pages tagged time series" rel="tag">time series</a></li>
  </ul>


        <!-- {::options parse_block_html="true" /} -->
<div id="entry-table-of-contents" class="toc-wrapper">
  <h2 id="toc-toggle">
    Table of Contents <i
      class="toc-toggle-icon fas fa-chevron-down"></i>
  </h2>
  <!-- 1. toc
{:toc} -->
  <!-- <div id="markdown-toc"> -->

    <ul class="inline_toc" id="my_toc"><li><a href="#åŸºæœ¬ç»“æ„">åŸºæœ¬ç»“æ„</a><ul><li><a href="#ssmstate-space-model">SSM(state space model)</a></li><li><a href="#linear-attention">Linear Attention</a></li></ul></li><li><a href="#ä¸€äº›åˆ©ç”¨ssmlinear-attentionçš„è¯­è¨€æ¨¡å‹å·¥ä½œ">ä¸€äº›åˆ©ç”¨SSM/linear attentionçš„è¯­è¨€æ¨¡å‹å·¥ä½œ</a><ul><li><a href="#s4structured-state-space-sequence-ç»“æ„">S4(Structured state space sequence) ç»“æ„</a></li><li><a href="#h3hungry-hungry-hippos-ç»“æ„">H3(Hungry Hungry Hippos) ç»“æ„</a></li><li><a href="#retnet">RetNet</a></li><li><a href="#rwkv">RWKV</a></li><li><a href="#mamba">Mamba</a></li></ul></li><li><a href="#reference">reference</a></li><li><a href="#å…¶ä»–è®¨è®ºçš„é“¾æ¥">å…¶ä»–è®¨è®ºçš„é“¾æ¥</a></li></ul>
  <!-- </div> -->

</div>
<!-- {::options parse_block_html="false" /} -->
        

<div id="entry-table-of-contents" class="toc-wrapper">
    <h2 id="toc-toggle" class="no_toc">
        Related Posts <i class="toc-toggle-icon fas fa-chevron-down"></i>
    </h2>
    <!-- <div id="markdown-toc"> -->
    <ul>
        
        <li><a href="/personal_homepage/blender/blender-learning21/">blenderå­¦ä¹ :  å†…å‘å…‰æ•ˆæœ</a></li>
        
        <li><a href="/personal_homepage/tool/papers/">ç­›é€‰é€‚åˆé˜…è¯»çš„è®ºæ–‡</a></li>
        
        <li><a href="/personal_homepage/finance/alpha5/">worldquant BRAIN è‚¡ç¥¨å› å­ (äº” ) -- æ•°æ®é›†ç†è§£</a></li>
        
        <li><a href="/personal_homepage/blender/blender-learning20/">blenderå­¦ä¹ :  åˆ¶ä½œéšéŸ³ä¹å¾‹åŠ¨çš„è§†é¢‘</a></li>
        
        <li><a href="/personal_homepage/algorithm/color-moderate3/">ä¼ æ„Ÿå™¨é¢œè‰²è°ƒåˆ¶ (å››) -- å¤šå…‰è°±é¢œè‰²æ ¡å‡†</a></li>
        
    </ul>
    <!-- </div> -->

</div>





      </div>

      <div class="page-content">
        <div class="e-content">
          <p>åœ¨tokenå¾ˆå¤š(å¤§æ¨¡å‹ç”¨çš„è¶…é•¿æ–‡æœ¬), æˆ–è€…æœ¬èº«æ•°æ®æ˜¯æ—¶é—´åºåˆ—(æ¯”å¦‚è¯­éŸ³, è§†é¢‘æµ)çš„æƒ…å†µä¸‹, attentioné‡Œé¢çš„weightä¼šå¸¦æ¥$O(token^2)$ çš„å†…å­˜æ¶ˆè€—. state space modelå¯ä»¥è§£å†³è¿™ä¸ªé—®é¢˜.</p>

<h2 id="åŸºæœ¬ç»“æ„">åŸºæœ¬ç»“æ„</h2>

<h3 id="ssmstate-space-model">SSM(state space model)</h3>

<ul>
  <li><a href="https://en.wikipedia.org/wiki/State-space_representation">çŠ¶æ€ç©ºé—´(state space)</a> : ç”¨state vectorè®°å½•å†å²çš„input, è€Œä¸æ˜¯è®°å½•æ‰€æœ‰çš„å†å²token
    <ul>
      <li><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240317183044.png" alt="Pasted image 20240317183044.png" width="200" /></li>
    </ul>
  </li>
  <li>è¿ç»­è¡¨ç¤º:
    <ul>
      <li>linear state space modelçš„ä¸€èˆ¬å½¢å¼:
        <ul>
          <li>$\dot x(t)=A(t)x(t)+B(t)u(t)$ (ç”¨input uæ›´æ–°çŠ¶æ€ x)</li>
          <li>$y(t)=C(t)x(t)+D(t)u(t)$  (ç”¨çŠ¶æ€x, ç”Ÿæˆoutput y)</li>
        </ul>
      </li>
      <li>å¦‚æœA,B,Cæ˜¯time invariant, ä»¥åŠçœç•¥D, å¾—åˆ°,
        <ul>
          <li>$\dot x(t)=Ax(t)+Bu(t)$
            <ul>
              <li>è§£å¾— $x(t)=e^{At}x(0)+\int^t_0 e^{A(t-\tau)}Bu(\tau)d\tau$    , $x(t)$ æ˜¯$u(t)$ çš„å·ç§¯å½¢å¼: $x(t)=K(t)*u(t)$, $K(t)=e^{At}B$</li>
            </ul>
          </li>
          <li>$y(t)=Cx(t)$</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>ç¦»æ•£è¡¨ç¤º:
    <ul>
      <li>è¿­ä»£çš„è¡¨ç¤º: $x_{k+1}=\bar A x_k + \bar Bu_k$ , $y_{k+1}=Cx_{k+1}$
        <ul>
          <li>ç”¨<a href="https://en.wikipedia.org/wiki/Zero-order_hold">Zero order hold</a> ç¦»æ•£è¡¨ç¤º$u(t)$ , $\Delta$ æ˜¯step_size, æœ‰: $\bar A=e^{\Delta A}$ , $\bar B=(\Delta A)^{-1}(e^{\Delta A}-I)\cdot \Delta B$
            <ul>
              <li><a href="https://en.wikipedia.org/wiki/Discretization">Discretization wiki</a>ä¸­æ­£å¥½æœ‰è¿™ä¸ªæ¨å¯¼</li>
            </ul>
          </li>
          <li>ç”¨blinearç¦»æ•£è¡¨ç¤º$u(t)$,  $\Delta$ æ˜¯step_size, æœ‰: $\bar A=(I-\Delta/2\cdot A)^{-1}(I+\Delta/2\cdot A)$ , $\bar B=(I-\Delta/2\cdot A)^{-1}\cdot \Delta B$</li>
        </ul>
      </li>
      <li>å·ç§¯çš„è¡¨ç¤º: $y=x * \bar K$ , $\bar K = (C\bar B, C\bar{AB}, \dots, C\bar A^{N-1}\bar B)$</li>
    </ul>
  </li>
  <li>ä¸€äº›çŠ¶æ€ç©ºé—´çš„åº”ç”¨:
    <ul>
      <li>Kalman filter</li>
      <li>control theory</li>
    </ul>
  </li>
  <li>ç‰¹ç‚¹:
    <ul>
      <li>è¿­ä»£å’Œå·ç§¯çš„å½¢å¼å®Œå…¨ç­‰ä»·, ä¸€èˆ¬åœ¨è®­ç»ƒæ—¶ç”¨å·ç§¯å½¢å¼æ–¹ä¾¿è®¡ç®—å¹¶è¡Œ, åœ¨æ¨ç†æ—¶ç”¨è¿­ä»£å½¢å¼èŠ‚çœæ—¶é—´å’Œå­˜å‚¨</li>
      <li>å¯¹äºå¾ˆå¤šä»»åŠ¡, è¾“å…¥$u(t)$ æ˜¯å¤šé€šé“çš„, è¿™ä¸ªæ—¶å€™ä¸€èˆ¬æŠŠç›¸åŒssmåº”ç”¨åœ¨æ¯ä¸ªé€šé“ä¸Š</li>
      <li>SSMæ”¯æŒæ— é™é•¿çš„åºåˆ—æ¨ç†, æ‰€æœ‰çš„å†å²tokenä¿¡æ¯éƒ½è¢«å‹ç¼©åˆ°stateé‡Œ</li>
    </ul>
  </li>
</ul>

<h3 id="linear-attention">Linear Attention</h3>

<p><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240317103547.png" alt="Pasted image 20240317103547.png" width="1000" /></p>

<ul>
  <li>æŠŠself-attentionçš„softmaxæ¢æ‰å°±å¯ä»¥æ”¹å†™æˆSSMçš„å½¢å¼, æ—¢æœ‰SSMè®¡ç®—èŠ‚çœå†…å­˜å’Œæ—¶é—´çš„ä¼˜ç‚¹, åˆè´´è¿‘self-attentionçš„æ•ˆæœ
    <ul>
      <li>æŒ‰ç…§è¿™ç§å†™æ³•, å¯ä»¥æŠŠSSMè¿­ä»£ä¸­çš„$B, C$ å†™æˆ$K, Q$</li>
    </ul>
  </li>
  <li>å®é™…ä½¿ç”¨ä¸­å‘ç°, linear attentionçš„stateå®¹é‡æ˜¯æœ‰é™åº¦çš„, è™½ç„¶ç†è®ºä¸Šæ”¯æŒæ— é™é•¿çš„token, ä½†æ˜¯å¯¹å†å²ä¿¡æ¯çš„é—å¤±ä¸å¥½å¤„ç†, æ•ˆæœè¿œä¸å¦‚åŸºäºself-attentionçš„æ–¹æ³•</li>
</ul>

<h2 id="ä¸€äº›åˆ©ç”¨ssmlinear-attentionçš„è¯­è¨€æ¨¡å‹å·¥ä½œ">ä¸€äº›åˆ©ç”¨SSM/linear attentionçš„è¯­è¨€æ¨¡å‹å·¥ä½œ</h2>
<h3 id="s4structured-state-space-sequence-ç»“æ„">S4(Structured state space sequence) ç»“æ„</h3>

<p>å‘ç°ç”¨HiPPO(High-order Polynomial Projection Operator)çŸ©é˜µåšä¸ºSSM ä¸­çš„çŠ¶æ€è½¬ç§»çŸ©é˜µ$A$ çš„åˆå§‹åŒ–å¯ä»¥æå¤§çš„æé«˜SSMçš„æ€§èƒ½.</p>

<ul>
  <li>motivation:
    <ol>
      <li>åœ¨measure function  $\omega^{(t)}$ ä¸‹, ç”¨ä¸€ç»„orthogonal basisè¡¨ç¤ºæœ‰é™ç»´çš„state $x$ (ä¾‹å¦‚: <a href="https://en.wikipedia.org/wiki/Legendre_polynomials">Legendre polynomials</a>  $P_n^{(t)}$ , $x(t)=\sum_n c_n(t)*P_n^{(t)}$)</li>
      <li>åœ¨measure function  $\omega^{(t)}$ ä¸‹, æƒ³ä»state $x$  å°½é‡<strong>æ¢å¤å‡ºinput $u$ çš„å†å²ä¿¡æ¯</strong> $u_{\le t}$ , å³
  \(x=\text{argmin}_{c_n}||u_{\le t}-\sum_n c_n*P_n||_{\omega}\)
        <ol>
          <li>measure functionçš„é€‰æ‹©å½±å“äº†æˆ‘ä»¬åœ¨state $x$ ä¸­ä¿ç•™ä»€ä¹ˆæ ·çš„å†å²ä¿¡æ¯: ä¾‹å¦‚ $\downarrow$
            <ol>
              <li>LegS(scaled Legendre measure), $\omega(t)=\mathbb{1}[0,t]/t$ å¯¹å†å²ä¿¡æ¯æƒé‡å¹³å‡, ä¿ç•™$\le t$ çš„æ‰€æœ‰å†å²ä¿¡æ¯</li>
              <li>LegT(Translated Legendre), $\omega(t)=\frac{1}{\theta} \mathbb 1[t-\theta,t]$ , ä¿ç•™æœ€è¿‘ä¸€æ®µçš„ä¿¡æ¯</li>
            </ol>
          </li>
        </ol>
      </li>
    </ol>
  </li>
  <li>è®¡ç®—:
    <ol>
      <li>ä»¥ä¸Šä¸¤ç‚¹å¾—å‡º, $c_n$ æ˜¯$u_{\le t}$ åœ¨basis $P_n$, measure $\omega$ ,ä¸‹çš„projection, $c_n=&lt;u_{\le t}, P_n^{(t)}&gt;_{\omega}$ ,
        <ul>
          <li>å³ \(x=\sum_n &lt;u_{\le t}, P_n^{(t)}&gt;_{\omega} P_n^{(t)}=proj_{(\omega, P_n^{(t)})} (u_{\le t})\)</li>
          <li>å¦‚æœå†™æˆSSMå·ç§¯çš„å½¢å¼, \(x=K*u\) , \(K(t)=\sum_n P_n^{(t)}*\omega^{(t)}\)</li>
        </ul>
      </li>
      <li>å› ä¸º$P_n$ æœ¬èº«æ˜¯basis: $\dot P_n\in span(P_n)$,  æˆ‘ä»¬é€‰å–çš„$\omega$ ä¹Ÿå°½é‡æ»¡è¶³: $\dot \omega\in span(\omega)$</li>
      <li>æŠŠ $x$ å’Œ $u$ å¸¦å…¥SSMçš„ODE( $\dot x(t)=Ax(t)+Bu(t)$ ), å³å¾—åˆ°HippoçŸ©é˜µ $A(t)$ , å’Œ $B(t)$
        <ul>
          <li>åœ¨è®¡ç®—è¿‡ç¨‹ä¸­ä¸ºäº†æ–¹ä¾¿ä¼šåšä¸€äº›å˜é‡æ›¿æ¢</li>
          <li>å¯¹äºLegSçš„æƒ…å†µ, $A(t)$ ä¸æ˜¯time-invariantçš„, ä¸è¿‡, å¯ä»¥å†™æˆ $A(t)=\frac{1}{t}A$ çš„å½¢å¼
            <ul>
              <li>S4çš„ç»“æ„é‡Œç”¨çš„æ˜¯æ²¡æœ‰$\frac{1}{t}$ çš„ç‰ˆæœ¬, å¯ä»¥å¯¹åº”$w(t)=e^{-t}$ å’Œ $p_n(t)=L_n(e^{-t})$ . æŒ‰æ–‡ç« é‡Œçš„è¯´æ³•, æ˜¯å®éªŒæœ‰æ•ˆä¹‹åæ‰åè¿‡æ¥æ‰¾åˆ°å¯¹åº”çš„basiså’ŒmeasureğŸ¤”ï¸</li>
            </ul>
          </li>
        </ul>
      </li>
    </ol>
  </li>
</ul>

<p>è¿™ä¸ªæ¢å¤ä¿¡å·çš„æƒ³æ³•å’ŒVAEé‡Œé¢ç”¨latent spaceå‹ç¼©å›¾åƒä¿¡å·çš„æƒ³æ³•å¾ˆç›¸ä¼¼.</p>
<h3 id="h3hungry-hungry-hippos-ç»“æ„">H3(Hungry Hungry Hippos) ç»“æ„</h3>

<p><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240318042950.png" alt="Pasted image 20240318042950.png" width="400" /></p>

<ul>
  <li>å’Œlinear attentionä¸åŒ, H3é‡Œé¢KVå¹¶æ²¡æŠŠä¸åŒtokençš„ä¿¡æ¯åŠ åœ¨ä¸€èµ·, æ•´ä¸ªç»“æ„é‡Œ, ä¸åŒtokençš„ä¿¡æ¯åªåœ¨SSMçš„åœ°æ–¹è¿›è¡Œäº¤äº’</li>
  <li>SSMæ¨¡å—: è¿™ä¸¤ä¸ªSSMæ¨¡å—æ˜¯ä¸ºäº†è¯­è¨€æ¨¡å‹ç²¾å¿ƒè®¾è®¡çš„
    <ul>
      <li>å…¶ä¸­Shift SSMæŒ‡SSMä¸­çš„çŠ¶æ€è½¬ç§»çŸ©é˜µ$A$ æ˜¯ä¸ª<a href="https://en.wikipedia.org/wiki/Shift_matrix">shift matrix</a> , æœ‰ $A^k=0$ çš„ç‰¹ç‚¹, è·ç¦»å’Œå½“å‰tokenå¤§äºkçš„ä¿¡æ¯è¢«è‡ªç„¶çš„åˆ·æ–°, åœ¨ç½‘ç»œä¸­ç”¨äºè®°å½•è¾ƒè¿‘çš„ç´¢å¼•
        <ul>
          <li>ä»£ç : https://github.com/HazyResearch/H3/blob/main/src/models/ssm/ss_kernel_shift.py</li>
        </ul>
      </li>
      <li>Diag SSMæŒ‡è½¬ç§»çŸ©é˜µ$A$æ˜¯diagonal matrix, ä¿å­˜å…¨å±€è®°å¿†, å·²ç»è®°å½•çš„ä¿¡æ¯ä¸ä¼šéšç€æ–°è¾“å…¥tokenåˆ·æ–°</li>
    </ul>
  </li>
  <li><a href="https://www.youtube.com/watch?v=TkOSKrlpnU4&amp;t=716s">è§†é¢‘</a> é‡Œè¯¦ç»†è®²è§£äº†h3ç»“æ„å’Œassociatve recallçš„ä»»åŠ¡(è¾“å…¥ä¸€ä¸ªdict, æé—®æŸä¸ªkeyå¯¹åº”çš„value)çš„å…³ç³»: ä¹Ÿæ˜¯æˆ‘ç¬¬ä¸€æ¬¡çŸ¥é“q,k,vçš„åå­—å’Œä»»åŠ¡çš„è”ç³»</li>
</ul>

<h3 id="retnet">RetNet</h3>

<p><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240325140921.png" alt="Pasted image 20240325140921.png" width="700" /></p>

<ul>
  <li>åœ¨linear-attentionçš„åŸºç¡€ä¸Šå¢åŠ äº†stateçš„çŠ¶æ€è½¬ç§», æ˜¯å¯¹linear attentionçš„ä¸€ä¸ªå·§å¦™çš„å˜å½¢.</li>
  <li>è¯•å›¾ç”¨position embeddingå’ŒDçŸ©é˜µçš„exponential decayä»£æ›¿attentioné‡Œçš„softmaxçš„åŠŸèƒ½</li>
  <li><a href="https://github.com/Jamie-Stirling/RetNet/blob/main/src/retention.py">Rententionçš„ä»£ç </a> å†™çš„å¾ˆæ¸…æ™°</li>
</ul>

<h3 id="rwkv">RWKV</h3>

<ul>
  <li>time-mixingå’Œchannel-mixingä¸¤ä¸ªæ¨¡å—äº¤æ›¿: <a href="https://github.com/BlinkDL/ChatRWKV/blob/main/RWKV_in_150_lines.py">ä»£ç </a>
    <ul>
      <li>time-mixingæ˜¯ä¸€ä¸ªç±»linear attentionçš„ç»“æ„, é…ä¸€ä¸ªç‰¹æ®Šè®¾è®¡çš„stateè½¬ç§»(ä¹Ÿå°±æ˜¯æ–‡ç« é‡Œçš„WKV operator)
        <ul>
          <li>è¿™ä¹ˆè®¾è®¡stateè½¬ç§»çš„åŠ¨æœº, æˆ‘æ˜¯ä¸æ˜ç™½</li>
        </ul>
      </li>
      <li>channel-mixingæ˜¯åˆ†æˆä¸¤è·¯, åˆ†åˆ«åš conv(token window_size=2) + linear + activation</li>
    </ul>
  </li>
  <li>RWKVåšäº†é…å¥—çš„chatçš„å¾ˆå¤šç›¸å…³ä»£ç å·¥ä½œ</li>
</ul>

<h3 id="mamba">Mamba</h3>

<p><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240325123207.png" alt="Pasted image 20240325123207.png" width="300" /></p>

<ul>
  <li>åŸºç¡€çš„SSMæ¨¡å—æ”¹ä¸ºS6(Selective S4)æ¨¡å—: æŠŠSSMç¦»æ•£è¡¨ç¤ºä¸­$\Delta, B, C$ æ”¹æˆtime-variant. ä»¥é€‚åº”ä¸åŒä½ç½®çš„ä¿¡æ¯å¯†åº¦.
    <ul>
      <li>æˆ‘è§‰å¾—è¿™ç§æƒ³æ³•å’Œdeformable convolutionçš„æƒ³æ³•ç±»ä¼¼, éƒ½æ˜¯å¯¹ä¸€äº›ç»“æ„parametrize.</li>
    </ul>
  </li>
  <li>Mamba block:
    <ul>
      <li>ç”¨convolutionå’ŒS6, è€Œä¸æ˜¯attentionè¿›è¡Œä¿¡æ¯äº¤äº’.</li>
      <li>åŸºäºmamba blockçš„å¤§æ¨¡å‹ç”¨æ›´å°‘çš„å‚æ•°è¾¾åˆ°äº†å’ŒåŸºäºattentionçš„å¤§æ¨¡å‹ç±»ä¼¼çš„æ•ˆæœ.</li>
      <li>æœ€è¿‘æœ‰ä¸€äº›æ–‡ç« ç”¨mamba blocké­”æ”¹å»åšè§†è§‰ä»»åŠ¡? æ²¡æœ‰ç‰¹åˆ«äº†è§£ä½†æ˜¯æ„Ÿè§‰å¾ˆå¥‡æ€ª, å› ä¸ºSSMæœ¬èº«æœ‰ä¸ªé¡ºåºçš„ç»“æ„â€¦åœ¨è§†è§‰ä»»åŠ¡ä¸Šè¿™ä¸ªé¡ºåºè¦æ€ä¹ˆå®šä¹‰?</li>
    </ul>
  </li>
</ul>

<h2 id="reference">reference</h2>

<p>[1] Katharopoulos, Angelos, Apoorv Vyas, Nikolaos Pappas, and FranÃ§ois Fleuret. â€œTransformers Are RNNs: Fast Autoregressive Transformers with <strong>Linear Attention</strong>.â€ arXiv, August 31, 2020. <a href="https://doi.org/10.48550/arXiv.2006.16236">https://doi.org/10.48550/arXiv.2006.16236</a>.</p>

<p>[2] Gu, Albert, Karan Goel, and Christopher RÃ©. â€œEfficiently Modeling Long Sequences with <strong>Structured State Spaces</strong>.â€ arXiv, August 5, 2022. <a href="http://arxiv.org/abs/2111.00396">http://arxiv.org/abs/2111.00396</a>. (S4)</p>

<p>[3] Gu, Albert, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Re. â€œ<strong>HiPPO</strong>: Recurrent Memory with Optimal Polynomial Projections.â€ arXiv, October 22, 2020. <a href="https://doi.org/10.48550/arXiv.2008.07669">https://doi.org/10.48550/arXiv.2008.07669</a>.</p>

<p>[4] Gu, Albert, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher RÃ©. â€œHow to Train Your <strong>HiPPO</strong>: State Space Models with Generalized Orthogonal Basis Projections.â€ arXiv, August 5, 2022. <a href="http://arxiv.org/abs/2206.12037">http://arxiv.org/abs/2206.12037</a>.</p>

<p>[5] Fu, Daniel Y., Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, and Christopher RÃ©. â€œ<strong>Hungry Hungry Hippos</strong>: Towards Language Modeling with State Space Models.â€ arXiv, April 28, 2023. <a href="https://doi.org/10.48550/arXiv.2212.14052">https://doi.org/10.48550/arXiv.2212.14052</a>.</p>

<p>[6] Sun, Yutao, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. â€œ<strong>Retentive Network</strong>: A Successor to Transformer for Large Language Models.â€ arXiv, August 9, 2023. <a href="https://doi.org/10.48550/arXiv.2307.08621">https://doi.org/10.48550/arXiv.2307.08621</a>.</p>

<p>[7] Peng, Bo, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, et al. â€œ<strong>RWKV</strong>: Reinventing RNNs for the Transformer Era.â€ arXiv, December 10, 2023. <a href="https://doi.org/10.48550/arXiv.2305.13048">https://doi.org/10.48550/arXiv.2305.13048</a>.</p>

<p>[8] Gu, Albert, and Tri Dao. â€œ<strong>Mamba</strong>: Linear-Time Sequence Modeling with Selective State Spaces.â€ arXiv.org, December 1, 2023. <a href="https://arxiv.org/abs/2312.00752v1">https://arxiv.org/abs/2312.00752v1</a>.</p>
<h2 id="å…¶ä»–è®¨è®ºçš„é“¾æ¥">å…¶ä»–è®¨è®ºçš„é“¾æ¥</h2>

<p>[1] https://blog.csdn.net/v_JULY_v/article/details/134923301</p>

<p>[2]  <a href="https://github.com/state-spaces/s4/issues/40">https://github.com/state-spaces/s4/issues/40</a> ä½œè€…å¯¹hippoçš„ç­”ç–‘</p>

<p>[3] https://www.youtube.com/watch?v=TkOSKrlpnU4&amp;t=716s H3ä½œè€…çš„è®²è§£</p>

<p>[3] https://www.zhihu.com/question/602564718/answer/3042600470 RWKVçš„çŸ¥ä¹è®¨è®º</p>

<p>[4] https://cloud.tencent.com/developer/article/2377967 Mamba blockçš„ç†è§£</p>


        </div>

        

        
        
  <div class="page-comments">
    <div id="disqus_thread"></div>
    <script>
      var disqus_config = function () {
        this.page.url = 'https://roshameow.github.io//personal_homepage/deeplearning/attention2/';
        this.page.identifier = 'https://roshameow.github.io//personal_homepage/deeplearning/attention2/';
      };

      (function() {
        var d = document, s = d.createElement('script');
        s.src = 'https://https-roshameow-github-io-personal-homepage.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
      })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
  </div>


        

        <nav class="page-pagination" role="navigation">
  
    <a class="page-previous" href="/personal_homepage/blender/blender-learning7/">
      <h4 class="page-pagination-label">Previous</h4>
      <span class="page-pagination-title">
        <i class="fas fa-arrow-left"></i> blenderå­¦ä¹ : åšå¡é€šæè¾¹

      </span>
    </a>
  

  
    <a class="page-next" href="/personal_homepage/blender/blender-learning8/">
      <h4 class="page-pagination-label">Next</h4>
      <span class="page-pagination-title">
        blenderå­¦ä¹ : åšæµå…‰æ•ˆæœ
 <i class="fas fa-arrow-right"></i>
      </span>
    </a>
  
</nav>

      </div>

    </div>
  </article>
</main>

    <footer id="footer" class="site-footer">
  <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
<div class="social-icons"><a class="social-icon" href="/personal_homepage/atom.xml"><i class="fas fa-rss-square fa-2x" title="Feed"></i></a><a class="social-icon" href="https://github.com/roshameow"><i class="fab fa-github-square fa-2x" title="GitHub"></i></a><a class="social-icon" href="https://www.linkedin.com/in/wen-liu-06503a7b/"><i class="fab fa-linkedin fa-2x" title="LinkedIn"></i></a><a class="social-icon" href="mailto:w.liuatnk@gmail.com"><i class="fas fa-envelope-square fa-2x" title="e-mail"></i></a></div><div class="copyright">
    
      <p>&copy; 2025 Liu, Wen's Home Page. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://github.com/mmistakes/so-simple-theme" rel="nofollow">So Simple</a>.</p>
    
  </div>
</footer>

    <script src="https://code.jquery.com/jquery-3.3.1.min.js"
  integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script>
<script src="/personal_homepage/assets/js/main.min.js"></script>
<script src="https://use.fontawesome.com/releases/v5.0.12/js/all.js"></script><!-- <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'G-9TMSCDTBGN', 'auto');
  ga('send', 'pageview');
</script> -->

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-9TMSCDTBGN"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-9TMSCDTBGN');
</script>



<!-- MathJax -->

<script>
    // http://docs.mathjax.org/en/latest/upgrading/v2.html
    MathJax = {
        tex: {
            tags: "ams",    // eq numbering options: none, ams, all
            inlineMath: [['$', '$'], ["\\(", "\\)"]],  // è®¾ç½®è¡Œå†…æ•°å­¦å…¬å¼çš„å¼€å§‹å’Œç»“æŸæ ‡è®°
            displayMath: [['$$', '$$'], ["\\[", "\\]"]]  // è®¾ç½®è¡Œé—´æ•°å­¦å…¬å¼çš„å¼€å§‹å’Œç»“æŸæ ‡è®°
        },
        // options: {
        //   renderActions: {
        //     // for mathjax 3, handle <script "math/tex"> blocks inserted by kramdown
        //     find: [10, function (doc) {
        //       for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
        //         const display = !!node.type.match(/; *mode=display/);
        //         const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
        //         const text = document.createTextNode('');
        //         node.parentNode.replaceChild(text, node);
        //         math.start = { node: text, delim: '', n: 0 };
        //         math.end = { node: text, delim: '', n: 0 };
        //         doc.math.push(math);
        //       }
        //     }, '']
        //   }
        // }
    }
</script>

<script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  </body>

</html>
