<!DOCTYPE html>
<!--
    So Simple Jekyll Theme 3.2.0
    Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
    Free for personal and commercial use under the MIT license
    https://github.com/mmistakes/so-simple-theme/blob/master/LICENSE
-->
<html lang="en-US" class="no-js">
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  

  
  
  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>attention的优化– 引进时序结构 | Liu, Wen’s Home Page</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="attention的优化– 引进时序结构" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="在token很多(大模型用的超长文本), 或者本身数据是时间序列(比如语音, 视频流)的情况下, attention里面的weight会带来$O(token^2)$ 的内存消耗. state space model可以解决这个问题." />
<meta property="og:description" content="在token很多(大模型用的超长文本), 或者本身数据是时间序列(比如语音, 视频流)的情况下, attention里面的weight会带来$O(token^2)$ 的内存消耗. state space model可以解决这个问题." />
<link rel="canonical" href="https://roshameow.github.io//personal_homepage/deeplearning/attention2/" />
<meta property="og:url" content="https://roshameow.github.io//personal_homepage/deeplearning/attention2/" />
<meta property="og:site_name" content="Liu, Wen’s Home Page" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-03-09T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="attention的优化– 引进时序结构" />
<meta name="twitter:site" content="@" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-03-26T01:23:36+00:00","datePublished":"2024-03-09T00:00:00+00:00","description":"在token很多(大模型用的超长文本), 或者本身数据是时间序列(比如语音, 视频流)的情况下, attention里面的weight会带来$O(token^2)$ 的内存消耗. state space model可以解决这个问题.","headline":"attention的优化– 引进时序结构","mainEntityOfPage":{"@type":"WebPage","@id":"https://roshameow.github.io//personal_homepage/deeplearning/attention2/"},"url":"https://roshameow.github.io//personal_homepage/deeplearning/attention2/"}</script>
<!-- End Jekyll SEO tag -->


  

  <script>
    /* Cut the mustard */
    if ('querySelector' in document && 'addEventListener' in window) {
      document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + 'js';
    }
  </script>

  <link rel="stylesheet" href="/personal_homepage/assets/css/main.css">
  <link rel="stylesheet" href="/personal_homepage/assets/css/skins/default.css">
  
  <link rel="stylesheet"
    href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,400i,700,700i|Lora:400,400i,700,700i">
  <link rel="alternate" type="application/atom+xml" title="Liu, Wen&#39;s Home Page" href="/personal_homepage/atom.xml">
<!-- start custom head snippets -->

<!-- insert favicons. use http://realfavicongenerator.net/ -->
<link rel="shortcut icon" type="image/x-icon" href="/personal_homepage/docs/images/logo.ico">
<!-- end custom head snippets -->

</head>

  <body class="layout--post  attention的优化-引进时序结构">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#primary-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    
  <div class="navigation-wrapper">
    <a href="#menu-toggle" id="menu-toggle">Menu</a>
    <nav id="primary-nav" class="site-nav animated drop">
      <ul><li><a href="/personal_homepage/">Home</a></li><li><a href="/personal_homepage/posts/">Posts</a></li><li><a href="/personal_homepage/categories/">Categories</a></li><li><a href="/personal_homepage/tags/">Tags</a></li><li><a href="/personal_homepage/family/">Family</a></li><li><a href="/personal_homepage/search/">Search</a></li></ul>
    </nav>
  </div><!-- /.navigation-wrapper -->


    <header class="masthead">
  <div class="wrap">
    
    
    
      
        <div class="site-title animated fadeIn"><a href="/personal_homepage/">Liu, Wen's Home Page</a></div>
      
      <p class="site-description animated fadeIn" itemprop="description">Work, Experiments and Ideas.</p>
    
  </div>
</header><!-- /.masthead -->


    <main id="main" class="main-content" aria-label="Content">
  <article class="h-entry">
    

    <div class="page-wrapper">
      <header class="page-header">
        
        
        <h1 id="page-title" class="page-title p-name">attention的优化– 引进时序结构
</h1>
        
      </header>

      <div class="page-sidebar">
        <div class="page-author h-card p-author"><img src="/personal_homepage/docs/images/logo.svg" class="author-avatar u-photo" alt=""><div class="author-info">

<span class="read-time">2 min read</span>

    <time class="page-date dt-published" datetime="2024-03-09T00:00:00+00:00"><a class="u-url" href="">March 9, 2024</a>
</time>

  </div>
</div>

        
  <h3 class="page-taxonomies-title">Categories</h3>
  
  <ul class="page-taxonomies"><li class="page-taxonomy"><a class="p-category" href="/personal_homepage/categories/#deeplearning" title="Pages filed under deeplearning">deeplearning</a></li>
  </ul>


        
  <h3 class="page-taxonomies-title">Tags</h3>
  
  <ul class="page-taxonomies"><li class="page-taxonomy"><a href="/personal_homepage/tags/#mamba" title="Pages tagged Mamba" rel="tag">Mamba</a></li><li class="page-taxonomy"><a href="/personal_homepage/tags/#ssm" title="Pages tagged SSM" rel="tag">SSM</a></li><li class="page-taxonomy"><a href="/personal_homepage/tags/#attention" title="Pages tagged attention" rel="tag">attention</a></li><li class="page-taxonomy"><a href="/personal_homepage/tags/#content" title="Pages tagged content" rel="tag">content</a></li><li class="page-taxonomy"><a href="/personal_homepage/tags/#linear-attention" title="Pages tagged linear attention" rel="tag">linear attention</a></li><li class="page-taxonomy"><a href="/personal_homepage/tags/#state" title="Pages tagged state" rel="tag">state</a></li><li class="page-taxonomy"><a href="/personal_homepage/tags/#state-space-model" title="Pages tagged state space model" rel="tag">state space model</a></li><li class="page-taxonomy"><a href="/personal_homepage/tags/#time-series" title="Pages tagged time series" rel="tag">time series</a></li>
  </ul>


        <!-- {::options parse_block_html="true" /} -->
<div id="entry-table-of-contents" class="toc-wrapper">
  <h2 id="toc-toggle">
    Table of Contents <i
      class="toc-toggle-icon fas fa-chevron-down"></i>
  </h2>
  <!-- 1. toc
{:toc} -->
  <!-- <div id="markdown-toc"> -->

    <ul class="inline_toc" id="my_toc"><li><a href="#基本结构">基本结构</a><ul><li><a href="#ssmstate-space-model">SSM(state space model)</a></li><li><a href="#linear-attention">Linear Attention</a></li></ul></li><li><a href="#一些利用ssmlinear-attention的语言模型工作">一些利用SSM/linear attention的语言模型工作</a><ul><li><a href="#s4structured-state-space-sequence-结构">S4(Structured state space sequence) 结构</a></li><li><a href="#h3hungry-hungry-hippos-结构">H3(Hungry Hungry Hippos) 结构</a></li><li><a href="#retnet">RetNet</a></li><li><a href="#rwkv">RWKV</a></li><li><a href="#mamba">Mamba</a></li></ul></li><li><a href="#reference">reference</a></li><li><a href="#其他讨论的链接">其他讨论的链接</a></li></ul>
  <!-- </div> -->

</div>
<!-- {::options parse_block_html="false" /} -->
        

<div id="entry-table-of-contents" class="toc-wrapper">
    <h2 id="toc-toggle" class="no_toc">
        Related Posts <i class="toc-toggle-icon fas fa-chevron-down"></i>
    </h2>
    <!-- <div id="markdown-toc"> -->
    <ul>
        
        <li><a href="/personal_homepage/blender/blender-learning21/">blender学习:  内发光效果</a></li>
        
        <li><a href="/personal_homepage/tool/papers/">筛选适合阅读的论文</a></li>
        
        <li><a href="/personal_homepage/finance/alpha5/">worldquant BRAIN 股票因子 (五 ) -- 数据集理解</a></li>
        
        <li><a href="/personal_homepage/blender/blender-learning20/">blender学习:  制作随音乐律动的视频</a></li>
        
        <li><a href="/personal_homepage/algorithm/color-moderate3/">传感器颜色调制 (四) -- 多光谱颜色校准</a></li>
        
    </ul>
    <!-- </div> -->

</div>





      </div>

      <div class="page-content">
        <div class="e-content">
          <p>在token很多(大模型用的超长文本), 或者本身数据是时间序列(比如语音, 视频流)的情况下, attention里面的weight会带来$O(token^2)$ 的内存消耗. state space model可以解决这个问题.</p>

<h2 id="基本结构">基本结构</h2>

<h3 id="ssmstate-space-model">SSM(state space model)</h3>

<ul>
  <li><a href="https://en.wikipedia.org/wiki/State-space_representation">状态空间(state space)</a> : 用state vector记录历史的input, 而不是记录所有的历史token
    <ul>
      <li><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240317183044.png" alt="Pasted image 20240317183044.png" width="200" /></li>
    </ul>
  </li>
  <li>连续表示:
    <ul>
      <li>linear state space model的一般形式:
        <ul>
          <li>$\dot x(t)=A(t)x(t)+B(t)u(t)$ (用input u更新状态 x)</li>
          <li>$y(t)=C(t)x(t)+D(t)u(t)$  (用状态x, 生成output y)</li>
        </ul>
      </li>
      <li>如果A,B,C是time invariant, 以及省略D, 得到,
        <ul>
          <li>$\dot x(t)=Ax(t)+Bu(t)$
            <ul>
              <li>解得 $x(t)=e^{At}x(0)+\int^t_0 e^{A(t-\tau)}Bu(\tau)d\tau$    , $x(t)$ 是$u(t)$ 的卷积形式: $x(t)=K(t)*u(t)$, $K(t)=e^{At}B$</li>
            </ul>
          </li>
          <li>$y(t)=Cx(t)$</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>离散表示:
    <ul>
      <li>迭代的表示: $x_{k+1}=\bar A x_k + \bar Bu_k$ , $y_{k+1}=Cx_{k+1}$
        <ul>
          <li>用<a href="https://en.wikipedia.org/wiki/Zero-order_hold">Zero order hold</a> 离散表示$u(t)$ , $\Delta$ 是step_size, 有: $\bar A=e^{\Delta A}$ , $\bar B=(\Delta A)^{-1}(e^{\Delta A}-I)\cdot \Delta B$
            <ul>
              <li><a href="https://en.wikipedia.org/wiki/Discretization">Discretization wiki</a>中正好有这个推导</li>
            </ul>
          </li>
          <li>用blinear离散表示$u(t)$,  $\Delta$ 是step_size, 有: $\bar A=(I-\Delta/2\cdot A)^{-1}(I+\Delta/2\cdot A)$ , $\bar B=(I-\Delta/2\cdot A)^{-1}\cdot \Delta B$</li>
        </ul>
      </li>
      <li>卷积的表示: $y=x * \bar K$ , $\bar K = (C\bar B, C\bar{AB}, \dots, C\bar A^{N-1}\bar B)$</li>
    </ul>
  </li>
  <li>一些状态空间的应用:
    <ul>
      <li>Kalman filter</li>
      <li>control theory</li>
    </ul>
  </li>
  <li>特点:
    <ul>
      <li>迭代和卷积的形式完全等价, 一般在训练时用卷积形式方便计算并行, 在推理时用迭代形式节省时间和存储</li>
      <li>对于很多任务, 输入$u(t)$ 是多通道的, 这个时候一般把相同ssm应用在每个通道上</li>
      <li>SSM支持无限长的序列推理, 所有的历史token信息都被压缩到state里</li>
    </ul>
  </li>
</ul>

<h3 id="linear-attention">Linear Attention</h3>

<p><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240317103547.png" alt="Pasted image 20240317103547.png" width="1000" /></p>

<ul>
  <li>把self-attention的softmax换掉就可以改写成SSM的形式, 既有SSM计算节省内存和时间的优点, 又贴近self-attention的效果
    <ul>
      <li>按照这种写法, 可以把SSM迭代中的$B, C$ 写成$K, Q$</li>
    </ul>
  </li>
  <li>实际使用中发现, linear attention的state容量是有限度的, 虽然理论上支持无限长的token, 但是对历史信息的遗失不好处理, 效果远不如基于self-attention的方法</li>
</ul>

<h2 id="一些利用ssmlinear-attention的语言模型工作">一些利用SSM/linear attention的语言模型工作</h2>
<h3 id="s4structured-state-space-sequence-结构">S4(Structured state space sequence) 结构</h3>

<p>发现用HiPPO(High-order Polynomial Projection Operator)矩阵做为SSM 中的状态转移矩阵$A$ 的初始化可以极大的提高SSM的性能.</p>

<ul>
  <li>motivation:
    <ol>
      <li>在measure function  $\omega^{(t)}$ 下, 用一组orthogonal basis表示有限维的state $x$ (例如: <a href="https://en.wikipedia.org/wiki/Legendre_polynomials">Legendre polynomials</a>  $P_n^{(t)}$ , $x(t)=\sum_n c_n(t)*P_n^{(t)}$)</li>
      <li>在measure function  $\omega^{(t)}$ 下, 想从state $x$  尽量<strong>恢复出input $u$ 的历史信息</strong> $u_{\le t}$ , 即
  \(x=\text{argmin}_{c_n}||u_{\le t}-\sum_n c_n*P_n||_{\omega}\)
        <ol>
          <li>measure function的选择影响了我们在state $x$ 中保留什么样的历史信息: 例如 $\downarrow$
            <ol>
              <li>LegS(scaled Legendre measure), $\omega(t)=\mathbb{1}[0,t]/t$ 对历史信息权重平均, 保留$\le t$ 的所有历史信息</li>
              <li>LegT(Translated Legendre), $\omega(t)=\frac{1}{\theta} \mathbb 1[t-\theta,t]$ , 保留最近一段的信息</li>
            </ol>
          </li>
        </ol>
      </li>
    </ol>
  </li>
  <li>计算:
    <ol>
      <li>以上两点得出, $c_n$ 是$u_{\le t}$ 在basis $P_n$, measure $\omega$ ,下的projection, $c_n=&lt;u_{\le t}, P_n^{(t)}&gt;_{\omega}$ ,
        <ul>
          <li>即 \(x=\sum_n &lt;u_{\le t}, P_n^{(t)}&gt;_{\omega} P_n^{(t)}=proj_{(\omega, P_n^{(t)})} (u_{\le t})\)</li>
          <li>如果写成SSM卷积的形式, \(x=K*u\) , \(K(t)=\sum_n P_n^{(t)}*\omega^{(t)}\)</li>
        </ul>
      </li>
      <li>因为$P_n$ 本身是basis: $\dot P_n\in span(P_n)$,  我们选取的$\omega$ 也尽量满足: $\dot \omega\in span(\omega)$</li>
      <li>把 $x$ 和 $u$ 带入SSM的ODE( $\dot x(t)=Ax(t)+Bu(t)$ ), 即得到Hippo矩阵 $A(t)$ , 和 $B(t)$
        <ul>
          <li>在计算过程中为了方便会做一些变量替换</li>
          <li>对于LegS的情况, $A(t)$ 不是time-invariant的, 不过, 可以写成 $A(t)=\frac{1}{t}A$ 的形式
            <ul>
              <li>S4的结构里用的是没有$\frac{1}{t}$ 的版本, 可以对应$w(t)=e^{-t}$ 和 $p_n(t)=L_n(e^{-t})$ . 按文章里的说法, 是实验有效之后才反过来找到对应的basis和measure🤔️</li>
            </ul>
          </li>
        </ul>
      </li>
    </ol>
  </li>
</ul>

<p>这个恢复信号的想法和VAE里面用latent space压缩图像信号的想法很相似.</p>
<h3 id="h3hungry-hungry-hippos-结构">H3(Hungry Hungry Hippos) 结构</h3>

<p><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240318042950.png" alt="Pasted image 20240318042950.png" width="400" /></p>

<ul>
  <li>和linear attention不同, H3里面KV并没把不同token的信息加在一起, 整个结构里, 不同token的信息只在SSM的地方进行交互</li>
  <li>SSM模块: 这两个SSM模块是为了语言模型精心设计的
    <ul>
      <li>其中Shift SSM指SSM中的状态转移矩阵$A$ 是个<a href="https://en.wikipedia.org/wiki/Shift_matrix">shift matrix</a> , 有 $A^k=0$ 的特点, 距离和当前token大于k的信息被自然的刷新, 在网络中用于记录较近的索引
        <ul>
          <li>代码: https://github.com/HazyResearch/H3/blob/main/src/models/ssm/ss_kernel_shift.py</li>
        </ul>
      </li>
      <li>Diag SSM指转移矩阵$A$是diagonal matrix, 保存全局记忆, 已经记录的信息不会随着新输入token刷新</li>
    </ul>
  </li>
  <li><a href="https://www.youtube.com/watch?v=TkOSKrlpnU4&amp;t=716s">视频</a> 里详细讲解了h3结构和associatve recall的任务(输入一个dict, 提问某个key对应的value)的关系: 也是我第一次知道q,k,v的名字和任务的联系</li>
</ul>

<h3 id="retnet">RetNet</h3>

<p><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240325140921.png" alt="Pasted image 20240325140921.png" width="700" /></p>

<ul>
  <li>在linear-attention的基础上增加了state的状态转移, 是对linear attention的一个巧妙的变形.</li>
  <li>试图用position embedding和D矩阵的exponential decay代替attention里的softmax的功能</li>
  <li><a href="https://github.com/Jamie-Stirling/RetNet/blob/main/src/retention.py">Rentention的代码</a> 写的很清晰</li>
</ul>

<h3 id="rwkv">RWKV</h3>

<ul>
  <li>time-mixing和channel-mixing两个模块交替: <a href="https://github.com/BlinkDL/ChatRWKV/blob/main/RWKV_in_150_lines.py">代码</a>
    <ul>
      <li>time-mixing是一个类linear attention的结构, 配一个特殊设计的state转移(也就是文章里的WKV operator)
        <ul>
          <li>这么设计state转移的动机, 我是不明白</li>
        </ul>
      </li>
      <li>channel-mixing是分成两路, 分别做 conv(token window_size=2) + linear + activation</li>
    </ul>
  </li>
  <li>RWKV做了配套的chat的很多相关代码工作</li>
</ul>

<h3 id="mamba">Mamba</h3>

<p><img src="/personal_homepage/docs/attachment/Pasted%20image%2020240325123207.png" alt="Pasted image 20240325123207.png" width="300" /></p>

<ul>
  <li>基础的SSM模块改为S6(Selective S4)模块: 把SSM离散表示中$\Delta, B, C$ 改成time-variant. 以适应不同位置的信息密度.
    <ul>
      <li>我觉得这种想法和deformable convolution的想法类似, 都是对一些结构parametrize.</li>
    </ul>
  </li>
  <li>Mamba block:
    <ul>
      <li>用convolution和S6, 而不是attention进行信息交互.</li>
      <li>基于mamba block的大模型用更少的参数达到了和基于attention的大模型类似的效果.</li>
      <li>最近有一些文章用mamba block魔改去做视觉任务? 没有特别了解但是感觉很奇怪, 因为SSM本身有个顺序的结构…在视觉任务上这个顺序要怎么定义?</li>
    </ul>
  </li>
</ul>

<h2 id="reference">reference</h2>

<p>[1] Katharopoulos, Angelos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. “Transformers Are RNNs: Fast Autoregressive Transformers with <strong>Linear Attention</strong>.” arXiv, August 31, 2020. <a href="https://doi.org/10.48550/arXiv.2006.16236">https://doi.org/10.48550/arXiv.2006.16236</a>.</p>

<p>[2] Gu, Albert, Karan Goel, and Christopher Ré. “Efficiently Modeling Long Sequences with <strong>Structured State Spaces</strong>.” arXiv, August 5, 2022. <a href="http://arxiv.org/abs/2111.00396">http://arxiv.org/abs/2111.00396</a>. (S4)</p>

<p>[3] Gu, Albert, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Re. “<strong>HiPPO</strong>: Recurrent Memory with Optimal Polynomial Projections.” arXiv, October 22, 2020. <a href="https://doi.org/10.48550/arXiv.2008.07669">https://doi.org/10.48550/arXiv.2008.07669</a>.</p>

<p>[4] Gu, Albert, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher Ré. “How to Train Your <strong>HiPPO</strong>: State Space Models with Generalized Orthogonal Basis Projections.” arXiv, August 5, 2022. <a href="http://arxiv.org/abs/2206.12037">http://arxiv.org/abs/2206.12037</a>.</p>

<p>[5] Fu, Daniel Y., Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, and Christopher Ré. “<strong>Hungry Hungry Hippos</strong>: Towards Language Modeling with State Space Models.” arXiv, April 28, 2023. <a href="https://doi.org/10.48550/arXiv.2212.14052">https://doi.org/10.48550/arXiv.2212.14052</a>.</p>

<p>[6] Sun, Yutao, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. “<strong>Retentive Network</strong>: A Successor to Transformer for Large Language Models.” arXiv, August 9, 2023. <a href="https://doi.org/10.48550/arXiv.2307.08621">https://doi.org/10.48550/arXiv.2307.08621</a>.</p>

<p>[7] Peng, Bo, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, et al. “<strong>RWKV</strong>: Reinventing RNNs for the Transformer Era.” arXiv, December 10, 2023. <a href="https://doi.org/10.48550/arXiv.2305.13048">https://doi.org/10.48550/arXiv.2305.13048</a>.</p>

<p>[8] Gu, Albert, and Tri Dao. “<strong>Mamba</strong>: Linear-Time Sequence Modeling with Selective State Spaces.” arXiv.org, December 1, 2023. <a href="https://arxiv.org/abs/2312.00752v1">https://arxiv.org/abs/2312.00752v1</a>.</p>
<h2 id="其他讨论的链接">其他讨论的链接</h2>

<p>[1] https://blog.csdn.net/v_JULY_v/article/details/134923301</p>

<p>[2]  <a href="https://github.com/state-spaces/s4/issues/40">https://github.com/state-spaces/s4/issues/40</a> 作者对hippo的答疑</p>

<p>[3] https://www.youtube.com/watch?v=TkOSKrlpnU4&amp;t=716s H3作者的讲解</p>

<p>[3] https://www.zhihu.com/question/602564718/answer/3042600470 RWKV的知乎讨论</p>

<p>[4] https://cloud.tencent.com/developer/article/2377967 Mamba block的理解</p>


        </div>

        

        
        
  <div class="page-comments">
    <div id="disqus_thread"></div>
    <script>
      var disqus_config = function () {
        this.page.url = 'https://roshameow.github.io//personal_homepage/deeplearning/attention2/';
        this.page.identifier = 'https://roshameow.github.io//personal_homepage/deeplearning/attention2/';
      };

      (function() {
        var d = document, s = d.createElement('script');
        s.src = 'https://https-roshameow-github-io-personal-homepage.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
      })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
  </div>


        

        <nav class="page-pagination" role="navigation">
  
    <a class="page-previous" href="/personal_homepage/blender/blender-learning7/">
      <h4 class="page-pagination-label">Previous</h4>
      <span class="page-pagination-title">
        <i class="fas fa-arrow-left"></i> blender学习: 做卡通描边

      </span>
    </a>
  

  
    <a class="page-next" href="/personal_homepage/blender/blender-learning8/">
      <h4 class="page-pagination-label">Next</h4>
      <span class="page-pagination-title">
        blender学习: 做流光效果
 <i class="fas fa-arrow-right"></i>
      </span>
    </a>
  
</nav>

      </div>

    </div>
  </article>
</main>

    <footer id="footer" class="site-footer">
  <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
<div class="social-icons"><a class="social-icon" href="/personal_homepage/atom.xml"><i class="fas fa-rss-square fa-2x" title="Feed"></i></a><a class="social-icon" href="https://github.com/roshameow"><i class="fab fa-github-square fa-2x" title="GitHub"></i></a><a class="social-icon" href="https://www.linkedin.com/in/wen-liu-06503a7b/"><i class="fab fa-linkedin fa-2x" title="LinkedIn"></i></a><a class="social-icon" href="mailto:w.liuatnk@gmail.com"><i class="fas fa-envelope-square fa-2x" title="e-mail"></i></a></div><div class="copyright">
    
      <p>&copy; 2025 Liu, Wen's Home Page. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://github.com/mmistakes/so-simple-theme" rel="nofollow">So Simple</a>.</p>
    
  </div>
</footer>

    <script src="https://code.jquery.com/jquery-3.3.1.min.js"
  integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script>
<script src="/personal_homepage/assets/js/main.min.js"></script>
<script src="https://use.fontawesome.com/releases/v5.0.12/js/all.js"></script><!-- <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'G-9TMSCDTBGN', 'auto');
  ga('send', 'pageview');
</script> -->

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-9TMSCDTBGN"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-9TMSCDTBGN');
</script>



<!-- MathJax -->

<script>
    // http://docs.mathjax.org/en/latest/upgrading/v2.html
    MathJax = {
        tex: {
            tags: "ams",    // eq numbering options: none, ams, all
            inlineMath: [['$', '$'], ["\\(", "\\)"]],  // 设置行内数学公式的开始和结束标记
            displayMath: [['$$', '$$'], ["\\[", "\\]"]]  // 设置行间数学公式的开始和结束标记
        },
        // options: {
        //   renderActions: {
        //     // for mathjax 3, handle <script "math/tex"> blocks inserted by kramdown
        //     find: [10, function (doc) {
        //       for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
        //         const display = !!node.type.match(/; *mode=display/);
        //         const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
        //         const text = document.createTextNode('');
        //         node.parentNode.replaceChild(text, node);
        //         math.start = { node: text, delim: '', n: 0 };
        //         math.end = { node: text, delim: '', n: 0 };
        //         doc.math.push(math);
        //       }
        //     }, '']
        //   }
        // }
    }
</script>

<script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  </body>

</html>
