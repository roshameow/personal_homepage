---
layout: post
title: attentionçš„ä¼˜åŒ–-- å¼•è¿›æ—¶åºç»“æ„
categories:
  - deeplearning
tags:
  - content
  - SSM
  - state space model
  - state
  - linear attention
  - time series
  - Mamba
  - attention
last_modified_at: 2024-03-25T15:08:10-08:00
---

åœ¨tokenå¾ˆå¤š(å¤§æ¨¡å‹ç”¨çš„è¶…é•¿æ–‡æœ¬), æˆ–è€…æœ¬èº«æ•°æ®æ˜¯æ—¶é—´åºåˆ—(æ¯”å¦‚è¯­éŸ³, è§†é¢‘æµ)çš„æƒ…å†µä¸‹, attentioné‡Œé¢çš„weightä¼šå¸¦æ¥$O(token^2)$ çš„å†…å­˜æ¶ˆè€—   


## åŸºæœ¬ç»“æ„

### SSM(state space model)

- [çŠ¶æ€ç©ºé—´(state space)](https://en.wikipedia.org/wiki/State-space_representation) : ç”¨state vectorè®°å½•å†å²çš„input, è€Œä¸æ˜¯è®°å½•æ‰€æœ‰çš„å†å²token
	- ![Pasted image 20240317183044.png]({{ '/docs/attachment/Pasted image 20240317183044.png' | relative_url }}){:width="200"} 
- è¿ç»­è¡¨ç¤º:
	- linear state space modelçš„ä¸€èˆ¬å½¢å¼:
		- $\dot x(t)=A(t)x(t)+B(t)u(t)$ (ç”¨input uæ›´æ–°çŠ¶æ€ x)
		- $y(t)=C(t)x(t)+D(t)u(t)$  (ç”¨çŠ¶æ€x, ç”Ÿæˆoutput y)
	- å¦‚æœA,B,Cæ˜¯time invariant, ä»¥åŠçœç•¥D, å¾—åˆ°,
		- $\dot x(t)=Ax(t)+Bu(t)$
			- è§£å¾— $x(t)=e^{At}x(0)+\int^t_0 e^{A(t-\tau)}Bu(\tau)d\tau$    , $x(t)$ æ˜¯$u(t)$ çš„å·ç§¯å½¢å¼: $x(t)=K(t)*u(t)$, $K(t)=e^{At}B$ 
		- $y(t)=Cx(t)$ 
- ç¦»æ•£è¡¨ç¤º:
	- è¿­ä»£çš„è¡¨ç¤º: $x_{k+1}=\bar A x_k + \bar Bu_k$ , $y_{k+1}=Cx_{k+1}$ 
		- ç”¨[Zero order hold](https://en.wikipedia.org/wiki/Zero-order_hold) ç¦»æ•£è¡¨ç¤º$u(t)$ , $\Delta$ æ˜¯step_size, æœ‰: $\bar A=e^{\Delta A}$ , $\bar B=(\Delta A)^{-1}(e^{\Delta A}-I)\cdot \Delta B$ 
			- [Discretization wiki](https://en.wikipedia.org/wiki/Discretization )ä¸­æ­£å¥½æœ‰è¿™ä¸ªæ¨å¯¼
		- ç”¨blinearç¦»æ•£è¡¨ç¤º$u(t)$,  $\Delta$ æ˜¯step_size, æœ‰: $\bar A=(I-\Delta/2\cdot A)^{-1}(I+\Delta/2\cdot A)$ , $\bar B=(I-\Delta/2\cdot A)^{-1}\cdot \Delta B$ 
	- å·ç§¯çš„è¡¨ç¤º: $y=x * \bar K$ , $\bar K = (C\bar B, C\bar{AB}, \dots, C\bar A^{N-1}\bar B)$  
- ä¸€äº›çŠ¶æ€ç©ºé—´çš„åº”ç”¨: 
	- Kalman filter
	- control theory
- ç‰¹ç‚¹:
	- è¿­ä»£å’Œå·ç§¯çš„å½¢å¼å®Œå…¨ç­‰ä»·, ä¸€èˆ¬åœ¨è®­ç»ƒæ—¶ç”¨å·ç§¯å½¢å¼æ–¹ä¾¿è®¡ç®—å¹¶è¡Œ, åœ¨æ¨ç†æ—¶ç”¨è¿­ä»£å½¢å¼èŠ‚çœæ—¶é—´å’Œå­˜å‚¨
	- å¯¹äºå¾ˆå¤šä»»åŠ¡, è¾“å…¥$u(t)$ æ˜¯å¤šé€šé“çš„, è¿™ä¸ªæ—¶å€™ä¸€èˆ¬æŠŠç›¸åŒssmåº”ç”¨åœ¨æ¯ä¸ªé€šé“ä¸Š
	- SSMæ”¯æŒæ— é™é•¿çš„åºåˆ—æ¨ç†, æ‰€æœ‰çš„å†å²tokenä¿¡æ¯éƒ½è¢«å‹ç¼©åˆ°stateé‡Œ

### Linear Attention

![Pasted image 20240317103547.png]({{ '/docs/attachment/Pasted image 20240317103547.png' | relative_url }}){:width="1000"}

- æŠŠself-attentionçš„softmaxæ¢æ‰å°±å¯ä»¥æ”¹å†™æˆSSMçš„å½¢å¼, æ—¢æœ‰SSMè®¡ç®—èŠ‚çœå†…å­˜å’Œæ—¶é—´çš„ä¼˜ç‚¹, åˆè´´è¿‘self-attentionçš„æ•ˆæœ
	- æŒ‰ç…§è¿™ç§å†™æ³•, å¯ä»¥æŠŠSSMè¿­ä»£ä¸­çš„$B, C$ å†™æˆ$K, Q$ 
- å®é™…ä½¿ç”¨ä¸­å‘ç°, linear attentionçš„stateå®¹é‡æ˜¯æœ‰é™åº¦çš„, è™½ç„¶ç†è®ºä¸Šæ”¯æŒæ— é™é•¿çš„token, ä½†æ˜¯å¯¹å†å²ä¿¡æ¯çš„é—å¤±ä¸å¥½å¤„ç†, æ•ˆæœè¿œä¸å¦‚åŸºäºself-attentionçš„æ–¹æ³•


## ä¸€äº›åˆ©ç”¨SSM/linear attentionçš„è¯­è¨€æ¨¡å‹å·¥ä½œ
### S4(Structured state space sequence) ç»“æ„

å‘ç°ç”¨HiPPO(High-order Polynomial Projection Operator)çŸ©é˜µåšä¸ºSSM ä¸­çš„çŠ¶æ€è½¬ç§»çŸ©é˜µ$A$ çš„åˆå§‹åŒ–å¯ä»¥æå¤§çš„æé«˜SSMçš„æ€§èƒ½.

- motivation: 
	1. åœ¨measure function  $\omega^{(t)}$ ä¸‹, ç”¨ä¸€ç»„orthogonal basisè¡¨ç¤ºæœ‰é™ç»´çš„state $x$ (ä¾‹å¦‚: [Legendre polynomials](https://en.wikipedia.org/wiki/Legendre_polynomials)  $P_n^{(t)}$ , $x(t)=\sum_n c_n(t)*P_n^{(t)}$)
	2. åœ¨measure function  $\omega^{(t)}$ ä¸‹, æƒ³ä»state $x$  å°½é‡**æ¢å¤å‡ºinput $u$ çš„å†å²ä¿¡æ¯** $u_{\le t}$ , å³
	$$x=\text{argmin}_{c_n}||u_{\le t}-\sum_n c_n*P_n||_{\omega}$$
		2. measure functionçš„é€‰æ‹©å½±å“äº†æˆ‘ä»¬åœ¨state $x$ ä¸­ä¿ç•™ä»€ä¹ˆæ ·çš„å†å²ä¿¡æ¯: ä¾‹å¦‚ $\downarrow$ 
			1. LegS(scaled Legendre measure), $\omega(t)=\mathbb{1}[0,t]/t$ å¯¹å†å²ä¿¡æ¯æƒé‡å¹³å‡, ä¿ç•™$\le t$ çš„æ‰€æœ‰å†å²ä¿¡æ¯
			2. LegT(Translated Legendre), $\omega(t)=\frac{1}{\theta} \mathbb 1[t-\theta,t]$ , ä¿ç•™æœ€è¿‘ä¸€æ®µçš„ä¿¡æ¯
- è®¡ç®—:
	1. ä»¥ä¸Šä¸¤ç‚¹å¾—å‡º, $c_n$ æ˜¯$u_{\le t}$ åœ¨basis $P_n$, measure $\omega$ ,ä¸‹çš„projection, $c_n=<u_{\le t}, P_n^{(t)}>_{\omega}$ , 
		- å³ $$x=\sum_n <u_{\le t}, P_n^{(t)}>_{\omega} P_n^{(t)}=proj_{(\omega, P_n^{(t)})} (u_{\le t})$$  
		- å¦‚æœå†™æˆSSMå·ç§¯çš„å½¢å¼, $$x=K*u$$ , $$K(t)=\sum_n P_n^{(t)}*\omega^{(t)}$$   
	1. å› ä¸º$P_n$ æœ¬èº«æ˜¯basis: $\dot P_n\in span(P_n)$,  æˆ‘ä»¬é€‰å–çš„$\omega$ ä¹Ÿå°½é‡æ»¡è¶³: $\dot \omega\in span(\omega)$ 
	2. æŠŠ $x$ å’Œ $u$ å¸¦å…¥SSMçš„ODE( $\dot x(t)=Ax(t)+Bu(t)$ ), å³å¾—åˆ°HippoçŸ©é˜µ $A(t)$ , å’Œ $B(t)$ 
		- åœ¨è®¡ç®—è¿‡ç¨‹ä¸­ä¸ºäº†æ–¹ä¾¿ä¼šåšä¸€äº›å˜é‡æ›¿æ¢
		- å¯¹äºLegSçš„æƒ…å†µ, $A(t)$ ä¸æ˜¯time-invariantçš„, ä¸è¿‡, å¯ä»¥å†™æˆ $A(t)=\frac{1}{t}A$ çš„å½¢å¼
			- S4çš„ç»“æ„é‡Œç”¨çš„æ˜¯æ²¡æœ‰$\frac{1}{t}$ çš„ç‰ˆæœ¬, å¯ä»¥å¯¹åº”$w(t)=e^{-t}$ å’Œ $p_n(t)=L_n(e^{-t})$ . æŒ‰æ–‡ç« é‡Œçš„è¯´æ³•, æ˜¯å®éªŒæœ‰æ•ˆä¹‹åæ‰åè¿‡æ¥æ‰¾åˆ°å¯¹åº”çš„basiså’ŒmeasureğŸ¤”ï¸

è¿™ä¸ªæ¢å¤ä¿¡å·çš„æƒ³æ³•å’ŒVAEé‡Œé¢ç”¨latent spaceå‹ç¼©å›¾åƒä¿¡å·çš„æƒ³æ³•å¾ˆç›¸ä¼¼. 
### H3(Hungry Hungry Hippos) ç»“æ„

![Pasted image 20240318042950.png]({{ '/docs/attachment/Pasted image 20240318042950.png' | relative_url }}){:width="400"} 

- å’Œlinear attentionä¸åŒ, H3é‡Œé¢KVå¹¶æ²¡æŠŠä¸åŒtokençš„ä¿¡æ¯åŠ åœ¨ä¸€èµ·, æ•´ä¸ªç»“æ„é‡Œ, ä¸åŒtokençš„ä¿¡æ¯åªåœ¨SSMçš„åœ°æ–¹è¿›è¡Œäº¤äº’
- SSMæ¨¡å—: è¿™ä¸¤ä¸ªSSMæ¨¡å—æ˜¯ä¸ºäº†è¯­è¨€æ¨¡å‹ç²¾å¿ƒè®¾è®¡çš„
	-  å…¶ä¸­Shift SSMæŒ‡SSMä¸­çš„çŠ¶æ€è½¬ç§»çŸ©é˜µ$A$ æ˜¯ä¸ª[shift matrix](https://en.wikipedia.org/wiki/Shift_matrix) , æœ‰ $A^k=0$ çš„ç‰¹ç‚¹, è·ç¦»å’Œå½“å‰tokenå¤§äºkçš„ä¿¡æ¯è¢«è‡ªç„¶çš„åˆ·æ–°, åœ¨ç½‘ç»œä¸­ç”¨äºè®°å½•è¾ƒè¿‘çš„ç´¢å¼•
		- ä»£ç : https://github.com/HazyResearch/H3/blob/main/src/models/ssm/ss_kernel_shift.py
	- Diag SSMæŒ‡è½¬ç§»çŸ©é˜µ$A$æ˜¯diagonal matrix, ä¿å­˜å…¨å±€è®°å¿†, å·²ç»è®°å½•çš„ä¿¡æ¯ä¸ä¼šéšç€æ–°è¾“å…¥tokenåˆ·æ–°
- [è§†é¢‘](https://www.youtube.com/watch?v=TkOSKrlpnU4&t=716s) é‡Œè¯¦ç»†è®²è§£äº†h3ç»“æ„å’Œassociatve recallçš„ä»»åŠ¡(è¾“å…¥ä¸€ä¸ªdict, æé—®æŸä¸ªkeyå¯¹åº”çš„value)çš„å…³ç³»: ä¹Ÿæ˜¯æˆ‘ç¬¬ä¸€æ¬¡çŸ¥é“q,k,vçš„åå­—å’Œä»»åŠ¡çš„è”ç³»

### RetNet

![Pasted image 20240325140921.png]({{ '/docs/attachment/Pasted image 20240325140921.png' | relative_url }}){:width="700"}

- åœ¨linear-attentionçš„åŸºç¡€ä¸Šå¢åŠ äº†stateçš„çŠ¶æ€è½¬ç§», æ˜¯å¯¹linear attentionçš„ä¸€ä¸ªå·§å¦™çš„å˜å½¢. 
- è¯•å›¾ç”¨position embeddingå’ŒDçŸ©é˜µçš„exponential decayä»£æ›¿attentioné‡Œçš„softmaxçš„åŠŸèƒ½
- [Rententionçš„ä»£ç ](https://github.com/Jamie-Stirling/RetNet/blob/main/src/retention.py) å†™çš„å¾ˆæ¸…æ™°

### RWKV

- time-mixingå’Œchannel-mixingä¸¤ä¸ªæ¨¡å—äº¤æ›¿: [ä»£ç ](https://github.com/BlinkDL/ChatRWKV/blob/main/RWKV_in_150_lines.py) 
	- time-mixingæ˜¯ä¸€ä¸ªç±»linear attentionçš„ç»“æ„, é…ä¸€ä¸ªç‰¹æ®Šè®¾è®¡çš„stateè½¬ç§»(ä¹Ÿå°±æ˜¯æ–‡ç« é‡Œçš„WKV operator)
		- è¿™ä¹ˆè®¾è®¡stateè½¬ç§»çš„åŠ¨æœº, æˆ‘æ˜¯ä¸æ˜ç™½
	- channel-mixingæ˜¯åˆ†æˆä¸¤è·¯, åˆ†åˆ«åš conv(token window_size=2) + linear + activation
- RWKVåšäº†é…å¥—çš„chatçš„å¾ˆå¤šç›¸å…³ä»£ç å·¥ä½œ

### Mamba

![Pasted image 20240325123207.png]({{ '/docs/attachment/Pasted image 20240325123207.png' | relative_url }}){:width="300"} 

- åŸºç¡€çš„SSMæ¨¡å—æ”¹ä¸ºS6(Selective S4)æ¨¡å—: æŠŠSSMç¦»æ•£è¡¨ç¤ºä¸­$\Delta, B, C$ æ”¹æˆtime-variant. ä»¥é€‚åº”ä¸åŒä½ç½®çš„ä¿¡æ¯å¯†åº¦. 
	- æˆ‘è§‰å¾—è¿™ç§æƒ³æ³•å’Œdeformable convolutionçš„æƒ³æ³•ç±»ä¼¼, éƒ½æ˜¯å¯¹ä¸€äº›ç»“æ„parametrize. 
- Mamba block:
	- ç”¨convolutionå’ŒS6, è€Œä¸æ˜¯attentionè¿›è¡Œä¿¡æ¯äº¤äº’.
	- åŸºäºmamba blockçš„å¤§æ¨¡å‹ç”¨æ›´å°‘çš„å‚æ•°è¾¾åˆ°äº†å’ŒåŸºäºattentionçš„å¤§æ¨¡å‹ç±»ä¼¼çš„æ•ˆæœ. 
	- æœ€è¿‘æœ‰ä¸€äº›æ–‡ç« ç”¨mamba blocké­”æ”¹å»åšè§†è§‰ä»»åŠ¡? æ²¡æœ‰ç‰¹åˆ«äº†è§£ä½†æ˜¯æ„Ÿè§‰å¾ˆå¥‡æ€ª, å› ä¸ºSSMæœ¬èº«æœ‰ä¸ªé¡ºåºçš„ç»“æ„...åœ¨è§†è§‰ä»»åŠ¡ä¸Šè¿™ä¸ªé¡ºåºè¦æ€ä¹ˆå®šä¹‰?

## reference 

[1] Katharopoulos, Angelos, Apoorv Vyas, Nikolaos Pappas, and FranÃ§ois Fleuret. â€œTransformers Are RNNs: Fast Autoregressive Transformers with **Linear Attention**.â€ arXiv, August 31, 2020. [https://doi.org/10.48550/arXiv.2006.16236](https://doi.org/10.48550/arXiv.2006.16236).

[2] Gu, Albert, Karan Goel, and Christopher RÃ©. â€œEfficiently Modeling Long Sequences with **Structured State Spaces**.â€ arXiv, August 5, 2022. [http://arxiv.org/abs/2111.00396](http://arxiv.org/abs/2111.00396). (S4)

[3] Gu, Albert, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Re. â€œ**HiPPO**: Recurrent Memory with Optimal Polynomial Projections.â€ arXiv, October 22, 2020. [https://doi.org/10.48550/arXiv.2008.07669](https://doi.org/10.48550/arXiv.2008.07669).

[4] Gu, Albert, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher RÃ©. â€œHow to Train Your **HiPPO**: State Space Models with Generalized Orthogonal Basis Projections.â€ arXiv, August 5, 2022. [http://arxiv.org/abs/2206.12037](http://arxiv.org/abs/2206.12037).

[5] Fu, Daniel Y., Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, and Christopher RÃ©. â€œ**Hungry Hungry Hippos**: Towards Language Modeling with State Space Models.â€ arXiv, April 28, 2023. [https://doi.org/10.48550/arXiv.2212.14052](https://doi.org/10.48550/arXiv.2212.14052).

[6] Sun, Yutao, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. â€œ**Retentive Network**: A Successor to Transformer for Large Language Models.â€ arXiv, August 9, 2023. [https://doi.org/10.48550/arXiv.2307.08621](https://doi.org/10.48550/arXiv.2307.08621).

[7] Peng, Bo, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, et al. â€œ**RWKV**: Reinventing RNNs for the Transformer Era.â€ arXiv, December 10, 2023. [https://doi.org/10.48550/arXiv.2305.13048](https://doi.org/10.48550/arXiv.2305.13048).

[8] Gu, Albert, and Tri Dao. â€œ**Mamba**: Linear-Time Sequence Modeling with Selective State Spaces.â€ arXiv.org, December 1, 2023. [https://arxiv.org/abs/2312.00752v1](https://arxiv.org/abs/2312.00752v1).
## å…¶ä»–è®¨è®ºçš„é“¾æ¥

[1] https://blog.csdn.net/v_JULY_v/article/details/134923301

[2]  [https://github.com/state-spaces/s4/issues/40](https://github.com/state-spaces/s4/issues/40) ä½œè€…å¯¹hippoçš„ç­”ç–‘

[3] https://www.youtube.com/watch?v=TkOSKrlpnU4&t=716s H3ä½œè€…çš„è®²è§£

[3] https://www.zhihu.com/question/602564718/answer/3042600470 RWKVçš„çŸ¥ä¹è®¨è®º

[4] https://cloud.tencent.com/developer/article/2377967 Mamba blockçš„ç†è§£

