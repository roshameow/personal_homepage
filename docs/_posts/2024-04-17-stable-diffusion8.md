---
layout: post
title: stable-diffusion中 k-sampling的不同版本
categories: 
tags:
  - content
  - jekyll
last_modified_at: 2024-06-03T11:58:23-08:00
---

|                                                                                               | 步骤                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                  |
| --------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Euler<br><br>和SDE的[Euler–Maruyama](https://en.wikipedia.org/wiki/Euler–Maruyama_method) 的解法不同 | noise injection:<br>- increased noise $\hat \sigma$  : $\hat \sigma\leftarrow \sigma_i + \gamma\sigma_i$ <br>-  sample x with increased noise: $\hat x \leftarrow x_i + \sqrt{\hat \sigma^2-\sigma_i^2}\cdot\epsilon$ <br>Take Euler Step: <br>- $dt=\sigma_{i+1}-\hat \sigma$<br>- $denoised=model(\hat x,\hat \sigma)$ <br>- gradient: $d=(\hat x-denoised)/{\hat \sigma}$ <br>- Euler step: $x_{i+1}=\hat x+dt \cdot d$                                               | 令, $\alpha_t=1$ <br><br>$f(t)\rightarrow 0$ <br>$g(t)^2\rightarrow 2\dot\sigma_t\sigma_t$ <br><br>$d\rightarrow-\dot\sigma_t\sigma_t\nabla_x\log p(x;\sigma_t)=\frac{\dot \sigma_t}{\sigma_t}\cdot\sigma_t\frac{x-D(x,t)}{\sigma_t}$   <br><br>令 $\sigma_t=t$ ?<br>省略参数$\frac{\dot \sigma_t}{\sigma_t}$ ?<br><br>$d \rightarrow\frac{x-D(x;\sigma_t)}{\sigma_t}$ |
| [heun](https://en.wikipedia.org/wiki/Heun%27s_method)<br>Euler方法的改进                           | noise injection:  <br>得到$\hat\sigma, \hat x, denoised$ <br> Left tangent prediction: <br>- $x_2=\hat x + dt\cdot d$<br>Right tangent prediction:<br>- $d_2=(x_2-model(x_2,\sigma_{i+1}))/\sigma_{i+1}$ <br>结果: <br>- $d^\prime=\frac{d+d_2}{2}$ <br>- $x_{i+1}=\hat x+d^\prime\cdot dt$                                                                                                                                                                                  |                                                                                                                                                                                                                                                                                                                                                                  |
| Euler Ancestral                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |                                                                                                                                                                                                                                                                                                                                                                  |
| Euler Ancestral                                                                               | Take Euler Step to $\sigma_{down}$:  <br>- $dt=\sigma_{down}-\sigma_i$ <br>- $denoised=model(x,\sigma_i)$ <br>- numerical derivative: $d=(x-denoised)/{\sigma_i}$ <br>- Euler step: $x_{down}=x+dt \cdot d$ <br>Add ancestral noise:<br>- $x_{i+1}=x_{down}+noise*\sigma_{up}$ <br>                                                                                                                                                                                      | <br>$\sigma_{up}=\min(\sigma_{i+1},\eta\cdot(\frac{\sigma_{i+1}^2}{\sigma_i^2}(\sigma_i^2-\sigma_{i+1}^2)))$<br>$\sigma_{down}=\sqrt{\sigma_{i+1}^2-\sigma_{up}^2}$                                                                                                                                                                                              |
| DDPM                                                                                          |                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |                                                                                                                                                                                                                                                                                                                                                                  |
| DDIM                                                                                          |                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |                                                                                                                                                                                                                                                                                                                                                                  |
| DPM-Solver                                                                                    |                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | 根据probability ODE: <br>$\dot x=f(t)\cdot x-\frac{1}{2}g^2(t)\nabla_x\log p(x,t)$<br><br>令, $\alpha_t=1$ <br><br>                                                                                                                                                                                                                                                 |
| dpm_2                                                                                         | noise injection: <br>得到$\hat\sigma, \hat x, denoised$ <br>DPM-Solver-2: <br>- 在$\sigma_{i+1},\hat \sigma$ 之间取: $\sigma_{mid}=e^{\frac{\log \hat\sigma+\log \sigma_{i+1}}{2}}$ <br>- $dt_1=\sigma_{mid}-\hat \sigma$ <br>- $dt_2=\sigma_{i+1}-\hat \sigma$ <br>- $x_{mid}=\hat x+dt_1\cdot(\hat x-model(\hat x,\hat \sigma))/\hat \sigma$  <br>用$\sigma_{mid}$ 处的numerial derivative<br>- $x=\hat x+dt_2\cdot(x_{mid}-model(x_{mid},\sigma_{mid}))/\sigma_{mid}$ <br><br> |                                                                                                                                                                                                                                                                                                                                                                  |
| dpmpp_2m                                                                                      | - $t=-\log(\sigma_i)$ , $t_{next}=-\log(\sigma_{i+1})$ , $t_{last}=-\log(\sigma_{i-1})$ <br>- $h=t_{next}-t$, $h_{last}=t-t_{last}$ <br>- $r=h_{last}/h$ <br>- $denoised_d=(1+\frac{1}{2r})\cdot denoised-\frac{1}{2r}\cdot denoised_{old}$  <br>- $x_{i+1}=\frac{\sigma_{i+1}}{\sigma_i}\cdot x_i-(e^{-h}-1)\cdot denoised_d$ <br>                                                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                  |
| lcm                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |                                                                                                                                                                                                                                                                                                                                                                  |

Euler: 