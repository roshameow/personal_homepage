<!DOCTYPE html>
<!--
    So Simple Jekyll Theme 3.2.0
    Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
    Free for personal and commercial use under the MIT license
    https://github.com/mmistakes/so-simple-theme/blob/master/LICENSE
-->
<html lang="en-US" class="no-js">
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  

  
  
  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>stable-diffusion中 k-sampling的不同版本 | Liu, Wen’s Home Page</title>
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="stable-diffusion中 k-sampling的不同版本" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="  步骤   Euler和SDE的Euler–Maruyama 的解法不同 noise injection:- increased noise $\hat \sigma$ : $\hat \sigma\leftarrow \sigma_i + \gamma\sigma_i$ - sample x with increased noise: $\hat x \leftarrow x_i + \sqrt{\hat \sigma^2-\sigma_i^2}\cdot\epsilon$ Take Euler Step: - $dt=\sigma_{i+1}-\hat \sigma$- $denoised=model(\hat x,\hat \sigma)$ - gradient: $d=(\hat x-denoised)/{\hat \sigma}$ - Euler step: $x_{i+1}=\hat x+dt \cdot d$ 令, $\alpha_t=1$ $f(t)\rightarrow 0$ $g(t)^2\rightarrow 2\dot\sigma_t\sigma_t$ $d\rightarrow-\dot\sigma_t\sigma_t\nabla_x\log p(x;\sigma_t)=\frac{\dot \sigma_t}{\sigma_t}\cdot\sigma_t\frac{x-D(x,t)}{\sigma_t}$ 令 $\sigma_t=t$ ?省略参数$\frac{\dot \sigma_t}{\sigma_t}$ ?$d \rightarrow\frac{x-D(x;\sigma_t)}{\sigma_t}$ heunEuler方法的改进 noise injection: 得到$\hat\sigma, \hat x, denoised$ Left tangent prediction: - $x_2=\hat x + dt\cdot d$Right tangent prediction:- $d_2=(x_2-model(x_2,\sigma_{i+1}))/\sigma_{i+1}$ 结果: - $d^\prime=\frac{d+d_2}{2}$ - $x_{i+1}=\hat x+d^\prime\cdot dt$   Euler Ancestral     Euler Ancestral Take Euler Step to $\sigma_{down}$: - $dt=\sigma_{down}-\sigma_i$ - $denoised=model(x,\sigma_i)$ - numerical derivative: $d=(x-denoised)/{\sigma_i}$ - Euler step: $x_{down}=x+dt \cdot d$ Add ancestral noise:- $x_{i+1}=x_{down}+noise*\sigma_{up}$ $\sigma_{up}=\min(\sigma_{i+1},\eta\cdot(\frac{\sigma_{i+1}^2}{\sigma_i^2}(\sigma_i^2-\sigma_{i+1}^2)))$$\sigma_{down}=\sqrt{\sigma_{i+1}^2-\sigma_{up}^2}$ DDPM     DDIM     DPM-Solver   根据probability ODE: $\dot x=f(t)\cdot x-\frac{1}{2}g^2(t)\nabla_x\log p(x,t)$令, $\alpha_t=1$ dpm_2 noise injection: 得到$\hat\sigma, \hat x, denoised$ DPM-Solver-2: - 在$\sigma_{i+1},\hat \sigma$ 之间取: $\sigma_{mid}=e^{\frac{\log \hat\sigma+\log \sigma_{i+1}}{2}}$ - $dt_1=\sigma_{mid}-\hat \sigma$ - $dt_2=\sigma_{i+1}-\hat \sigma$ - $x_{mid}=\hat x+dt_1\cdot(\hat x-model(\hat x,\hat \sigma))/\hat \sigma$ 用$\sigma_{mid}$ 处的numerial derivative- $x=\hat x+dt_2\cdot(x_{mid}-model(x_{mid},\sigma_{mid}))/\sigma_{mid}$   dpmpp_2m - $t=-\log(\sigma_i)$ , $t_{next}=-\log(\sigma_{i+1})$ , $t_{last}=-\log(\sigma_{i-1})$ - $h=t_{next}-t$, $h_{last}=t-t_{last}$ - $r=h_{last}/h$ - $denoised_d=(1+\frac{1}{2r})\cdot denoised-\frac{1}{2r}\cdot denoised_{old}$ - $x_{i+1}=\frac{\sigma_{i+1}}{\sigma_i}\cdot x_i-(e^{-h}-1)\cdot denoised_d$   lcm    " />
<meta property="og:description" content="  步骤   Euler和SDE的Euler–Maruyama 的解法不同 noise injection:- increased noise $\hat \sigma$ : $\hat \sigma\leftarrow \sigma_i + \gamma\sigma_i$ - sample x with increased noise: $\hat x \leftarrow x_i + \sqrt{\hat \sigma^2-\sigma_i^2}\cdot\epsilon$ Take Euler Step: - $dt=\sigma_{i+1}-\hat \sigma$- $denoised=model(\hat x,\hat \sigma)$ - gradient: $d=(\hat x-denoised)/{\hat \sigma}$ - Euler step: $x_{i+1}=\hat x+dt \cdot d$ 令, $\alpha_t=1$ $f(t)\rightarrow 0$ $g(t)^2\rightarrow 2\dot\sigma_t\sigma_t$ $d\rightarrow-\dot\sigma_t\sigma_t\nabla_x\log p(x;\sigma_t)=\frac{\dot \sigma_t}{\sigma_t}\cdot\sigma_t\frac{x-D(x,t)}{\sigma_t}$ 令 $\sigma_t=t$ ?省略参数$\frac{\dot \sigma_t}{\sigma_t}$ ?$d \rightarrow\frac{x-D(x;\sigma_t)}{\sigma_t}$ heunEuler方法的改进 noise injection: 得到$\hat\sigma, \hat x, denoised$ Left tangent prediction: - $x_2=\hat x + dt\cdot d$Right tangent prediction:- $d_2=(x_2-model(x_2,\sigma_{i+1}))/\sigma_{i+1}$ 结果: - $d^\prime=\frac{d+d_2}{2}$ - $x_{i+1}=\hat x+d^\prime\cdot dt$   Euler Ancestral     Euler Ancestral Take Euler Step to $\sigma_{down}$: - $dt=\sigma_{down}-\sigma_i$ - $denoised=model(x,\sigma_i)$ - numerical derivative: $d=(x-denoised)/{\sigma_i}$ - Euler step: $x_{down}=x+dt \cdot d$ Add ancestral noise:- $x_{i+1}=x_{down}+noise*\sigma_{up}$ $\sigma_{up}=\min(\sigma_{i+1},\eta\cdot(\frac{\sigma_{i+1}^2}{\sigma_i^2}(\sigma_i^2-\sigma_{i+1}^2)))$$\sigma_{down}=\sqrt{\sigma_{i+1}^2-\sigma_{up}^2}$ DDPM     DDIM     DPM-Solver   根据probability ODE: $\dot x=f(t)\cdot x-\frac{1}{2}g^2(t)\nabla_x\log p(x,t)$令, $\alpha_t=1$ dpm_2 noise injection: 得到$\hat\sigma, \hat x, denoised$ DPM-Solver-2: - 在$\sigma_{i+1},\hat \sigma$ 之间取: $\sigma_{mid}=e^{\frac{\log \hat\sigma+\log \sigma_{i+1}}{2}}$ - $dt_1=\sigma_{mid}-\hat \sigma$ - $dt_2=\sigma_{i+1}-\hat \sigma$ - $x_{mid}=\hat x+dt_1\cdot(\hat x-model(\hat x,\hat \sigma))/\hat \sigma$ 用$\sigma_{mid}$ 处的numerial derivative- $x=\hat x+dt_2\cdot(x_{mid}-model(x_{mid},\sigma_{mid}))/\sigma_{mid}$   dpmpp_2m - $t=-\log(\sigma_i)$ , $t_{next}=-\log(\sigma_{i+1})$ , $t_{last}=-\log(\sigma_{i-1})$ - $h=t_{next}-t$, $h_{last}=t-t_{last}$ - $r=h_{last}/h$ - $denoised_d=(1+\frac{1}{2r})\cdot denoised-\frac{1}{2r}\cdot denoised_{old}$ - $x_{i+1}=\frac{\sigma_{i+1}}{\sigma_i}\cdot x_i-(e^{-h}-1)\cdot denoised_d$   lcm    " />
<link rel="canonical" href="https://roshameow.github.io//personal_homepage/docs/stable-diffusion8/" />
<meta property="og:url" content="https://roshameow.github.io//personal_homepage/docs/stable-diffusion8/" />
<meta property="og:site_name" content="Liu, Wen’s Home Page" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-04-17T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="stable-diffusion中 k-sampling的不同版本" />
<meta name="twitter:site" content="@" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-06-03T19:58:23+00:00","datePublished":"2024-04-17T00:00:00+00:00","description":"  步骤   Euler和SDE的Euler–Maruyama 的解法不同 noise injection:- increased noise $\\hat \\sigma$ : $\\hat \\sigma\\leftarrow \\sigma_i + \\gamma\\sigma_i$ - sample x with increased noise: $\\hat x \\leftarrow x_i + \\sqrt{\\hat \\sigma^2-\\sigma_i^2}\\cdot\\epsilon$ Take Euler Step: - $dt=\\sigma_{i+1}-\\hat \\sigma$- $denoised=model(\\hat x,\\hat \\sigma)$ - gradient: $d=(\\hat x-denoised)/{\\hat \\sigma}$ - Euler step: $x_{i+1}=\\hat x+dt \\cdot d$ 令, $\\alpha_t=1$ $f(t)\\rightarrow 0$ $g(t)^2\\rightarrow 2\\dot\\sigma_t\\sigma_t$ $d\\rightarrow-\\dot\\sigma_t\\sigma_t\\nabla_x\\log p(x;\\sigma_t)=\\frac{\\dot \\sigma_t}{\\sigma_t}\\cdot\\sigma_t\\frac{x-D(x,t)}{\\sigma_t}$ 令 $\\sigma_t=t$ ?省略参数$\\frac{\\dot \\sigma_t}{\\sigma_t}$ ?$d \\rightarrow\\frac{x-D(x;\\sigma_t)}{\\sigma_t}$ heunEuler方法的改进 noise injection: 得到$\\hat\\sigma, \\hat x, denoised$ Left tangent prediction: - $x_2=\\hat x + dt\\cdot d$Right tangent prediction:- $d_2=(x_2-model(x_2,\\sigma_{i+1}))/\\sigma_{i+1}$ 结果: - $d^\\prime=\\frac{d+d_2}{2}$ - $x_{i+1}=\\hat x+d^\\prime\\cdot dt$   Euler Ancestral     Euler Ancestral Take Euler Step to $\\sigma_{down}$: - $dt=\\sigma_{down}-\\sigma_i$ - $denoised=model(x,\\sigma_i)$ - numerical derivative: $d=(x-denoised)/{\\sigma_i}$ - Euler step: $x_{down}=x+dt \\cdot d$ Add ancestral noise:- $x_{i+1}=x_{down}+noise*\\sigma_{up}$ $\\sigma_{up}=\\min(\\sigma_{i+1},\\eta\\cdot(\\frac{\\sigma_{i+1}^2}{\\sigma_i^2}(\\sigma_i^2-\\sigma_{i+1}^2)))$$\\sigma_{down}=\\sqrt{\\sigma_{i+1}^2-\\sigma_{up}^2}$ DDPM     DDIM     DPM-Solver   根据probability ODE: $\\dot x=f(t)\\cdot x-\\frac{1}{2}g^2(t)\\nabla_x\\log p(x,t)$令, $\\alpha_t=1$ dpm_2 noise injection: 得到$\\hat\\sigma, \\hat x, denoised$ DPM-Solver-2: - 在$\\sigma_{i+1},\\hat \\sigma$ 之间取: $\\sigma_{mid}=e^{\\frac{\\log \\hat\\sigma+\\log \\sigma_{i+1}}{2}}$ - $dt_1=\\sigma_{mid}-\\hat \\sigma$ - $dt_2=\\sigma_{i+1}-\\hat \\sigma$ - $x_{mid}=\\hat x+dt_1\\cdot(\\hat x-model(\\hat x,\\hat \\sigma))/\\hat \\sigma$ 用$\\sigma_{mid}$ 处的numerial derivative- $x=\\hat x+dt_2\\cdot(x_{mid}-model(x_{mid},\\sigma_{mid}))/\\sigma_{mid}$   dpmpp_2m - $t=-\\log(\\sigma_i)$ , $t_{next}=-\\log(\\sigma_{i+1})$ , $t_{last}=-\\log(\\sigma_{i-1})$ - $h=t_{next}-t$, $h_{last}=t-t_{last}$ - $r=h_{last}/h$ - $denoised_d=(1+\\frac{1}{2r})\\cdot denoised-\\frac{1}{2r}\\cdot denoised_{old}$ - $x_{i+1}=\\frac{\\sigma_{i+1}}{\\sigma_i}\\cdot x_i-(e^{-h}-1)\\cdot denoised_d$   lcm    ","headline":"stable-diffusion中 k-sampling的不同版本","mainEntityOfPage":{"@type":"WebPage","@id":"https://roshameow.github.io//personal_homepage/docs/stable-diffusion8/"},"url":"https://roshameow.github.io//personal_homepage/docs/stable-diffusion8/"}</script>
<!-- End Jekyll SEO tag -->


  

  <script>
    /* Cut the mustard */
    if ('querySelector' in document && 'addEventListener' in window) {
      document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + 'js';
    }
  </script>

  <link rel="stylesheet" href="/personal_homepage/assets/css/main.css">
  <link rel="stylesheet" href="/personal_homepage/assets/css/skins/default.css">
  
  <link rel="stylesheet"
    href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,400i,700,700i|Lora:400,400i,700,700i">
  <link rel="alternate" type="application/atom+xml" title="Liu, Wen&#39;s Home Page" href="/personal_homepage/atom.xml">
<!-- start custom head snippets -->

<!-- insert favicons. use http://realfavicongenerator.net/ -->
<link rel="shortcut icon" type="image/x-icon" href="/personal_homepage/docs/images/logo.ico">
<!-- end custom head snippets -->

</head>

  <body class="layout--post  stable-diffusion中-k-sampling的不同版本">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#primary-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    
  <div class="navigation-wrapper">
    <a href="#menu-toggle" id="menu-toggle">Menu</a>
    <nav id="primary-nav" class="site-nav animated drop">
      <ul><li><a href="/personal_homepage/">Home</a></li><li><a href="/personal_homepage/posts/">Posts</a></li><li><a href="/personal_homepage/categories/">Categories</a></li><li><a href="/personal_homepage/tags/">Tags</a></li><li><a href="/personal_homepage/recipes/">Family</a></li><li><a href="/personal_homepage/search/">Search</a></li></ul>
    </nav>
  </div><!-- /.navigation-wrapper -->


    <header class="masthead">
  <div class="wrap">
    
    
    
      
        <div class="site-title animated fadeIn"><a href="/personal_homepage/">Liu, Wen's Home Page</a></div>
      
      <p class="site-description animated fadeIn" itemprop="description">Work, Experiments and Ideas.</p>
    
  </div>
</header><!-- /.masthead -->


    <main id="main" class="main-content" aria-label="Content">
  <article class="h-entry">
    

    <div class="page-wrapper">
      <header class="page-header">
        
        
        <h1 id="page-title" class="page-title p-name">stable-diffusion中 k-sampling的不同版本
</h1>
        
      </header>

      <div class="page-sidebar">
        <div class="page-author h-card p-author"><img src="/personal_homepage/docs/images/logo.svg" class="author-avatar u-photo" alt=""><div class="author-info">

<span class="read-time">~1 min read</span>

    <time class="page-date dt-published" datetime="2024-04-17T00:00:00+00:00"><a class="u-url" href="">April 17, 2024</a>
</time>

  </div>
</div>

        
  <h3 class="page-taxonomies-title">Categories</h3>
  
  <ul class="page-taxonomies"><li class="page-taxonomy"><a class="p-category" href="/personal_homepage/categories/#docs" title="Pages filed under docs">docs</a></li>
  </ul>


        
  <h3 class="page-taxonomies-title">Tags</h3>
  
  <ul class="page-taxonomies"><li class="page-taxonomy"><a href="/personal_homepage/tags/#content" title="Pages tagged content" rel="tag">content</a></li><li class="page-taxonomy"><a href="/personal_homepage/tags/#jekyll" title="Pages tagged jekyll" rel="tag">jekyll</a></li>
  </ul>


        <!-- {::options parse_block_html="true" /} -->
<div id="entry-table-of-contents" class="toc-wrapper">
<h2 id="toc-toggle" class="no_toc">
  Table of Contents <i class="toc-toggle-icon fas fa-chevron-down"></i>
</h2>
<!-- 1. toc
{:toc} -->

</div>
<!-- {::options parse_block_html="false" /} -->

      </div>

      <div class="page-content">
        <div class="e-content">
          <table>
  <thead>
    <tr>
      <th> </th>
      <th>步骤</th>
      <th> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Euler<br /><br />和SDE的<a href="https://en.wikipedia.org/wiki/Euler–Maruyama_method">Euler–Maruyama</a> 的解法不同</td>
      <td>noise injection:<br />- increased noise $\hat \sigma$  : $\hat \sigma\leftarrow \sigma_i + \gamma\sigma_i$ <br />-  sample x with increased noise: $\hat x \leftarrow x_i + \sqrt{\hat \sigma^2-\sigma_i^2}\cdot\epsilon$ <br />Take Euler Step: <br />- $dt=\sigma_{i+1}-\hat \sigma$<br />- $denoised=model(\hat x,\hat \sigma)$ <br />- gradient: $d=(\hat x-denoised)/{\hat \sigma}$ <br />- Euler step: $x_{i+1}=\hat x+dt \cdot d$</td>
      <td>令, $\alpha_t=1$ <br /><br />$f(t)\rightarrow 0$ <br />$g(t)^2\rightarrow 2\dot\sigma_t\sigma_t$ <br /><br />$d\rightarrow-\dot\sigma_t\sigma_t\nabla_x\log p(x;\sigma_t)=\frac{\dot \sigma_t}{\sigma_t}\cdot\sigma_t\frac{x-D(x,t)}{\sigma_t}$   <br /><br />令 $\sigma_t=t$ ?<br />省略参数$\frac{\dot \sigma_t}{\sigma_t}$ ?<br /><br />$d \rightarrow\frac{x-D(x;\sigma_t)}{\sigma_t}$</td>
    </tr>
    <tr>
      <td><a href="https://en.wikipedia.org/wiki/Heun%27s_method">heun</a><br />Euler方法的改进</td>
      <td>noise injection:  <br />得到$\hat\sigma, \hat x, denoised$ <br /> Left tangent prediction: <br />- $x_2=\hat x + dt\cdot d$<br />Right tangent prediction:<br />- $d_2=(x_2-model(x_2,\sigma_{i+1}))/\sigma_{i+1}$ <br />结果: <br />- $d^\prime=\frac{d+d_2}{2}$ <br />- $x_{i+1}=\hat x+d^\prime\cdot dt$</td>
      <td> </td>
    </tr>
    <tr>
      <td>Euler Ancestral</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>Euler Ancestral</td>
      <td>Take Euler Step to $\sigma_{down}$:  <br />- $dt=\sigma_{down}-\sigma_i$ <br />- $denoised=model(x,\sigma_i)$ <br />- numerical derivative: $d=(x-denoised)/{\sigma_i}$ <br />- Euler step: $x_{down}=x+dt \cdot d$ <br />Add ancestral noise:<br />- $x_{i+1}=x_{down}+noise*\sigma_{up}$ <br /></td>
      <td><br />$\sigma_{up}=\min(\sigma_{i+1},\eta\cdot(\frac{\sigma_{i+1}^2}{\sigma_i^2}(\sigma_i^2-\sigma_{i+1}^2)))$<br />$\sigma_{down}=\sqrt{\sigma_{i+1}^2-\sigma_{up}^2}$</td>
    </tr>
    <tr>
      <td>DDPM</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>DDIM</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>DPM-Solver</td>
      <td> </td>
      <td>根据probability ODE: <br />$\dot x=f(t)\cdot x-\frac{1}{2}g^2(t)\nabla_x\log p(x,t)$<br /><br />令, $\alpha_t=1$ <br /><br /></td>
    </tr>
    <tr>
      <td>dpm_2</td>
      <td>noise injection: <br />得到$\hat\sigma, \hat x, denoised$ <br />DPM-Solver-2: <br />- 在$\sigma_{i+1},\hat \sigma$ 之间取: $\sigma_{mid}=e^{\frac{\log \hat\sigma+\log \sigma_{i+1}}{2}}$ <br />- $dt_1=\sigma_{mid}-\hat \sigma$ <br />- $dt_2=\sigma_{i+1}-\hat \sigma$ <br />- $x_{mid}=\hat x+dt_1\cdot(\hat x-model(\hat x,\hat \sigma))/\hat \sigma$  <br />用$\sigma_{mid}$ 处的numerial derivative<br />- $x=\hat x+dt_2\cdot(x_{mid}-model(x_{mid},\sigma_{mid}))/\sigma_{mid}$ <br /><br /></td>
      <td> </td>
    </tr>
    <tr>
      <td>dpmpp_2m</td>
      <td>- $t=-\log(\sigma_i)$ , $t_{next}=-\log(\sigma_{i+1})$ , $t_{last}=-\log(\sigma_{i-1})$ <br />- $h=t_{next}-t$, $h_{last}=t-t_{last}$ <br />- $r=h_{last}/h$ <br />- $denoised_d=(1+\frac{1}{2r})\cdot denoised-\frac{1}{2r}\cdot denoised_{old}$  <br />- $x_{i+1}=\frac{\sigma_{i+1}}{\sigma_i}\cdot x_i-(e^{-h}-1)\cdot denoised_d$ <br /></td>
      <td> </td>
    </tr>
    <tr>
      <td>lcm</td>
      <td> </td>
      <td> </td>
    </tr>
  </tbody>
</table>

<p>Euler:</p>

        </div>

        

        
        
  <div class="page-comments">
    <div id="disqus_thread"></div>
    <script>
      var disqus_config = function () {
        this.page.url = 'https://roshameow.github.io//personal_homepage/docs/stable-diffusion8/';
        this.page.identifier = 'https://roshameow.github.io//personal_homepage/docs/stable-diffusion8/';
      };

      (function() {
        var d = document, s = d.createElement('script');
        s.src = 'https://https-roshameow-github-io-personal-homepage.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
      })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
  </div>


        

        <nav class="page-pagination" role="navigation">
  
    <a class="page-previous" href="/personal_homepage/docs/algorithm/stable-diffusion7/">
      <h4 class="page-pagination-label">Previous</h4>
      <span class="page-pagination-title">
        <i class="fas fa-arrow-left"></i> stable-diffusion中 k-sampling的不同版本

      </span>
    </a>
  

  
    <a class="page-next" href="/personal_homepage/docs/algorithm/optical_flow_train2/">
      <h4 class="page-pagination-label">Next</h4>
      <span class="page-pagination-title">
        小面积光流传感器算法测试 (二) – 特征训练
 <i class="fas fa-arrow-right"></i>
      </span>
    </a>
  
</nav>

      </div>
    </div>
  </article>
</main>

    <footer id="footer" class="site-footer">
  <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
<div class="social-icons"><a class="social-icon" href="/personal_homepage/atom.xml"><i class="fas fa-rss-square fa-2x" title="Feed"></i></a><a class="social-icon" href="https://github.com/roshameow"><i class="fab fa-github-square fa-2x" title="GitHub"></i></a><a class="social-icon" href="https://www.linkedin.com/in/wen-liu-06503a7b/"><i class="fab fa-linkedin fa-2x" title="LinkedIn"></i></a><a class="social-icon" href="mailto:w.liuatnk@gmail.com"><i class="fas fa-envelope-square fa-2x" title="e-mail"></i></a></div><div class="copyright">
    
      <p>&copy; 2024 Liu, Wen's Home Page. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://github.com/mmistakes/so-simple-theme" rel="nofollow">So Simple</a>.</p>
    
  </div>
</footer>

    <script src="https://code.jquery.com/jquery-3.3.1.min.js"
  integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script>
<script src="/personal_homepage/assets/js/main.min.js"></script>
<script src="https://use.fontawesome.com/releases/v5.0.12/js/all.js"></script><!-- <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'G-9TMSCDTBGN', 'auto');
  ga('send', 'pageview');
</script> -->

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-9TMSCDTBGN"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-9TMSCDTBGN');
</script>



<!-- MathJax -->

<script>
    // http://docs.mathjax.org/en/latest/upgrading/v2.html
    MathJax = {
        tex: {
            tags: "ams",    // eq numbering options: none, ams, all
            inlineMath: [['$', '$'], ["\\(", "\\)"]],  // 设置行内数学公式的开始和结束标记
            displayMath: [['$$', '$$'], ["\\[", "\\]"]]  // 设置行间数学公式的开始和结束标记
        },
        // options: {
        //   renderActions: {
        //     // for mathjax 3, handle <script "math/tex"> blocks inserted by kramdown
        //     find: [10, function (doc) {
        //       for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
        //         const display = !!node.type.match(/; *mode=display/);
        //         const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
        //         const text = document.createTextNode('');
        //         node.parentNode.replaceChild(text, node);
        //         math.start = { node: text, delim: '', n: 0 };
        //         math.end = { node: text, delim: '', n: 0 };
        //         doc.math.push(math);
        //       }
        //     }, '']
        //   }
        // }
    }
</script>

<script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  </body>

</html>
